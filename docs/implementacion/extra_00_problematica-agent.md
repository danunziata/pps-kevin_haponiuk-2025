Generalmente es mucho más común encontrar este tipo de arquitectura en YouTube o foros con APIs conectadas a algún modelo, por ejemplo ChatGPT, que termina funcionando bien (se termina omitiendo la herramienta que se llama Infopapers y se conecta directamente a la base de datos). Sin embargo, el objetivo de este proyecto es realizar todo de manera local. Para esto, N8N en las herramientas requiere este tipo de arquitectura, es decir, conectando Infopapers que requiere poner un modelo más de Ollama. Se estuvo probando con distintos modelos y distintos prompts, pero no se pudo solucionar la problemática. Lo que termina haciendo es que, una vez recibida la query, al momento de realizar la búsqueda en la base de datos funciona correctamente y devuelve bien, pero el modelo que está en Infopapers termina generando una respuesta que le entrega al siguiente modelo del AI Agent, causando un doble procesamiento que termina afectando la respuesta y no funcionando como se espera. Por lo tanto, se buscó otra forma de armar la arquitectura, sabiendo que la base de datos está respondiendo bien pero la problemática está luego de la búsqueda en la base de datos.