# Prueba en **Dorotea** ‚Äî Ajustes de arquitectura y c√≥digo

> **Objetivo.** Salir del entorno local y validar el stack en un servidor remoto de altas prestaciones (Dorotea) sin exponer puertos p√∫blicos; mantener acceso seguro v√≠a **SSH** y dejar todo listo para pruebas de carga y funcionales.

---

## Requisitos Previos: Drivers de NVIDIA Instalados en Rocky Linux

El servidor debe tener los drivers propietarios de NVIDIA funcionando correctamente. Para verificarlo, ejecuta este comando:

```bash
nvidia-smi
```

Si ves una tabla con los detalles de tu GPU, est√°s listo. Si te da un error, necesitas instalar los drivers primero.

### Docker Instalado
Debes tener una instalaci√≥n funcional de Docker. Puedes verificarlo con:

```bash
docker --version
```

---

### Paso 1: Configurar los Repositorios de NVIDIA
Rocky Linux no sabe d√≥nde encontrar el toolkit por defecto. Primero, debemos agregar los repositorios oficiales de NVIDIA a nuestro sistema.

```bash
sudo dnf config-manager --add-repo https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo
```

### Paso 2: Instalar el NVIDIA Container Toolkit
Ahora que el sistema conoce el repositorio, puedes instalar el paquete usando dnf.

```bash
sudo dnf install -y nvidia-container-toolkit
```

### Paso 3: Configurar Docker para que Use el Toolkit
Una vez instalado, tienes que decirle a Docker que lo utilice como un "runtime" para GPUs. El toolkit incluye una herramienta para hacer esto autom√°ticamente.

```bash
sudo nvidia-ctk runtime configure --runtime=docker
```

Despu√©s de configurar, es crucial reiniciar el servicio de Docker para que aplique los cambios.

```bash
sudo systemctl restart docker
```

### Paso 4: Verificar la Instalaci√≥n ‚úÖ
La mejor forma de confirmar que todo funciona es ejecutar un contenedor de prueba que intente acceder a la GPU. El siguiente comando es el est√°ndar para esta prueba:

```bash
docker run --rm --gpus all nvidia/cuda:12.0.0-base-ubuntu22.04 nvidia-smi
```

---

## Resumen de la evoluci√≥n de la arquitectura

### Opci√≥n 1: NGINX ‚Üí 4 servicios (ideal)

<p align="center">
  <img src="../../images/dorotea_op1.png" width="100%">
</p>

  Un solo puerto expuesto por **NGINX** y, detr√°s, 4 servicios accesibles por **subpath**:

    - `/` ‚Üí n8n
    - `/grafana/` ‚Üí Grafana
    - `/webui/` ‚Üí OpenWebUI
    - `/qdrant/` ‚Üí Qdrant

Ventaja: 1 t√∫nel SSH, una √∫nica puerta de entrada y URLs limpias.


* **Problemas encontrados**

  * **WebSockets/SSE de n8n**: el proxy requer√≠a `proxy_http_version 1.1`, headers `Upgrade/Connection` y timeouts altos. Sin eso, fallaban eventos del editor y la UI mostraba ‚ÄúCould not connect to server‚Äù.
  * **Subpath**:

    * **Grafana** requiere `GF_SERVER_ROOT_URL` apuntando al subpath y `GF_SERVER_SERVE_FROM_SUB_PATH=true`. Aun con eso, aparecieron 301/redirects a `/grafana` y recursos est√°ticos fuera del path.
    * **OpenWebUI** sirve assets en rutas absolutas; al forzarlo bajo `/webui/` faltaban archivos (`/static/splash.png` 404) y la UI quedaba en blanco salvo el loader.

---

### Opci√≥n 2: solamente NGINX -> N8N (transici√≥n)

* 1 puerto ‚Üí NGINX ‚Üí **solo n8n** en `/`.
<p align="center">
  <img src="../../images/dorotea_op2.png" width="100%">
</p>

### Opci√≥n 3: 4 puertos v√≠a SSH (implementada)


<p align="center">
  <img src="../../images/dorotea_op3.png" width="100%">
</p>

Se abren **t√∫neles locales** (sin puertos p√∫blicos):

```bash
ssh -i ~/.ssh/dorotea-kevin -L 51234:localhost:51234 daninziata@200.7.141.59
ssh -i ~/.ssh/dorotea-kevin -L 51235:localhost:51235 daninziata@200.7.141.59
ssh -i ~/.ssh/dorotea-kevin -L 51236:localhost:51236 daninziata@200.7.141.59
ssh -i ~/.ssh/dorotea-kevin -L 51237:localhost:51237 daninziata@200.7.141.59
```

Con esto:

* `http://localhost:51234` ‚Üí **n8n** (pasa por NGINX)
* `http://localhost:51235` ‚Üí **Grafana**
* `http://localhost:51236` ‚Üí **OpenWebUI**
* `http://localhost:51237/dashboard#/collections` ‚Üí **Qdrant** (dashboard)

---
## Principales diferencias **ANTES** vs **DESPU√âS**

### Ajustes de NGINX para utilizar n8n en la ra√≠z (`/`)

El proxy qued√≥ ajustado para **WebSockets/SSE** y **conexiones largas**:

* `proxy_http_version 1.1`
* `Upgrade` / `Connection "upgrade"`
* timeouts altos (`proxy_read_timeout`, `proxy_send_timeout`)
* `proxy_buffering off` para *server-sent events* (SSE)

Con estos ajustes, el editor de n8n dej√≥ de mostrar errores de conexi√≥n y pudo autenticarse y trabajar con normalidad detr√°s del proxy.


### Cambio de almacenamiento (limitaci√≥n de `/var`)

**Contexto.** El servidor tiene:

```
/dev/mapper/vg_server-lv_root   79G   49G   30G  62%  /
/dev/nvme1n1p1                   7T  398G  6.2T   6%  /home
```

El *runtime* de Docker por defecto escribe en **`/var/lib/docker`** (dentro de `/`), lo que **r√°pidamente agotaba espacio**, especialmente al descargar **modelos de Ollama** y datos persistentes.

**Acci√≥n.** Se reemplazaron **vol√∫menes nombrados** por **bind mounts** hacia **`/home`**:

* Postgres ‚Üí `/home/daninziata/docker_data/postgres`
* Grafana ‚Üí `/home/daninziata/docker_data/grafana`
* OpenWebUI ‚Üí `/home/daninziata/docker_data/openwebui`
* Qdrant ‚Üí `/home/daninziata/docker_data/qdrant`
* Prometheus ‚Üí `/home/daninziata/docker_data/prometheus`
* **Ollama (modelos)** ‚Üí `/home/daninziata/ollama_data`

**Resultado.** Todo lo pesado sale de `/var` y pasa al disco grande de **7 TB**, eliminando errores por falta de espacio.

### Networking

**Antes:** muchos servicios con `ports:` publicados hacia el host.

**Despu√©s:**

  - **n8n** permanece detr√°s de **NGINX** (publicado en `51234`).
  - **Grafana**, **OpenWebUI** y **Qdrant** se publican en **puertos altos** (51235/51236/51237) s√≥lo para **t√∫neles SSH**.
  - Servicios internos (Postgres, Prometheus, cAdvisor, etc.) usan **`expose`** dentro de la red `demo` y no se publican al host.

### Persistencia

* **Antes:** vol√∫menes **nombrados** (`grafana_data`, `prometheus_data`, etc.).
* **Despu√©s:** **bind mounts** a rutas en `/home/daninziata/...` para cada servicio, y **`/home/daninziata/ollama_data`** para los modelos.

### Nombres/hosts

* **Antes:** contenedores cortos (`ollama`, `qdrant`).
* **Despu√©s:** sufijo `-k` para distinguir el despliegue remoto (ej. `ollama-k`, `qdrant-k`, `nginx-proxy-k`, etc.). Las apps que dependen por URL se actualizaron (ej. `OLLAMA_HOST=ollama-k:11434`).

---

### Monitoreo: cAdvisor + Prometheus

* **S√≠ntoma inicial en Prometheus:**
  `connect: connection refused` al intentar scrapear `cadvisor-k:8081/metrics`.


**Causa:** El contenedor cAdvisor **escucha en 8080**, pero el *job* de Prometheus apuntaba a **8081**.

**Arreglo aplicado:**

  * En Compose: `cadvisor` con `expose: "8080"`.
  * En Prometheus: job `cadvisor` ‚Üí `targets: ['cadvisor-k:8080']`.

---

### Servicio **qdrant-search** (Flask + Qdrant Client + Ollama)

**Par√°metros por variables de entorno** (en Compose):

  * `OLLAMA_URL=http://ollama-k:11434`
  * `QDRANT_URL=http://qdrant-k:6333`
  * `COLLECTION_NAME=TEST_DOROTEA`

> **Importante:** Si se cambia el nombre de **colecci√≥n**, hay que **reconstruir** la imagen para propagar variables al contenedor:

> ```bash
    docker compose build --no-cache qdrant-search
    docker compose up -d qdrant-search
    docker exec -it qdrant-search-k sh
    cat /app/client_qdrant_flask_docker.py | grep collection_name
> ```

El microservicio devolv√≠a `Not found: Collection 'prueba_CRAG3' doesn‚Äôt exist!` aun cuando `COLLECTION_NAME=TEST_DOROTEA`. **Causa**: el contenedor estaba corriendo con **capa antigua** (variables de una build previa). **Soluci√≥n**: **rebuild** del servicio. Tras eso, las consultas desde n8n y desde la red Docker funcionaron.

### GPU y modelos (Ollama)

* **Verificaci√≥n** de uso de GPU con `nvidia-smi`: durante generaci√≥n se observ√≥ `GPU-Util 100%` en la **A30** y consumo de **VRAM** acorde a los modelos cargados. Sin embargo, solo en una de las 2 placas. tambi√©n se observ√≥ que 

<p align="center">
  <img src="../../images/dorotea_nvidiauso.png" width="100%">
</p>

Logs de Ollama mostraron `loaded CUDA backend`, ‚ÄúVRAM available/required‚Äù y selecci√≥n de **GPULayers**, confirmando aceleraci√≥n CUDA.
**Mensaje ‚Äútruncating input prompt (limit=4096)‚Äù**: es el **l√≠mite de contexto** del modelo en **tokens**. Si el *prompt* (incluida la conversaci√≥n previa) lo supera, Ollama lo recorta. Mitigaci√≥n: prompts m√°s breves o modelos con mayor contexto (cuando est√©n disponibles).

## Resultados de las pruebas funcionales en Dorotea

> **Objetivo:** Validar el funcionamiento integral del stack con aceleraci√≥n GPU, recolecci√≥n de m√©tricas en Grafana y generaci√≥n de embeddings mediante Ollama y Qdrant, utilizando la colecci√≥n `TEST_DOROTEA` como caso de prueba.

---

### 1. Pruebas de carga ‚Äî Uso extremo de CPU (Profile CPU)

Durante la primera etapa de las pruebas se intent√≥ ejecutar un perfil intensivo del modelo (`Profile CPU`), pero el proceso **satur√≥ completamente los n√∫cleos** de la m√°quina, llevando el uso de CPU al 100% en todos los hilos.

El proceso fue ejecutado desde Ollama (`/usr/bin/ollama runner`) con m√∫ltiples hilos en paralelo, observ√°ndose que cada uno ejecutaba la carga de modelo en puertos independientes (`--port 33137`).

> Este test qued√≥ en ejecuci√≥n continua sin completar, por lo que se detuvo manualmente para evitar degradar el rendimiento del resto de los contenedores.

<p align="center">
  <img src="../../images/dorotea_cpu100.png" width="100%">
</p>

---

### 2. Monitoreo con Grafana y cAdvisor

Durante la ejecuci√≥n se dej√≥ **Grafana activo** con los dashboards de Prometheus y cAdvisor recolectando m√©tricas en tiempo real de todos los contenedores.

El monitoreo permiti√≥ observar:

* N√∫mero de contenedores activos: **13**
* Procesos m√°s demandantes de CPU:
  - `grobid-k` (36.6%)
  - `cadvisor-k` (10.9%)
  - `pgadmin-k`, `n8n-k`, `prometheus-k` en menor proporci√≥n
* Consumo de memoria: promedio general bajo, sin indicios de *swapping* ni saturaci√≥n.

> Estos datos validan que el entorno completo puede ejecutarse de manera estable durante largos periodos, incluso con procesos intensivos como el parsing de papers o la generaci√≥n de embeddings.

<p align="center">
  <img src="../../images/dorotea_grafana.png" width="100%">
</p>

---

### 3. Carga de datos ‚Äî Creaci√≥n de la colecci√≥n `TEST_DOROTEA`

Tras estabilizar el entorno, se procedi√≥ a realizar una carga de **10 papers cient√≠ficos** en formato JSON procesados por el pipeline Grobid ‚Üí Ollama ‚Üí Qdrant.

**Duraci√≥n total del proceso:** aproximadamente **2 horas**.

Esto sugiere que el cuello de botella podr√≠a encontrarse en:

* **Procesamiento de embeddings** dentro de Ollama (`nomic-embed-text`).
* **Paralelizaci√≥n limitada** o falta de optimizaci√≥n en la inserci√≥n de datos en Qdrant.

> Se recomienda analizar en profundidad la arquitectura de Ollama (modelos, batch size, n√∫mero de workers) para reducir los tiempos de carga en pruebas futuras.

#### Resultado:

La colecci√≥n `TEST_DOROTEA` se cre√≥ correctamente, con estado **green** y **554 puntos vectoriales** distribuidos en **8 segmentos**.

<p align="center">
  <img src="../../images/dorotea_collection.png" width="100%">
</p>

---

### 4. Visualizaci√≥n de la colecci√≥n en Qdrant

La exploraci√≥n de la colecci√≥n en la interfaz de Qdrant mostr√≥ una **distribuci√≥n coherente** de los embeddings en el espacio vectorial, confirmando que los puntos fueron generados correctamente y los metadatos (contenido, fuente, etc.) se mantienen accesibles.

<p align="center">
  <img src="../../images/dorotea_collection_nube.png" width="100%">
</p>

---

### 5. Pruebas de b√∫squeda sem√°ntica

Una vez creada la colecci√≥n, se probaron distintas consultas al agente a trav√©s de `qdrant-search-k`, verificando que las respuestas generadas correspond√≠an a la sem√°ntica de las preguntas y retornaban papers relevantes.

#### üîπ **Consulta 1**

<p align="center">
  <img src="../../images/dorotea_consulta1.png" width="100%">
</p>

**Pregunta:** ¬øQu√© trabajos hablan sobre Kubernetes o autoscaling en contenedores?

**Respuesta del agente:**

> No specific information was found in the texts to answer the question regarding the types of jobs (trabajos) related to Kubernetes or autoscaling in containers.  
>
> **Recommended Papers:**
>
> - Adaptive AI-based auto-scaling for Kubernetes  
> - Predictive Hybrid Autoscaling for Containerized Applications  
> - Time series forecasting-based Kubernetes autoscaling using Facebook Prophet and Long Short-Term Memory

---

#### üîπ **Consulta 2**

<p align="center">
  <img src="../../images/dorotea_consulta2.png" width="100%">
</p>

**Pregunta:** Dame referencias sobre los papers que mencionan MLOps o pipelines de machine learning.

**Respuesta del agente:**

> Here are the references to papers that mention MLOps or pipelines in machine learning:  
>
> - "MLOps: A Taxonomy and a Methodology"  
> - "Towards MLOps: A Case Study of ML Pipeline Platform"  
> - "MLOps: A Taxonomy and a Methodology"  
> - "MLOps: A Taxonomy and a Methodology"  
> - "MLOps: A Taxonomy and a Methodology"  
>
> These papers provide an overview of the MLOps methodology, its importance, and its application in industrial environments. They also discuss various aspects of MLOps, including pipelines, DevOps principles, and benchmarking tools.  
>
> The papers also mention other relevant topics such as:  
>
> - Machine Learning Operations (MLOps)  
> - ML pipeline platforms  
> - CI/CD routines  
> - Automated ML triggering  
> - Model registry storage  
> - Monitoring and performance  
>
> You can find more information about these papers by accessing the references provided in the database results.  
>
> **Recommended Papers:**  
>
> - MLOps: A Taxonomy and a Methodology  
> - Towards MLOps: A Case Study of ML Pipeline Platform

---

#### üîπ **Consulta 3**

<p align="center">
  <img src="../../images/dorotea_consulta3.png" width="100%">
</p>

**Pregunta:** ¬øQu√© papers analizan la eficiencia energ√©tica o el consumo de energ√≠a en la nube?

**Respuesta del agente:**

> "Energy efficiency is always a major concern in cloud computing and when it comes to the application level many recommendations have been made to optimise energy usage at the system level."  
>
> "No specific information was found in the texts to answer the question about papers that analyze the efficiency of energy consumption or energy consumption in the nube, specifically about papers with titles that include "Energy efficiency in cloud computing data centers" and "Energy efficiency" or mention "energy consumption"."  
>
> **Recommended Papers:**  
>
> - Energy efficiency in cloud computing data centers: a survey on software technologies  
> - Machine Learning-based Orchestration of Containers: A Taxonomy and Future Directions

---

#### üîπ **Consulta 4**

<p align="center">
  <img src="../../images/dorotea_consulta4.png" width="100%">
</p>

**Pregunta:** ¬øCu√°les de los papers hacen una revisi√≥n o resumen del estado del arte sobre tecnolog√≠as de contenedores?

**Respuesta del agente:**

> No specific information was found in the texts to answer the question about papers that analyze the efficiency of energy consumption or energy consumption in the nube.  
>
> **Recommended Papers:**  
>
> - Container Technologies for ARM Architecture: A Comprehensive Survey of the State-of-the-Art  
> - A novel energy-aware resource management technique using joint VM and container consolidation approach for green computing in cloud data centers

---

#### üîπ **Consulta 5**

<p align="center">
  <img src="../../images/dorotea_consulta5.png" width="100%">
</p>

**Pregunta:** ¬øQu√© trabajos proponen nuevos algoritmos o m√©todos para mejorar la elasticidad o el escalado en la nube?

**Respuesta del agente:**

> "No specific information was found in the texts to answer the question about new algorithms or methods for improving elasticity or scaling in the cloud."  
>
> **Recommended Papers:**  
>
> - A New Infrastructure Elasticity Control Algorithm for Containerized Cloud  
> - Adaptive AI-based auto-scaling for Kubernetes  
> - Time series forecasting-based Kubernetes autoscaling using Facebook Prophet and Long Short-Term Memory


## Test usando OpenwebUI + Preguntas en ingl√©s

Una vez dentro de OpenwebUI, el primer paso es configurar el Pipe que se utilizar√°. Para ello, sigue los pasos explicados en el archivo [`01_implementacion-final.md`](./01_implementacion-final.md). La √∫nica diferencia en este caso fue el c√≥digo empleado para configurar el Pipe, que se encuentra en [`DOCUMENTACION/pps-kevin_haponiuk-2025/src/self-hosted-ai-starter-kit/n8n-pipeline-function-dorotea.py`].

<p align="center">
  <img src="../../images/dorotea-webui1.png" width="100%">
</p>

Las preguntas realizadas fueron las mismas que en pruebas anteriores, pero esta vez en ingl√©s.

**Which papers discuss Kubernetes or autoscaling in containers?**

<p align="center">
  <img src="../../images/dorotea-webui-answer1.png" width="100%">
</p>

**Provide references to papers that mention MLOps or machine learning pipelines.**

<p align="center">
  <img src="../../images/dorotea-webui-answer2-1.png" width="100%">
</p>

En este caso, se realiz√≥ la misma pregunta por segunda vez, ya que la primera respuesta fue algo imprecisa. Esto permiti√≥ observar c√≥mo se comporta el sistema al repetir una consulta.

<p align="center">
  <img src="../../images/dorotea-webui-answer2.png" width="100%">
</p>

**Which papers analyze energy efficiency or energy consumption in the cloud?**

<p align="center">
  <img src="../../images/dorotea-webui-answer3.png" width="100%">
</p>

**Which papers provide a review or state-of-the-art summary on container technologies?**

<p align="center">
  <img src="../../images/dorotea-webui-answer4.png" width="100%">
</p>

**Which papers propose new algorithms or methods to improve elasticity or scaling in the cloud?**

<p align="center">
  <img src="../../images/dorotea-webui-answer5.png" width="100%">
</p>

Es importante destacar que las respuestas del agente mejoraron significativamente al realizar las preguntas en ingl√©s. Esto se debe a que el modelo de Ollama est√° entrenado principalmente en ingl√©s, lo que optimiza la calidad y precisi√≥n de las respuestas.

## Conclusiones

1. **El pipeline completo oper√≥ de manera estable**, abarcando desde la extracci√≥n y an√°lisis de los papers hasta la indexaci√≥n vectorial y la consulta sem√°ntica.
   
2. **El rendimiento general fue satisfactorio**, aunque la carga inicial de datos result√≥ **lenta (‚âà2 horas para 10 documentos)**, indicando posibles cuellos de botella en el procesamiento de embeddings o la inserci√≥n en Qdrant.
   
3. **Las respuestas del agente fueron sem√°nticamente relevantes**, con sugerencias precisas y papers correctamente asociados, validando el correcto funcionamiento de los embeddings y la b√∫squeda vectorial en Qdrant. Adem√°s, se observ√≥ que el agente mejora significativamente sus respuestas cuando las preguntas se formulan en ingl√©s, debido a que el modelo de Ollama est√° entrenado principalmente en este idioma. Esto optimiza la calidad y precisi√≥n de las respuestas, haciendo que las consultas sean m√°s efectivas.
   
4. Se recomienda explorar optimizaciones en:
      * **Uso de GPU en Ollama**: mejorar la paralelizaci√≥n durante la generaci√≥n de embeddings.
      * **Segmentaci√≥n del proceso de carga**: dividir tareas para reducir tiempos.
      * **Modelos alternativos**: evaluar arquitecturas m√°s eficientes para embeddings masivos.

> La infraestructura desplegada en Dorotea demostr√≥ ser robusta, con capacidad de procesamiento adecuada y un rendimiento aceptable para pruebas de investigaci√≥n y desarrollo, as√≠ como para futuras escalas.

---

## Trabajo futuro

1. **Migrar a la Opci√≥n 1**: Consolidar los servicios detr√°s de un √∫nico puerto y 4 subpaths en NGINX para simplificar el acceso y mejorar la organizaci√≥n.

2. **Fortalecer la seguridad**:
      * Implementar autenticaci√≥n en NGINX o integrar un sistema de inicio de sesi√≥n √∫nico (SSO).
      * Configurar **TLS end-to-end** si se planea exponer los servicios m√°s all√° del t√∫nel SSH (actualmente suficiente para pruebas internas).

3. **Optimizar el pipeline**:
      * Analizar el uso de GPU en Ollama para maximizar la eficiencia.
      * Evaluar herramientas o t√©cnicas para acelerar la carga de datos en Qdrant.
      * Considerar modelos con mayor capacidad de contexto para mejorar las respuestas sem√°nticas.

4. **Monitoreo avanzado**:
      * Ampliar los dashboards en Grafana para incluir m√©tricas espec√≠ficas de GPU y rendimiento de embeddings.
      * Automatizar alertas para detectar posibles cuellos de botella o fallos en tiempo real.