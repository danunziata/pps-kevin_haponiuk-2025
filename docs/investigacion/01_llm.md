# ¬øC√≥mo funciona un Modelo de Lenguaje Generativo? üß†üí¨

Para poder desarrollar una soluci√≥n efectiva, es fundamental comprender qu√© ocurre detr√°s de escena cuando le hacemos una solicitud a una inteligencia artificial de texto generativo como puede ser ChatGPT.

En esta secci√≥n vamos a desglosar y explicar, de forma simple y conceptual, los elementos esenciales que conforman un modelo de lenguaje generativo.

---

## ¬øQu√© es Machine Learning?

El *Machine Learning* es una rama de la inteligencia artificial que permite a las computadoras **aprender de los datos** sin que sea necesario programarlas expl√≠citamente para cada tarea.

Una definici√≥n cl√°sica lo describe as√≠:

> *Machine Learning es el campo de estudio que da a las computadoras la capacidad de aprender sin ser expl√≠citamente programadas.*  
> ‚Äî Arthur Samuel, 1959

Otra, m√°s enfocada en su aplicaci√≥n, dice:

> *Un programa aprende de la experiencia (E), con respecto a una tarea (T) y una medida de rendimiento (P), si mejora su rendimiento en T, medido por P, gracias a E.*  
> ‚Äî Tom Mitchell, 1997

### Un ejemplo cotidiano

Tomemos como ejemplo el **filtro de spam**. Este sistema analiza miles de correos que los usuarios marcaron como spam o no spam, y con eso **aprende a detectar patrones** para clasificar correctamente nuevos correos.  
Aqu√≠:

- **Tarea (T):** Clasificar correos como spam o no spam.  
- **Experiencia (E):** Correos etiquetados por usuarios.  
- **Rendimiento (P):** Porcentaje de correos clasificados correctamente (precisi√≥n o *accuracy*).

Con suficiente experiencia, el sistema mejora su desempe√±o en la tarea, sin necesidad de reglas escritas por humanos.


### Tipos de aprendizaje en Machine Learning

- **Aprendizaje Supervisado**: Es como aprender a tocar el piano con un profesor. El algoritmo se entrena con datos etiquetados, es decir, con las respuestas correctas ya conocidas. Aprende comparando sus resultados con las respuestas reales y ajustando su comportamiento. Se utiliza, por ejemplo, en sistemas que filtran correos spam.

- **Aprendizaje No Supervisado**: Similar a armar un rompecabezas sin saber c√≥mo deber√≠a verse al final. No se proporcionan etiquetas ni respuestas, y el algoritmo debe descubrir patrones o agrupaciones por s√≠ mismo. Un uso com√∫n es la segmentaci√≥n de clientes en marketing.

- **Aprendizaje por Refuerzo**: Como entrenar a una mascota con premios y castigos. El algoritmo aprende a trav√©s de prueba y error, recibiendo recompensas por buenas acciones y penalizaciones por errores. Es muy usado en videojuegos y rob√≥tica, donde el sistema mejora con la experiencia.

<p align="center">
  <img src="../../images/type_ml.webp" width="100%">
  <br>
  <em>Figura 1: Tipos de aprendizajes en machine learning</em>
</p>

## ¬øQu√© es una red neuronal?

Las redes neuronales artificiales (*Artificial Neural Networks*, o ANN) son **modelos matem√°ticos inspirados en el funcionamiento del cerebro humano**.
Por medio de una colecci√≥n de unidades llamadas *neuronas artificiales*, organizadas en capas y conectadas entre s√≠, que **aprende a realizar tareas complejas mediante la experiencia**, es decir, ajustando sus par√°metros a partir de los datos de entrenamiento. Gracias a esta capacidad, pueden realizar tareas como clasificar im√°genes, reconocer voz o jugar videojuegos, todo esto **aprendiendo a partir de datos**.

### Inspiraci√≥n biol√≥gica

Una **neurona biol√≥gica** es una c√©lula del cerebro que recibe se√±ales de otras neuronas a trav√©s de estructuras llamadas *dendritas*, procesa esa informaci√≥n en el cuerpo celular, y transmite se√±ales hacia otras neuronas a trav√©s del *ax√≥n*. Cuando recibe suficientes est√≠mulos, la neurona "dispara" una se√±al el√©ctrica.

<p align="center">
  <img src="../../images/neurona.png"  width="80%">
  <br>
  <em>Figura 2: Una neurona biol√≥gica</em>
</p>

<p align="center">
  <img src="../../images/red_neuronal_humana.png" width="80%">
  <br>
  <em>Figura 3: Representaci√≥n de una red neuronal humana</em>
</p>

De forma an√°loga, una **neurona artificial** recibe m√∫ltiples entradas num√©ricas (como los datos de una imagen o un texto), las combina mediante una funci√≥n matem√°tica y produce una salida. Las conexiones entre neuronas se representan con pesos, que determinan la importancia de cada entrada. Al entrenar la red, estos pesos se ajustan para mejorar el desempe√±o del modelo.


<p align="center">
  <img src="../../images/red_neuronal.png" width="60%">
  <br>
  <em>Figura 4: Representaci√≥n conceptual de una red neuronal artificial </em>
</p>

### Un poco de historia

Las primeras redes neuronales fueron propuestas en 1943 por **McCulloch y Pitts**, quienes crearon un modelo muy simplificado de neurona artificial capaz de realizar c√°lculos l√≥gicos (como AND, OR y NOT). Este fue el punto de partida para una larga evoluci√≥n.

Durante d√©cadas, las redes neuronales pasaron por varios ciclos de entusiasmo y olvido, hasta que en la √∫ltima d√©cada resurgieron con fuerza, gracias a:

- la disponibilidad de **grandes cantidades de datos** (Big Data),
- el aumento exponencial de la **capacidad de c√≥mputo** (especialmente con GPUs y TPUs),
- y las mejoras en los **algoritmos de entrenamiento** (como el descenso por gradiente y sus variantes).

Todo esto permiti√≥ entrenar redes neuronales m√°s profundas y complejas, dando lugar al auge de ia que hoy presenciamos.

## ¬øQu√© es un LLM?

Un **LLM** (*Large Language Model*, o *Modelo de Lenguaje Grande*) es un tipo de modelo de inteligencia artificial entrenado para comprender, generar y trabajar con lenguaje natural de manera fluida. Estos modelos est√°n basados en redes neuronales profundas, especialmente en arquitecturas conocidas como **transformers** (explicado m√°s adelante), que le permite procesar y generar texto a gran escala.

Lo que hace especial a un LLM es la enorme cantidad de datos con los que ha sido entrenado (normalmente textos provenientes de libros, art√≠culos, p√°ginas web y otros contenidos p√∫blicos) y la gran cantidad de par√°metros que posee, muchas veces llegando a **miles de millones de conexiones**. Gracias a esto, los LLM pueden:

- Redactar textos coherentes y contextuales.
- Traducir entre distintos idiomas.
- Resumir documentos extensos.
- Responder preguntas de forma informada.
- Asistir en programaci√≥n, an√°lisis de datos, y mucho m√°s.


## ¬øQu√© es el lenguaje natural y c√≥mo puede interpretarlo la IA?

El **lenguaje natural** es la forma de comunicaci√≥n que usamos los seres humanos en nuestra vida cotidiana, como el espa√±ol, ingl√©s, franc√©s, etc. Se caracteriza por ser **rico, flexible y ambiguo**, y est√° lleno de matices culturales, contextuales y emocionales.

En el √°mbito de la computaci√≥n, cuando se habla de **procesamiento de lenguaje natural (NLP "Natural Language Processing", por sus siglas en ingl√©s)**, se hace referencia a la capacidad de una m√°quina para **entender, interpretar, generar y responder** a textos o conversaciones escritos u orales del mismo modo en que lo har√≠a una persona.

Cuando escribimos un texto como:

```
"El gato duerme en el sof√°"
```

Este texto pasa por varios pasos para que la IA lo entienda:

### 1. Codificaci√≥n num√©rica

Lo primero que debemos hacer es **convertir las palabras en un formato que la computadora pueda entender**, como por ejemplo una codificaci√≥n num√©rica. Una forma sencilla de hacerlo es utilizando c√≥digos como **ASCII**, donde a cada car√°cter se le asigna un n√∫mero espec√≠fico.


<p align="center">
  <img src="../../images/codificacion.png" width="80%">
  <br>
  <em>Figura 5: Proceso de codificaci√≥n</em>
</p>

### 2. Tokenizaci√≥n

A nuestra red neuronal la **entrenamos aliment√°ndola con una gran cantidad de datos ya codificados**. A medida que procesa esta informaci√≥n, **aprende a reconocer patrones repetitivos** en el lenguaje. Estos patrones, conocidos como **tokens**, se van organizando y catalogando en una especie de vocabulario interno o lista, que luego usar√° para comprender y generar texto.


<p align="center">
  <img src="../../images/patrones.png" width="80%">
  <br>
  <em>Figura 6: Red neuronal encontrando patrones</em>
</p>


> üîç **Aclaraci√≥n:** un *token* no es necesariamente una palabra. Puede ser una palabra completa, una parte de una palabra, o incluso un conjunto de palabras o s√≠mbolos, dependiendo del modelo y del sistema de tokenizaci√≥n utilizado.


### 3. Compresi√≥n

Una vez que nuestra red neuronal ha identificado los patrones en los datos, **el siguiente paso es representar cada token mediante un n√∫mero o identificador √∫nico (ID)**. Esta conversi√≥n permite **simplificar el procesamiento y optimizar el almacenamiento**, ya que trabajar con n√∫meros es mucho m√°s eficiente que hacerlo directamente con texto.

Por ejemplo:

| Token      |Token completo   |  ID  |
|------------|-----------------|------|
| hola       | 72 111 108 97   | 1923 |
| hace       | 104 97 99 101   | 1234 |
| fr√≠o       | 102 114 237 111 | 6543 |

De esta forma, cuando la red procesa una frase como "hola hace fr√≠o", en realidad est√° trabajando internamente con una secuencia de n√∫meros: `1923, 1234, 6543`.


<p align="center">
  <img src="../../images/id-token.png" width="80%">
  <br>
  <em>Figura 7: Representaci√≥n mediante ID</em>
</p>


### 4. Marcadores / Embedding

Es importante entender que **el modelo no "sabe" lo que significa cada palabra** como lo har√≠a una persona. No tiene una comprensi√≥n sem√°ntica real de los t√©rminos. Sin embargo, **s√≠ puede identificar qu√© tokens est√°n relacionados entre s√≠** gracias a c√≥mo se organizan en el espacio de embedding.

Por ejemplo, puede aprender que el token `"la"` aparece frecuentemente cerca de `"reina"`, y por eso los ubica **cerca en el espacio vectorial**. Esa proximidad representa una relaci√≥n estad√≠stica, no un entendimiento real del lenguaje.

Esta capacidad de asociar t√©rminos seg√∫n su contexto permite que el modelo genere texto coherente y mantenga el sentido general, aunque no comprenda el significado como lo har√≠amos nosotros.


<p align="center">
  <img src="../../images/la_reina.png" width="80%">
  <br>
  <em>Figura 8: Ejemplo de relaci√≥n </em>
</p>

De esta manera, puedo encontrar relaciones entre los token y agregar una marca en cada ocasi√≥n que tengan relaci√≥n entre ellos:

<p align="center">
  <img src="../../images/relacion_token.png" width="60%">
  <br>
  <em>Figura 9: marcaci√≥n entre tokens </em>
</p>

la idea es seguir sumando marcadores entre distintos token, de esta manera el modelo aunque no sabe exactamente qu√© significan, s√≠ saber como clasificarlos.

<p align="center">
  <img src="../../images/relacion_token2.png" width="70%">
  <br>
  <em>Figura x: Relaci√≥n entre tokens </em>
</p>

Esto se puede extender hasta **cientos o miles de marcadores** (dimensiones), formando lo que podr√≠amos imaginar como un tipo de **ADN de la palabra** (o del token, en general).

Cada token se representa mediante un **vector num√©rico de gran dimensi√≥n**, donde **cada posici√≥n del vector captura una caracter√≠stica o relaci√≥n aprendida del contexto**. As√≠, dos tokens con significados o usos similares tendr√°n vectores parecidos.

> üìå Nota: aunque en este caso hablamos de "token = palabra", un token tambi√©n puede ser parte de una palabra o incluso una secuencia de palabras, seg√∫n el modelo de tokenizaci√≥n utilizado.


<p align="center">
  <img src="../../images/relacion_token3.png" width="80%">
  <br>
  <em>Figura 10: Relaci√≥n entre tokens </em>
</p>

Otra forma de representar estos vectores es imaginarlos en un **espacio tridimensional**. En la pr√°ctica, cada marcador corresponde a una dimensi√≥n, por lo que los vectores pueden tener **cientos o miles de dimensiones**. Sin embargo, al proyectarlos en 3D podemos visualizar mejor la **relaci√≥n de cercan√≠a entre tokens**.

Esta idea de "cercan√≠a sem√°ntica" nos permite ver, por ejemplo, que tokens como `"rey"` y `"reina"` est√°n m√°s cerca entre s√≠ que de `"auto"`.

>  Esta representaci√≥n permite entender c√≥mo el modelo "comprende" las relaciones entre conceptos sin saber realmente qu√© significan.

 El siguiente ejemplo fue obtenido de [esta herramienta de visualizaci√≥n de embeddings de TensorFlow](https://projector.tensorflow.org/), donde pod√©s explorar estas relaciones en 2D o 3D de forma interactiva.


<p align="center">
  <img src="../../images/embedding_argentina.png" width="100%">
  <br>
  <em>Figura 11: Representaci√≥n tridimensional embedding </em>
</p>

En este caso, podemos ver que palabras cercanas a "argentina" son:

<p align="center">
  <img src="../../images/puntos_arg.png" width="50%">
  <br>
  <em>Figura 12: Puntos m√°s cercanos a la palabra "argentina" </em>
</p>

Esto permite realizar operaciones matem√°ticas entre vectores que representan palabras, tambi√©n conocidos como *embeddings*. Gracias a esta propiedad, se pueden hacer analog√≠as sem√°nticas, como por ejemplo:

- **"CORONA" + "HOMBRE" ‚âà "REY"**
- **"REY" - "HOMBRE" + "MUJER" ‚âà "REINA"**

Estos c√°lculos se basan en relaciones sem√°nticas aprendidas por el modelo a partir de grandes cantidades de texto. As√≠, los embeddings no solo capturan el significado de las palabras, sino tambi√©n sus relaciones contextuales.

### 5. Normalizaci√≥n y Lematizaci√≥n

Cuando trabajamos con texto, especialmente al darle instrucciones a una IA, hay muchos t√©rminos que no aportan valor significativo. Palabras como **"la", "lo", "el", "y", "con"**, o incluso los **signos de puntuaci√≥n**, suelen ser irrelevantes para el an√°lisis, ya que aparecen en casi todos los contextos y no cambian el significado central de una frase.

Para simplificar y optimizar el procesamiento, lo primero que hacemos es **limpiar** los textos antes de pasarlos al sistema. Veamos un ejemplo con la frase:

> **"El gato est√° durmiendo en el sof√°."**

1. **Frase original:**  
   `"El gato est√° durmiendo en el sof√°."`

2. **Eliminaci√≥n de palabras irrelevantes y signos de puntuaci√≥n:**  
   `"gato est√° durmiendo sof√°"`

3. **Lematizaci√≥n (reducci√≥n de las palabras a su forma base):**  
   `"gato es dormir sof√°"`

4. **Tokenizaci√≥n (divisi√≥n en unidades b√°sicas):**  
   `"gato", "dormir", "sof√°"`

Esta frase queda **comprimida** y m√°s eficiente para el procesamiento por parte del modelo. En lugar de trabajar con muchas palabras y tokens innecesarios, esta versi√≥n reduce el texto a su **m√≠nima expresi√≥n √∫til**, acelerando la b√∫squeda y el an√°lisis dentro del modelo.

Modelos como ChatGPT **preparan internamente nuestra entrada** de esta forma para compararla con representaciones (embeddings) similares y generar una respuesta adecuada. Este proceso implica una **p√©rdida de informaci√≥n**, ya que comprimimos el mensaje original. Sin embargo, esto no es algo negativo: la IA no repite textualmente, sino que genera una **respuesta original** basada en la comprensi√≥n del contenido.

> Un buen ejemplo ser√≠a escuchar una clase y al d√≠a siguiente explicarle el concepto a alguien m√°s: no recordamos todas las palabras exactas, pero s√≠ entendemos y transmitimos la idea principal.

### 6. Sampling

El *sampling* (muestreo) es una t√©cnica que permite al modelo generar respuestas de forma m√°s **variada y creativa**, introduciendo un **componente aleatorio** en el proceso de generaci√≥n de texto.

En lugar de elegir siempre la palabra m√°s probable seg√∫n el modelo, el sampling permite explorar otras opciones que, aunque sean ligeramente menos probables, siguen siendo **coherentes** dentro del contexto. Es como moverse ligeramente dentro del **espacio de embeddings**, eligiendo caminos diferentes que llevan a respuestas distintas pero igualmente v√°lidas.

> Esto evita que el modelo siempre d√© las mismas respuestas ante los mismos inputs y permite generar resultados m√°s **naturales y diversos**, sin perder el sentido general de la conversaci√≥n.

## Evoluci√≥n de las Redes Neuronales

Las redes neuronales tradicionales tienen una gran limitaci√≥n: **no tienen memoria**. Es decir, procesan cada entrada (input) de manera aislada, sin recordar lo que pas√≥ antes. Pero, ¬øc√≥mo puede una red entender el contexto de una conversaci√≥n o el significado completo de una frase si solo ve una palabra a la vez?

### Redes Neuronales Recurrentes (RNN)

Para resolver este problema apareci√≥ una arquitectura llamada **Red Neuronal Recurrente** (RNN), que introduce la idea de "memoria" al alimentar el resultado de un paso como entrada del siguiente.

- Procesan palabra por palabra en **secuencia**.
- Cada palabra se analiza considerando lo anterior.
- Problema: **pierden contexto** cuando el texto es muy largo.
- No permiten **paralelizar**, por lo que el entrenamiento es m√°s lento.

<p align="center">
  <img src="../../images/rnn.png" width="20%">
  <br>
  <em>Figura 13: Ejemplo de RNN </em>
</p>

### LSTM (Long Short-Term Memory)

Las **LSTM** son una mejora de las RNN tradicionales. Introducen una especie de "filtro de memoria" que decide qu√© informaci√≥n mantener y cu√°l olvidar.

- Manejan mejor el contexto de largo plazo.
- Son m√°s precisas que las RNN simples.
- Aun as√≠, siguen siendo **secuenciales** y **no paralelizables**, lo que limita su eficiencia.


## El Gran Cambio: Transformers

El gran salto lleg√≥ en 2017 con el paper [**‚ÄúAttention is All You Need‚Äù**](https://arxiv.org/abs/1706.03762), que introdujo el modelo **Transformer**. Esta arquitectura cambi√≥ por completo la forma en que las redes procesan secuencias.

### ¬øQu√© aportan los Transformers?

- Utilizan un mecanismo llamado **Self-Attention** para identificar la importancia de cada palabra en relaci√≥n con todas las dem√°s, **en paralelo**.
- Permiten analizar **todas las palabras al mismo tiempo**, no de forma secuencial.
- Esto facilita el entrenamiento y mejora la comprensi√≥n del **contexto completo**.
- Son **altamente paralelizables**, lo que acelera enormemente el proceso.

### Ejemplo con Self-Attention

En la frase:

> ‚ÄúLa vida es bella, as√≠ que v√≠vela cada d√≠a.‚Äù

El modelo analiza cada palabra en relaci√≥n con todas las dem√°s. Por ejemplo:

- "la" con respecto a "vida", "bella", "v√≠vela", etc.
- "vida" con respecto a todas las dem√°s tambi√©n.

<p align="center">
  <img src="../../images/ejemplo_atencion.png" width="100%">
  <br>
  <em>Figura 14: Ejemplo de uso de transformers </em>
</p>

<p align="center">
  <img src="../../images/paralelizacion.png" width="100%">
  <br>
  <em>Figura 15: Ejemplo de paralelizaci√≥n </em>
</p>

Esto permite entender cu√°les son las palabras m√°s importantes para construir el significado global de la frase, incluso si est√°n separadas por otras palabras.

Gracias a los Transformers, hoy podemos contar con modelos como GPT, BERT y otros LLM (Large Language Models) que comprenden mejor el lenguaje y generan respuestas m√°s coherentes, r√°pidas y √∫tiles.


---


Bibliograf√≠a:

https://www.youtube.com/watch?v=FdZ8LKiJBhQ&list=PL3ei_Xb7-ic5pkJDTplPxWvE8t13mm19W&index=8
https://www.ntiva.com/blog/what-is-machine-learning
Aur√©lien G√©ron - Hands-On Machine Learning with Scikit-Learn, Keras and TensorFlow
http://personal.cimat.mx:8181/~mrivera/cursos/aprendizaje_profundo/RNN_LTSM/introduccion_rnn.html
https://visajourneypro.com/
https://projector.tensorflow.org/
https://arxiv.org/abs/1706.03762