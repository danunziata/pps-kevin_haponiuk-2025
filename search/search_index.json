{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Agente de Inteligencia Artificial para Investigaci\u00f3n Cient\u00edfica El proyecto busca dise\u00f1ar e implementar un agente de inteligencia artificial (IA) para la recomendaci\u00f3n y asistencia en investigaci\u00f3n cient\u00edfica, utilizando herramientas de c\u00f3digo abierto que permitan optimizar la b\u00fasqueda, clasificaci\u00f3n y an\u00e1lisis de literatura cient\u00edfica. Para su desarrollo, se relevar\u00e1n tanto herramientas tecnol\u00f3gicas como metodolog\u00edas y estrategias utilizadas en los procesos de investigaci\u00f3n con el fin de dise\u00f1ar los flujos de trabajo (pipelines) adecuados. Este agente servir\u00e1 como una herramienta clave en el \u00e1mbito acad\u00e9mico, facilitando el acceso, la organizaci\u00f3n del conocimiento y la generaci\u00f3n de recomendaciones personalizadas. Fundamentos La implementaci\u00f3n de un sistema de recomendaciones basado en inteligencia artificial representa una oportunidad estrat\u00e9gica para modernizar la investigaci\u00f3n cient\u00edfica, optimizar procesos y mejorar la interacci\u00f3n de los investigadores con la literatura acad\u00e9mica. Este proyecto se centra en tres aspectos clave: Consulta inteligente de documentaci\u00f3n cient\u00edfica para investigadores. Relevamiento de metodolog\u00edas y estrategias utilizadas en procesos de investigaci\u00f3n para definir los pipelines del agente de IA. Un agente de IA que automatice la identificaci\u00f3n de art\u00edculos relevantes, tendencias y conexiones entre investigaciones. El acceso r\u00e1pido y preciso a la informaci\u00f3n es esencial para la investigaci\u00f3n cient\u00edfica. Sin embargo, el volumen y la complejidad de la literatura acad\u00e9mica dificultan la b\u00fasqueda y selecci\u00f3n de material relevante. Un agente inteligente de IA permitir\u00e1 la consulta r\u00e1pida y precisa de publicaciones cient\u00edficas, mejorando la toma de decisiones en el desarrollo de nuevos estudios y optimizando el tiempo invertido en revisiones bibliogr\u00e1ficas. Adem\u00e1s, su dise\u00f1o estar\u00e1 basado en el relevamiento de metodolog\u00edas utilizadas en los procesos de investigaci\u00f3n, asegurando que los pipelines dise\u00f1ados reflejen las necesidades reales de los investigadores. Este sistema automatizado permitir\u00e1 mejorar la eficiencia operativa mediante la reducci\u00f3n de tiempos de b\u00fasqueda y la organizaci\u00f3n inteligente del conocimiento. Tambi\u00e9n contribuir\u00e1 a una mayor eficiencia en la planificaci\u00f3n de nuevas investigaciones al identificar vac\u00edos en la literatura y conectar estudios relevantes de manera automatizada. La implementaci\u00f3n de este agente de IA abre nuevas oportunidades significativas. En primer lugar, busca mejorar la productividad de los investigadores al facilitar el acceso a informaci\u00f3n clave sin necesidad de b\u00fasquedas manuales extensivas. En segundo lugar, fomentar\u00e1 la colaboraci\u00f3n interdisciplinaria al sugerir trabajos relacionados en diversas \u00e1reas del conocimiento. Adem\u00e1s, un sistema accesible y eficiente incentivar\u00e1 una mayor integraci\u00f3n de la inteligencia artificial en el \u00e1mbito cient\u00edfico, promoviendo la innovaci\u00f3n en metodolog\u00edas de investigaci\u00f3n. Finalmente, se contempla que el sistema tenga un enfoque de dise\u00f1o escalable y adaptable a futuras necesidades, asegurando su relevancia a largo plazo. Objetivos Generales Desarrollar un agente de inteligencia artificial que asista a investigadores en la revisi\u00f3n bibliogr\u00e1fica y generaci\u00f3n de recomendaciones cient\u00edficas, optimizando la b\u00fasqueda, selecci\u00f3n y an\u00e1lisis de literatura acad\u00e9mica. Objetivos Espec\u00edficos Realizar un relevamiento de herramientas de c\u00f3digo abierto para evaluar su aplicabilidad en el procesamiento de informaci\u00f3n cient\u00edfica y la automatizaci\u00f3n de flujos de trabajo. Analizar metodolog\u00edas y estrategias utilizadas en procesos de investigaci\u00f3n, identificando los requerimientos clave para dise\u00f1ar pipelines de automatizaci\u00f3n eficientes. Definir la arquitectura del agente de IA, estableciendo modelos de procesamiento de lenguaje natural, flujos de datos y esquemas de integraci\u00f3n con bases de datos cient\u00edficas. Desarrollar y entrenar al agente, optimizando su capacidad de extracci\u00f3n, clasificaci\u00f3n y recomendaci\u00f3n de informaci\u00f3n acad\u00e9mica. Implementar estrategias de automatizaci\u00f3n de tareas, asegurando una integraci\u00f3n fluida con herramientas de gesti\u00f3n de informaci\u00f3n. Evaluar el desempe\u00f1o del sistema, aplicando m\u00e9tricas de precisi\u00f3n, recall y relevancia en la recomendaci\u00f3n de contenido cient\u00edfico. Dise\u00f1ar una interfaz intuitiva, que permita a los investigadores interactuar de manera eficiente con el agente. Documentar el proceso de desarrollo e implementaci\u00f3n, incluyendo configuraciones, pruebas y resultados.","title":"Introducci\u00f3n"},{"location":"#agente-de-inteligencia-artificial-para-investigacion-cientifica","text":"El proyecto busca dise\u00f1ar e implementar un agente de inteligencia artificial (IA) para la recomendaci\u00f3n y asistencia en investigaci\u00f3n cient\u00edfica, utilizando herramientas de c\u00f3digo abierto que permitan optimizar la b\u00fasqueda, clasificaci\u00f3n y an\u00e1lisis de literatura cient\u00edfica. Para su desarrollo, se relevar\u00e1n tanto herramientas tecnol\u00f3gicas como metodolog\u00edas y estrategias utilizadas en los procesos de investigaci\u00f3n con el fin de dise\u00f1ar los flujos de trabajo (pipelines) adecuados. Este agente servir\u00e1 como una herramienta clave en el \u00e1mbito acad\u00e9mico, facilitando el acceso, la organizaci\u00f3n del conocimiento y la generaci\u00f3n de recomendaciones personalizadas.","title":"Agente de Inteligencia Artificial para Investigaci\u00f3n Cient\u00edfica"},{"location":"#fundamentos","text":"La implementaci\u00f3n de un sistema de recomendaciones basado en inteligencia artificial representa una oportunidad estrat\u00e9gica para modernizar la investigaci\u00f3n cient\u00edfica, optimizar procesos y mejorar la interacci\u00f3n de los investigadores con la literatura acad\u00e9mica. Este proyecto se centra en tres aspectos clave: Consulta inteligente de documentaci\u00f3n cient\u00edfica para investigadores. Relevamiento de metodolog\u00edas y estrategias utilizadas en procesos de investigaci\u00f3n para definir los pipelines del agente de IA. Un agente de IA que automatice la identificaci\u00f3n de art\u00edculos relevantes, tendencias y conexiones entre investigaciones. El acceso r\u00e1pido y preciso a la informaci\u00f3n es esencial para la investigaci\u00f3n cient\u00edfica. Sin embargo, el volumen y la complejidad de la literatura acad\u00e9mica dificultan la b\u00fasqueda y selecci\u00f3n de material relevante. Un agente inteligente de IA permitir\u00e1 la consulta r\u00e1pida y precisa de publicaciones cient\u00edficas, mejorando la toma de decisiones en el desarrollo de nuevos estudios y optimizando el tiempo invertido en revisiones bibliogr\u00e1ficas. Adem\u00e1s, su dise\u00f1o estar\u00e1 basado en el relevamiento de metodolog\u00edas utilizadas en los procesos de investigaci\u00f3n, asegurando que los pipelines dise\u00f1ados reflejen las necesidades reales de los investigadores. Este sistema automatizado permitir\u00e1 mejorar la eficiencia operativa mediante la reducci\u00f3n de tiempos de b\u00fasqueda y la organizaci\u00f3n inteligente del conocimiento. Tambi\u00e9n contribuir\u00e1 a una mayor eficiencia en la planificaci\u00f3n de nuevas investigaciones al identificar vac\u00edos en la literatura y conectar estudios relevantes de manera automatizada. La implementaci\u00f3n de este agente de IA abre nuevas oportunidades significativas. En primer lugar, busca mejorar la productividad de los investigadores al facilitar el acceso a informaci\u00f3n clave sin necesidad de b\u00fasquedas manuales extensivas. En segundo lugar, fomentar\u00e1 la colaboraci\u00f3n interdisciplinaria al sugerir trabajos relacionados en diversas \u00e1reas del conocimiento. Adem\u00e1s, un sistema accesible y eficiente incentivar\u00e1 una mayor integraci\u00f3n de la inteligencia artificial en el \u00e1mbito cient\u00edfico, promoviendo la innovaci\u00f3n en metodolog\u00edas de investigaci\u00f3n. Finalmente, se contempla que el sistema tenga un enfoque de dise\u00f1o escalable y adaptable a futuras necesidades, asegurando su relevancia a largo plazo.","title":"Fundamentos"},{"location":"#objetivos-generales","text":"Desarrollar un agente de inteligencia artificial que asista a investigadores en la revisi\u00f3n bibliogr\u00e1fica y generaci\u00f3n de recomendaciones cient\u00edficas, optimizando la b\u00fasqueda, selecci\u00f3n y an\u00e1lisis de literatura acad\u00e9mica.","title":"Objetivos Generales"},{"location":"#objetivos-especificos","text":"Realizar un relevamiento de herramientas de c\u00f3digo abierto para evaluar su aplicabilidad en el procesamiento de informaci\u00f3n cient\u00edfica y la automatizaci\u00f3n de flujos de trabajo. Analizar metodolog\u00edas y estrategias utilizadas en procesos de investigaci\u00f3n, identificando los requerimientos clave para dise\u00f1ar pipelines de automatizaci\u00f3n eficientes. Definir la arquitectura del agente de IA, estableciendo modelos de procesamiento de lenguaje natural, flujos de datos y esquemas de integraci\u00f3n con bases de datos cient\u00edficas. Desarrollar y entrenar al agente, optimizando su capacidad de extracci\u00f3n, clasificaci\u00f3n y recomendaci\u00f3n de informaci\u00f3n acad\u00e9mica. Implementar estrategias de automatizaci\u00f3n de tareas, asegurando una integraci\u00f3n fluida con herramientas de gesti\u00f3n de informaci\u00f3n. Evaluar el desempe\u00f1o del sistema, aplicando m\u00e9tricas de precisi\u00f3n, recall y relevancia en la recomendaci\u00f3n de contenido cient\u00edfico. Dise\u00f1ar una interfaz intuitiva, que permita a los investigadores interactuar de manera eficiente con el agente. Documentar el proceso de desarrollo e implementaci\u00f3n, incluyendo configuraciones, pruebas y resultados.","title":"Objetivos Espec\u00edficos"},{"location":"implementacion/00_workflow-completo/","text":"Explicaci\u00f3n del Workflow del Proyecto Figura 1: Workflow Completo \ud83d\udfe1 Procesamiento y Carga de Informaci\u00f3n | \ud83d\udd35 Agent AI (primer versi\u00f3n) | \ud83d\udfe2 Agent AI (versi\u00f3n final) En esta secci\u00f3n se va a explicar el funcionamiento completo del workflow del proyecto. Est\u00e1 estructurado en dos grandes secciones principales: la preparaci\u00f3n de la base de datos (bloque amarillo) donde se almacena toda la informaci\u00f3n extra\u00edda de los papers, y la implementaci\u00f3n del agente que realiza las b\u00fasquedas. Se presentan dos tipos de agentes: el bloque azul representa una primera versi\u00f3n que actualmente no funciona correctamente, aunque podr\u00eda ser funcional con futuras actualizaciones. Para m\u00e1s informaci\u00f3n sobre esta problem\u00e1tica, se puede consultar la documentaci\u00f3n en esta secci\u00f3n , que ayuda a comprender por qu\u00e9 se desarroll\u00f3 una alternativa m\u00e1s manual que es la que finalmente se utiliza en el proyecto (bloque verde). Procesamiento y Carga de Informaci\u00f3n Figura 2: Workflow de Procesamiento y Carga de Informaci\u00f3n El flujo comienza con el nodo When clicking 'Test workflow' que act\u00faa como disparador/trigger del workflow. Desde all\u00ed, el proceso contin\u00faa hacia el nodo Search all Content donde se debe seleccionar manualmente la carpeta donde se realizar\u00e1 la b\u00fasqueda. Este nodo genera como salida un listado completo de todos los elementos contenidos en la carpeta de Drive. Posteriormente, el flujo pasa al nodo Get Content que se encarga de descargar los archivos. Para procesar todos los archivos listados anteriormente, se configura con un identificador \"{{ $json.id }}\" que var\u00eda iterativamente hasta cubrir todos los elementos listados y los descarga. A continuaci\u00f3n, el proceso contin\u00faa hacia el nodo Write Files to input_papers . El objetivo es transferir todos los archivos descargados desde n8n para poder procesarlos con GROBID de manera local. Existe una carpeta compartida entre el contenedor y la PC local, definida en el docker-compose: volumes: - ./shared:/data/shared Por lo tanto, todos los archivos se descargan en el directorio '/data/shared/input_papers/{{ $json.name }}'. Luego, el nodo Execute GROBID (local) ejecuta el comando 'python3 /data/shared/cliente_grobid.py', que convierte todos los archivos PDF a formato XML y los ubica en la carpeta 'output_papers'. Una vez completado el procesamiento, el flujo contin\u00faa hacia Read Files from output_papers . Despu\u00e9s, el proceso pasa al nodo Extract from XML que est\u00e1 configurado para extraer campos comunes de todos los papers y convertirlos a formato JSON. Al estar bien estructurado gracias a la implementaci\u00f3n de GROBID, permite visualizar toda la estructura que mantienen los papers. Luego se realiza un proceso de formateo en el nodo Information extraction donde se mapean las variables que se utilizar\u00e1n para cargar en la base de datos, incluyendo informaci\u00f3n importante y metadata \u00fatil. Las variables extra\u00eddas son: title (metadata) abstract (metadata) publisher (metadata) year published (metadata) DOI (metadata) keywords (metadata) BODY_div (campo que contiene la informaci\u00f3n principal) BODY_fig (informaci\u00f3n de todas las figuras y sus descripciones) A continuaci\u00f3n, el flujo contin\u00faa hacia el nodo Information union que combina los dos campos mencionados anteriormente en un solo lugar, uniendo BODY_div con BODY_fig para formar INFO_BODY. Posteriormente, el proceso pasa al nodo Split Chunks , que consiste en un c\u00f3digo JavaScript que divide un texto extenso en fragmentos (chunks) m\u00e1s peque\u00f1os, respetando los l\u00edmites de las oraciones para mantener la coherencia del contenido. Luego, el nodo Separe List se encarga \u00fanicamente de separar una lista en varios elementos individuales. Finalmente, se ingresa a la secci\u00f3n final de este proceso, donde se encuentra un nodo Loop que permite iterar sobre todos los elementos, procesando 5 chunks a la vez. El flujo contin\u00faa hacia Create context , que es un nodo b\u00e1sico de LLM conectado a un modelo de Ollama (en este caso \"llama3.1:8b\"). En este proceso (como se explic\u00f3 en Contextual RAG ), se proporciona el documento completo \"INFO_BODY\" para generar un contexto espec\u00edfico para cada chunk basado en dicho documento. Una vez obtenido el contexto junto con el chunk original, ambos se cargan en la base de datos mediante el nodo Qdrant Vector Store , que est\u00e1 conectado a otros dos nodos: Embeddings Ollama , donde se define un modelo de Embedding (en este caso \"nomic-embed-text\"). Es importante destacar que se debe utilizar este mismo modelo al realizar b\u00fasquedas posteriores en la base de datos. El segundo nodo conectado es Default Data Loader , donde se especifica c\u00f3mo se cargar\u00e1 la informaci\u00f3n en la base de datos (incluyendo la metadata en este punto). Tambi\u00e9n tiene configurado un text splitter sin overlap, con una distancia de 5000 chunks size. Dado que anteriormente se realiz\u00f3 un split manual con un tama\u00f1o de 1000 al cargar en la base de datos (sumado a un contexto no muy extenso), nunca se alcanza los 5000, por lo que no se realiza un split adicional en esta zona, que es el comportamiento deseado. Finalmente, este proceso se repite en loop hasta completar todos los chunks. Una vez completado todo el proceso de procesamiento y carga, es posible verificar que los datos se han almacenado correctamente en la base de datos vectorial. Para ello, se puede acceder al panel de administraci\u00f3n de Qdrant a trav\u00e9s de la siguiente URL: http://localhost:6333/dashboard#/collections/ Al seleccionar la colecci\u00f3n creada, se podr\u00e1 visualizar la estructura de los datos almacenados, como se muestra en la siguiente imagen: Figura 3: Estructura de datos en Qdrant Vector Store En la visualizaci\u00f3n de Qdrant se pueden identificar tres componentes principales: \ud83d\udd34 Chunk (Rojo) : Contiene el fragmento original del documento extra\u00eddo durante el proceso de chunking \ud83d\udfe1 Contexto (Amarillo) : Representa el contexto generado por el modelo LLM que sit\u00faa el chunk dentro del documento completo \ud83d\udfe2 Metadata (Verde) : Incluye toda la informaci\u00f3n descriptiva del documento como t\u00edtulo, abstract, DOI, keywords, a\u00f1o de publicaci\u00f3n, etc. Esta estructura permite que el sistema de b\u00fasqueda sem\u00e1ntica pueda recuperar no solo el contenido relevante, sino tambi\u00e9n el contexto que lo hace comprensible y la metadata que facilita la identificaci\u00f3n del documento fuente. Agent AI Figura 4: Workflow completo del Agent AI En esta secci\u00f3n se explica el funcionamiento del Agente de Inteligencia Artificial. El sistema ofrece dos formas de interactuar con el agente: Puntos de Entrada When chat message received : Nodo nativo de n8n que permite interactuar con el agente directamente desde la interfaz de n8n para realizar pruebas de manera r\u00e1pida. Webhook : Nodo que espera solicitudes mediante m\u00e9todo POST para ser utilizado con OpenWebUI como interfaz final para el usuario. Figura 5: Puntos de entrada del Agent AI Ambos nodos se conectan al nodo Chat and session Id , que realiza un mapeo de variables para estandarizar la entrada, asegurando que los nodos posteriores ( Reference? y Query optimization ) reciban el mismo formato de datos independientemente del origen. Procesamiento y respuesta de consultas El flujo se divide en dos ramas principales donde primero se ejecuta la Rama 1 y luego la Rama 2 y finalmente ambas rambas convergen en una para poder brindar la respuesta final: Rama 1: Optimizaci\u00f3n -> B\u00fasqueda -> Respuesta Figura 6: Rama 1 \"Optimizaci\u00f3n, B\u00fasqueda, Respuesta\" Query Optimization : Utiliza un modelo de bajo procesamiento (gemma3:1b) para optimizar, limpiar y extraer solo la informaci\u00f3n esencial de la consulta del usuario para realizar b\u00fasquedas m\u00e1s precisas en la base de datos. Request qdrant-search : Se conecta al servicio qdrant-search (especificado en el Docker Compose) que escucha en el puerto 5000. Este servicio maneja las consultas a la base de datos vectorial. La configuraci\u00f3n de la colecci\u00f3n espec\u00edfica se encuentra en el cliente Python (insertar link). El cliente define: Cantidad de resultados: 20 mejores coincidencias Modelo de embedding: Debe coincidir con el utilizado durante la carga de datos Separate List 2 : Procesa la respuesta \u00fanica de la base de datos (formato JSON) y la separa en 20 objetos JSON individuales, cada uno conteniendo un chunk con su metadata correspondiente. Edit Fields2 : Realiza un mapeo de variables para preparar los datos. Aggregate1 : Combina toda la informaci\u00f3n que se proporcionar\u00e1 como entrada al AI Agent. AI Agent : Utiliza el modelo \"qwen3:8b\" para generar una respuesta basada en: La consulta original del usuario La informaci\u00f3n recuperada de la base de datos El contexto de la conversaci\u00f3n almacenado en \"PostgreSQL Chat Memory\" PostgreSQL Chat Memory : Almacena autom\u00e1ticamente el historial de conversaci\u00f3n, identificando usuarios por SessionID. Merge : Espera a que ambas ramas (Rama 1 y 2) est\u00e9n completas antes de continuar. Rama 2: Detecci\u00f3n de Referencias Figura 7: Rama 2 \"Detecci\u00f3n y Procesamiento de Referencias\" Reference? : Utiliza el modelo \"gemma3:1b\" para determinar si el usuario solicit\u00f3 referencias de papers. Devuelve: 1 : Si el usuario pidi\u00f3 referencias 0 : Si el usuario no pidi\u00f3 referencias If : Nodo condicional que eval\u00faa la respuesta del nodo anterior. Si la respuesta es 0 (no se pidieron referencias, camino rosa), el flujo pasa directamente al nodo Merge . Si la respuesta es 1 (se pidieron referencias, camino amarillo), el flujo contin\u00faa por: Merge3 : Combina la respuesta de la base de datos con el valor de referencia. Titles : Extrae los t\u00edtulos de los papers de las 20 mejores coincidencias (ordenadas por puntaje de similitud). Remove Duplicates : Elimina t\u00edtulos duplicados, ya que es com\u00fan que m\u00faltiples chunks del mismo paper aparezcan en los resultados. Limit : Limita la cantidad de referencias (configurado en 3 t\u00edtulos m\u00e1ximo). Aggregate : Combina los t\u00edtulos \u00fanicos en un solo objeto JSON. Edit Fields5 : Realiza el mapeo final de variables para las referencias. Merge : Espera a que ambas ramas (Rama 1 y 2) est\u00e9n completas antes de continuar. Etapa Final: Uni\u00f3n de ramas Figura 8: Uni\u00f3n de ramas, respuesta final Merge : Espera a que ambas ramas (respuesta principal y referencias) est\u00e9n completas antes de continuar. Aggregate2 : Combina la respuesta del agente con las referencias (si las hay). Edit Fields6 : Realiza el mapeo final y define el formato de salida, incluyendo la presentaci\u00f3n condicional de referencias. Respond to Webhook1 : Devuelve la respuesta final al punto de entrada correspondiente.","title":"Explicaci\u00f3n Workflow"},{"location":"implementacion/00_workflow-completo/#explicacion-del-workflow-del-proyecto","text":"Figura 1: Workflow Completo \ud83d\udfe1 Procesamiento y Carga de Informaci\u00f3n | \ud83d\udd35 Agent AI (primer versi\u00f3n) | \ud83d\udfe2 Agent AI (versi\u00f3n final) En esta secci\u00f3n se va a explicar el funcionamiento completo del workflow del proyecto. Est\u00e1 estructurado en dos grandes secciones principales: la preparaci\u00f3n de la base de datos (bloque amarillo) donde se almacena toda la informaci\u00f3n extra\u00edda de los papers, y la implementaci\u00f3n del agente que realiza las b\u00fasquedas. Se presentan dos tipos de agentes: el bloque azul representa una primera versi\u00f3n que actualmente no funciona correctamente, aunque podr\u00eda ser funcional con futuras actualizaciones. Para m\u00e1s informaci\u00f3n sobre esta problem\u00e1tica, se puede consultar la documentaci\u00f3n en esta secci\u00f3n , que ayuda a comprender por qu\u00e9 se desarroll\u00f3 una alternativa m\u00e1s manual que es la que finalmente se utiliza en el proyecto (bloque verde).","title":"Explicaci\u00f3n del Workflow del Proyecto"},{"location":"implementacion/00_workflow-completo/#procesamiento-y-carga-de-informacion","text":"Figura 2: Workflow de Procesamiento y Carga de Informaci\u00f3n El flujo comienza con el nodo When clicking 'Test workflow' que act\u00faa como disparador/trigger del workflow. Desde all\u00ed, el proceso contin\u00faa hacia el nodo Search all Content donde se debe seleccionar manualmente la carpeta donde se realizar\u00e1 la b\u00fasqueda. Este nodo genera como salida un listado completo de todos los elementos contenidos en la carpeta de Drive. Posteriormente, el flujo pasa al nodo Get Content que se encarga de descargar los archivos. Para procesar todos los archivos listados anteriormente, se configura con un identificador \"{{ $json.id }}\" que var\u00eda iterativamente hasta cubrir todos los elementos listados y los descarga. A continuaci\u00f3n, el proceso contin\u00faa hacia el nodo Write Files to input_papers . El objetivo es transferir todos los archivos descargados desde n8n para poder procesarlos con GROBID de manera local. Existe una carpeta compartida entre el contenedor y la PC local, definida en el docker-compose: volumes: - ./shared:/data/shared Por lo tanto, todos los archivos se descargan en el directorio '/data/shared/input_papers/{{ $json.name }}'. Luego, el nodo Execute GROBID (local) ejecuta el comando 'python3 /data/shared/cliente_grobid.py', que convierte todos los archivos PDF a formato XML y los ubica en la carpeta 'output_papers'. Una vez completado el procesamiento, el flujo contin\u00faa hacia Read Files from output_papers . Despu\u00e9s, el proceso pasa al nodo Extract from XML que est\u00e1 configurado para extraer campos comunes de todos los papers y convertirlos a formato JSON. Al estar bien estructurado gracias a la implementaci\u00f3n de GROBID, permite visualizar toda la estructura que mantienen los papers. Luego se realiza un proceso de formateo en el nodo Information extraction donde se mapean las variables que se utilizar\u00e1n para cargar en la base de datos, incluyendo informaci\u00f3n importante y metadata \u00fatil. Las variables extra\u00eddas son: title (metadata) abstract (metadata) publisher (metadata) year published (metadata) DOI (metadata) keywords (metadata) BODY_div (campo que contiene la informaci\u00f3n principal) BODY_fig (informaci\u00f3n de todas las figuras y sus descripciones) A continuaci\u00f3n, el flujo contin\u00faa hacia el nodo Information union que combina los dos campos mencionados anteriormente en un solo lugar, uniendo BODY_div con BODY_fig para formar INFO_BODY. Posteriormente, el proceso pasa al nodo Split Chunks , que consiste en un c\u00f3digo JavaScript que divide un texto extenso en fragmentos (chunks) m\u00e1s peque\u00f1os, respetando los l\u00edmites de las oraciones para mantener la coherencia del contenido. Luego, el nodo Separe List se encarga \u00fanicamente de separar una lista en varios elementos individuales. Finalmente, se ingresa a la secci\u00f3n final de este proceso, donde se encuentra un nodo Loop que permite iterar sobre todos los elementos, procesando 5 chunks a la vez. El flujo contin\u00faa hacia Create context , que es un nodo b\u00e1sico de LLM conectado a un modelo de Ollama (en este caso \"llama3.1:8b\"). En este proceso (como se explic\u00f3 en Contextual RAG ), se proporciona el documento completo \"INFO_BODY\" para generar un contexto espec\u00edfico para cada chunk basado en dicho documento. Una vez obtenido el contexto junto con el chunk original, ambos se cargan en la base de datos mediante el nodo Qdrant Vector Store , que est\u00e1 conectado a otros dos nodos: Embeddings Ollama , donde se define un modelo de Embedding (en este caso \"nomic-embed-text\"). Es importante destacar que se debe utilizar este mismo modelo al realizar b\u00fasquedas posteriores en la base de datos. El segundo nodo conectado es Default Data Loader , donde se especifica c\u00f3mo se cargar\u00e1 la informaci\u00f3n en la base de datos (incluyendo la metadata en este punto). Tambi\u00e9n tiene configurado un text splitter sin overlap, con una distancia de 5000 chunks size. Dado que anteriormente se realiz\u00f3 un split manual con un tama\u00f1o de 1000 al cargar en la base de datos (sumado a un contexto no muy extenso), nunca se alcanza los 5000, por lo que no se realiza un split adicional en esta zona, que es el comportamiento deseado. Finalmente, este proceso se repite en loop hasta completar todos los chunks. Una vez completado todo el proceso de procesamiento y carga, es posible verificar que los datos se han almacenado correctamente en la base de datos vectorial. Para ello, se puede acceder al panel de administraci\u00f3n de Qdrant a trav\u00e9s de la siguiente URL: http://localhost:6333/dashboard#/collections/ Al seleccionar la colecci\u00f3n creada, se podr\u00e1 visualizar la estructura de los datos almacenados, como se muestra en la siguiente imagen: Figura 3: Estructura de datos en Qdrant Vector Store En la visualizaci\u00f3n de Qdrant se pueden identificar tres componentes principales: \ud83d\udd34 Chunk (Rojo) : Contiene el fragmento original del documento extra\u00eddo durante el proceso de chunking \ud83d\udfe1 Contexto (Amarillo) : Representa el contexto generado por el modelo LLM que sit\u00faa el chunk dentro del documento completo \ud83d\udfe2 Metadata (Verde) : Incluye toda la informaci\u00f3n descriptiva del documento como t\u00edtulo, abstract, DOI, keywords, a\u00f1o de publicaci\u00f3n, etc. Esta estructura permite que el sistema de b\u00fasqueda sem\u00e1ntica pueda recuperar no solo el contenido relevante, sino tambi\u00e9n el contexto que lo hace comprensible y la metadata que facilita la identificaci\u00f3n del documento fuente.","title":"Procesamiento y Carga de Informaci\u00f3n"},{"location":"implementacion/00_workflow-completo/#agent-ai","text":"Figura 4: Workflow completo del Agent AI En esta secci\u00f3n se explica el funcionamiento del Agente de Inteligencia Artificial. El sistema ofrece dos formas de interactuar con el agente:","title":"Agent AI"},{"location":"implementacion/00_workflow-completo/#puntos-de-entrada","text":"When chat message received : Nodo nativo de n8n que permite interactuar con el agente directamente desde la interfaz de n8n para realizar pruebas de manera r\u00e1pida. Webhook : Nodo que espera solicitudes mediante m\u00e9todo POST para ser utilizado con OpenWebUI como interfaz final para el usuario. Figura 5: Puntos de entrada del Agent AI Ambos nodos se conectan al nodo Chat and session Id , que realiza un mapeo de variables para estandarizar la entrada, asegurando que los nodos posteriores ( Reference? y Query optimization ) reciban el mismo formato de datos independientemente del origen.","title":"Puntos de Entrada"},{"location":"implementacion/00_workflow-completo/#procesamiento-y-respuesta-de-consultas","text":"El flujo se divide en dos ramas principales donde primero se ejecuta la Rama 1 y luego la Rama 2 y finalmente ambas rambas convergen en una para poder brindar la respuesta final:","title":"Procesamiento y respuesta de consultas"},{"location":"implementacion/00_workflow-completo/#rama-1-optimizacion-busqueda-respuesta","text":"Figura 6: Rama 1 \"Optimizaci\u00f3n, B\u00fasqueda, Respuesta\" Query Optimization : Utiliza un modelo de bajo procesamiento (gemma3:1b) para optimizar, limpiar y extraer solo la informaci\u00f3n esencial de la consulta del usuario para realizar b\u00fasquedas m\u00e1s precisas en la base de datos. Request qdrant-search : Se conecta al servicio qdrant-search (especificado en el Docker Compose) que escucha en el puerto 5000. Este servicio maneja las consultas a la base de datos vectorial. La configuraci\u00f3n de la colecci\u00f3n espec\u00edfica se encuentra en el cliente Python (insertar link). El cliente define: Cantidad de resultados: 20 mejores coincidencias Modelo de embedding: Debe coincidir con el utilizado durante la carga de datos Separate List 2 : Procesa la respuesta \u00fanica de la base de datos (formato JSON) y la separa en 20 objetos JSON individuales, cada uno conteniendo un chunk con su metadata correspondiente. Edit Fields2 : Realiza un mapeo de variables para preparar los datos. Aggregate1 : Combina toda la informaci\u00f3n que se proporcionar\u00e1 como entrada al AI Agent. AI Agent : Utiliza el modelo \"qwen3:8b\" para generar una respuesta basada en: La consulta original del usuario La informaci\u00f3n recuperada de la base de datos El contexto de la conversaci\u00f3n almacenado en \"PostgreSQL Chat Memory\" PostgreSQL Chat Memory : Almacena autom\u00e1ticamente el historial de conversaci\u00f3n, identificando usuarios por SessionID. Merge : Espera a que ambas ramas (Rama 1 y 2) est\u00e9n completas antes de continuar.","title":"Rama 1: Optimizaci\u00f3n -&gt; B\u00fasqueda -&gt; Respuesta"},{"location":"implementacion/00_workflow-completo/#rama-2-deteccion-de-referencias","text":"Figura 7: Rama 2 \"Detecci\u00f3n y Procesamiento de Referencias\" Reference? : Utiliza el modelo \"gemma3:1b\" para determinar si el usuario solicit\u00f3 referencias de papers. Devuelve: 1 : Si el usuario pidi\u00f3 referencias 0 : Si el usuario no pidi\u00f3 referencias If : Nodo condicional que eval\u00faa la respuesta del nodo anterior. Si la respuesta es 0 (no se pidieron referencias, camino rosa), el flujo pasa directamente al nodo Merge . Si la respuesta es 1 (se pidieron referencias, camino amarillo), el flujo contin\u00faa por: Merge3 : Combina la respuesta de la base de datos con el valor de referencia. Titles : Extrae los t\u00edtulos de los papers de las 20 mejores coincidencias (ordenadas por puntaje de similitud). Remove Duplicates : Elimina t\u00edtulos duplicados, ya que es com\u00fan que m\u00faltiples chunks del mismo paper aparezcan en los resultados. Limit : Limita la cantidad de referencias (configurado en 3 t\u00edtulos m\u00e1ximo). Aggregate : Combina los t\u00edtulos \u00fanicos en un solo objeto JSON. Edit Fields5 : Realiza el mapeo final de variables para las referencias. Merge : Espera a que ambas ramas (Rama 1 y 2) est\u00e9n completas antes de continuar.","title":"Rama 2: Detecci\u00f3n de Referencias"},{"location":"implementacion/00_workflow-completo/#etapa-final-union-de-ramas","text":"Figura 8: Uni\u00f3n de ramas, respuesta final Merge : Espera a que ambas ramas (respuesta principal y referencias) est\u00e9n completas antes de continuar. Aggregate2 : Combina la respuesta del agente con las referencias (si las hay). Edit Fields6 : Realiza el mapeo final y define el formato de salida, incluyendo la presentaci\u00f3n condicional de referencias. Respond to Webhook1 : Devuelve la respuesta final al punto de entrada correspondiente.","title":"Etapa Final: Uni\u00f3n de ramas"},{"location":"implementacion/01_implementacion-final/","text":"Requisitos Previos Actualizaci\u00f3n del Sistema sudo apt update sudo apt upgrade Instalaci\u00f3n de Python sudo apt install python python3-pip python3-venv Instalaci\u00f3n de Docker sudo apt install docker.io sudo systemctl enable docker sudo systemctl start docker docker --version Instalaci\u00f3n de Docker-Compose sudo curl -L \"https://github.com/docker/compose/releases/latest/download/docker-compose- $( uname -s ) - $( uname -m ) \" -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose docker-compose --version Puesta en Marcha Clonaci\u00f3n del Repositorio git clone git@github.com:danunziata/pps-kevin_haponiuk-2025.git cd pps-kevin_haponiuk-2025/src/self-hosted-ai-starter-kit Ejecuci\u00f3n con Docker Compose El proyecto soporta dos perfiles de ejecuci\u00f3n seg\u00fan el hardware disponible: Perfil Comando CPU docker compose --profile cpu up GPU NVIDIA (recomendado) docker compose --profile gpu-nvidia up Descarga de Modelos en Ollama # Acceder al contenedor de Ollama docker exec -it ollama bash # Descargar modelos necesarios ollama run llama3.1:8b ollama run gemma3:1b ollama run qwen3:8b ollama pull nomic-embed-text Nota : Al finalizar cada ejecuci\u00f3n de modelo, salir con /bye . Configuraci\u00f3n de Servicios N8N Acceder a http://localhost:5678 Configurar credenciales locales: Email: prueba@gmail.com First Name: Prueba Last Name: Prueba Password: Prueba123 Verificar que aparezca el workflow \"Agent-AI-PPS_Kevin-Haponiuk\"... Si no aparece, importar desde: pps-kevin_haponiuk-2025/src/self-hosted-ai-starter-kit/n8n/backup/workflows/Agent-AI-PPS_Kevin-Haponiuk.json Credenciales de Google Configurar las credenciales de Google siguiendo la gu\u00eda de instalaci\u00f3n . Verificaci\u00f3n de GROBID Prueba desde navegador : http://localhost:8070/api/isalive debe retornar true Prueba desde N8N : Ejecutar el bloque \"TEST GROBID\" para verificar la conectividad Procesamiento de Datos Carga de Informaci\u00f3n Configurar la carpeta de PDFs en el bloque \"Search all Content\" Ejecutar el workflow \"CARGA DE INFORMACI\u00d3N\" en N8N Verificar la carga en http://localhost:6333/dashboard#/collections Configuraci\u00f3n de PostgreSQL Obtener la IP del servicio de postgres: docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' self-hosted-ai-starter-kit-postgres-1 Configurar credenciales del nodo de Postgres en N8N: Host: [IP obtenida] Database: n8n Username: root Password: password PgAdmin (Opcional) Acceder a http://localhost:5050/ Login: admin@admin.com / admin Agregar nuevo servidor con la IP de postgres y mismos par\u00e1metros utilizados en el punto anterior. Consulta al Agente Existen dos formas de interactuar con el agente: Chat directo : Usar el bloque \"when chat message received\" en N8N Webhook con OpenWebUI : Configurar integraci\u00f3n externa Configuraci\u00f3n de OpenWebUI Para poder hacer la consulta al agente mediante el bloque webhook, debemos configurar el servicio de OpenWebUI. Acceder a OpenWebUI : Ir a http://localhost:3000 y logearse con un mail y password. Crear nueva funci\u00f3n : Ir abajo a la izquierda y entrar al Admin Panel \u2192 Functions y crear una nueva funci\u00f3n. C\u00f3digo de la funci\u00f3n : Copiar el siguiente c\u00f3digo en la funci\u00f3n: # C\u00f3digo completo disponible en: src/self-hosted-ai-starter-kit/n8n-pipeline-function.py Configurar par\u00e1metros : Configure los siguientes par\u00e1metros en la funci\u00f3n creada: Nombre : \"N8N Pipeline\" Descripci\u00f3n : \"Pipeline para interactuar con N8N\" Acceder al bloque Webhook : En N8N, localice y configure el bloque Webhook de la siguiente manera (copie el Test URL generado): En las configuraciones de la funci\u00f3n, utilice la URL copiada de N8N y realice el siguiente cambio: Reemplace localhost por el nombre del contenedor de N8N (en este caso \"n8n\") Mantenga el resto de configuraciones en valores por defecto Activar el workflow : Active el workflow en N8N para comenzar a recibir solicitudes: Configurar escucha : Configure el workflow para escuchar y recibir respuestas externas: Verificaci\u00f3n Final Una vez completados todos los pasos anteriores, ya puede realizar consultas al agente directamente desde n8n o desde Open WebUI. Visualizaci\u00f3n de m\u00e9tricas cAdvisor (M\u00e9tricas Raw) URL: http://localhost:8081 Qu\u00e9 ver: Lista de contenedores y m\u00e9tricas b\u00e1sicas No requiere configuraci\u00f3n Prometheus (Base de Datos de M\u00e9tricas) URL: http://localhost:9090 Qu\u00e9 ver: Interfaz de consultas PromQL No requiere configuraci\u00f3n inicial Grafana (Dashboards) URL: http://localhost:3001 Usuario: admin Contrase\u00f1a: admin A continuaci\u00f3n se explica c\u00f3mo configurar Grafana agregando Prometheus como Fuente de Datos Ir a http://localhost:3001 Login con admin/admin Ir a Configuration \u2192 Data Sources Click en Add data source Seleccionar Prometheus URL: http://prometheus:9090 Click en Save & Test Luego se debe Importar un Dashboard Ir a Dashboards \u2192 Import Usar ID: 19908 o 893 Seleccionar Prometheus como fuente de datos Click en Import \ud83d\uded1 Detener la ejecuci\u00f3n # Para perfil CPU docker compose --profile cpu down # Para perfil GPU NVIDIA docker compose --profile gpu-nvidia down Puertos Utilizados Servicio Puerto N8N 5678 Ollama 11434 PgAdmin 5050 Qdrant 6333 Qdrant Search 5000 PostgreSQL 5432 OpenWebUI 3000 Grobid 8070 Cadvisor 8081 Prometheus 9090 Grafana 3001","title":"Implementaci\u00f3n"},{"location":"implementacion/01_implementacion-final/#requisitos-previos","text":"Actualizaci\u00f3n del Sistema sudo apt update sudo apt upgrade Instalaci\u00f3n de Python sudo apt install python python3-pip python3-venv Instalaci\u00f3n de Docker sudo apt install docker.io sudo systemctl enable docker sudo systemctl start docker docker --version Instalaci\u00f3n de Docker-Compose sudo curl -L \"https://github.com/docker/compose/releases/latest/download/docker-compose- $( uname -s ) - $( uname -m ) \" -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose docker-compose --version","title":"Requisitos Previos"},{"location":"implementacion/01_implementacion-final/#puesta-en-marcha","text":"","title":"Puesta en Marcha"},{"location":"implementacion/01_implementacion-final/#clonacion-del-repositorio","text":"git clone git@github.com:danunziata/pps-kevin_haponiuk-2025.git cd pps-kevin_haponiuk-2025/src/self-hosted-ai-starter-kit","title":"Clonaci\u00f3n del Repositorio"},{"location":"implementacion/01_implementacion-final/#ejecucion-con-docker-compose","text":"El proyecto soporta dos perfiles de ejecuci\u00f3n seg\u00fan el hardware disponible: Perfil Comando CPU docker compose --profile cpu up GPU NVIDIA (recomendado) docker compose --profile gpu-nvidia up","title":"Ejecuci\u00f3n con Docker Compose"},{"location":"implementacion/01_implementacion-final/#descarga-de-modelos-en-ollama","text":"# Acceder al contenedor de Ollama docker exec -it ollama bash # Descargar modelos necesarios ollama run llama3.1:8b ollama run gemma3:1b ollama run qwen3:8b ollama pull nomic-embed-text Nota : Al finalizar cada ejecuci\u00f3n de modelo, salir con /bye .","title":"Descarga de Modelos en Ollama"},{"location":"implementacion/01_implementacion-final/#configuracion-de-servicios","text":"","title":"Configuraci\u00f3n de Servicios"},{"location":"implementacion/01_implementacion-final/#n8n","text":"Acceder a http://localhost:5678 Configurar credenciales locales: Email: prueba@gmail.com First Name: Prueba Last Name: Prueba Password: Prueba123 Verificar que aparezca el workflow \"Agent-AI-PPS_Kevin-Haponiuk\"... Si no aparece, importar desde: pps-kevin_haponiuk-2025/src/self-hosted-ai-starter-kit/n8n/backup/workflows/Agent-AI-PPS_Kevin-Haponiuk.json","title":"N8N"},{"location":"implementacion/01_implementacion-final/#credenciales-de-google","text":"Configurar las credenciales de Google siguiendo la gu\u00eda de instalaci\u00f3n .","title":"Credenciales de Google"},{"location":"implementacion/01_implementacion-final/#verificacion-de-grobid","text":"Prueba desde navegador : http://localhost:8070/api/isalive debe retornar true Prueba desde N8N : Ejecutar el bloque \"TEST GROBID\" para verificar la conectividad","title":"Verificaci\u00f3n de GROBID"},{"location":"implementacion/01_implementacion-final/#procesamiento-de-datos","text":"","title":"Procesamiento de Datos"},{"location":"implementacion/01_implementacion-final/#carga-de-informacion","text":"Configurar la carpeta de PDFs en el bloque \"Search all Content\" Ejecutar el workflow \"CARGA DE INFORMACI\u00d3N\" en N8N Verificar la carga en http://localhost:6333/dashboard#/collections","title":"Carga de Informaci\u00f3n"},{"location":"implementacion/01_implementacion-final/#configuracion-de-postgresql","text":"Obtener la IP del servicio de postgres: docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' self-hosted-ai-starter-kit-postgres-1 Configurar credenciales del nodo de Postgres en N8N: Host: [IP obtenida] Database: n8n Username: root Password: password","title":"Configuraci\u00f3n de PostgreSQL"},{"location":"implementacion/01_implementacion-final/#pgadmin-opcional","text":"Acceder a http://localhost:5050/ Login: admin@admin.com / admin Agregar nuevo servidor con la IP de postgres y mismos par\u00e1metros utilizados en el punto anterior.","title":"PgAdmin (Opcional)"},{"location":"implementacion/01_implementacion-final/#consulta-al-agente","text":"Existen dos formas de interactuar con el agente: Chat directo : Usar el bloque \"when chat message received\" en N8N Webhook con OpenWebUI : Configurar integraci\u00f3n externa","title":"Consulta al Agente"},{"location":"implementacion/01_implementacion-final/#configuracion-de-openwebui","text":"Para poder hacer la consulta al agente mediante el bloque webhook, debemos configurar el servicio de OpenWebUI. Acceder a OpenWebUI : Ir a http://localhost:3000 y logearse con un mail y password. Crear nueva funci\u00f3n : Ir abajo a la izquierda y entrar al Admin Panel \u2192 Functions y crear una nueva funci\u00f3n. C\u00f3digo de la funci\u00f3n : Copiar el siguiente c\u00f3digo en la funci\u00f3n: # C\u00f3digo completo disponible en: src/self-hosted-ai-starter-kit/n8n-pipeline-function.py Configurar par\u00e1metros : Configure los siguientes par\u00e1metros en la funci\u00f3n creada: Nombre : \"N8N Pipeline\" Descripci\u00f3n : \"Pipeline para interactuar con N8N\" Acceder al bloque Webhook : En N8N, localice y configure el bloque Webhook de la siguiente manera (copie el Test URL generado): En las configuraciones de la funci\u00f3n, utilice la URL copiada de N8N y realice el siguiente cambio: Reemplace localhost por el nombre del contenedor de N8N (en este caso \"n8n\") Mantenga el resto de configuraciones en valores por defecto Activar el workflow : Active el workflow en N8N para comenzar a recibir solicitudes: Configurar escucha : Configure el workflow para escuchar y recibir respuestas externas:","title":"Configuraci\u00f3n de OpenWebUI"},{"location":"implementacion/01_implementacion-final/#verificacion-final","text":"Una vez completados todos los pasos anteriores, ya puede realizar consultas al agente directamente desde n8n o desde Open WebUI.","title":"Verificaci\u00f3n Final"},{"location":"implementacion/01_implementacion-final/#visualizacion-de-metricas","text":"cAdvisor (M\u00e9tricas Raw) URL: http://localhost:8081 Qu\u00e9 ver: Lista de contenedores y m\u00e9tricas b\u00e1sicas No requiere configuraci\u00f3n Prometheus (Base de Datos de M\u00e9tricas) URL: http://localhost:9090 Qu\u00e9 ver: Interfaz de consultas PromQL No requiere configuraci\u00f3n inicial Grafana (Dashboards) URL: http://localhost:3001 Usuario: admin Contrase\u00f1a: admin A continuaci\u00f3n se explica c\u00f3mo configurar Grafana agregando Prometheus como Fuente de Datos Ir a http://localhost:3001 Login con admin/admin Ir a Configuration \u2192 Data Sources Click en Add data source Seleccionar Prometheus URL: http://prometheus:9090 Click en Save & Test Luego se debe Importar un Dashboard Ir a Dashboards \u2192 Import Usar ID: 19908 o 893 Seleccionar Prometheus como fuente de datos Click en Import","title":"Visualizaci\u00f3n de m\u00e9tricas"},{"location":"implementacion/01_implementacion-final/#detener-la-ejecucion","text":"# Para perfil CPU docker compose --profile cpu down # Para perfil GPU NVIDIA docker compose --profile gpu-nvidia down","title":"\ud83d\uded1 Detener la ejecuci\u00f3n"},{"location":"implementacion/01_implementacion-final/#puertos-utilizados","text":"Servicio Puerto N8N 5678 Ollama 11434 PgAdmin 5050 Qdrant 6333 Qdrant Search 5000 PostgreSQL 5432 OpenWebUI 3000 Grobid 8070 Cadvisor 8081 Prometheus 9090 Grafana 3001","title":"Puertos Utilizados"},{"location":"implementacion/02_implementacion-dorotea/","text":"Prueba en Dorotea \u2014 Ajustes de arquitectura y c\u00f3digo Objetivo. Salir del entorno local y validar el stack en un servidor remoto de altas prestaciones (Dorotea) sin exponer puertos p\u00fablicos; mantener acceso seguro v\u00eda SSH y dejar todo listo para pruebas de carga y funcionales. Requisitos Previos: Drivers de NVIDIA Instalados en Rocky Linux El servidor debe tener los drivers propietarios de NVIDIA funcionando correctamente. Para verificarlo, ejecuta este comando: nvidia-smi Si ves una tabla con los detalles de tu GPU, est\u00e1s listo. Si te da un error, necesitas instalar los drivers primero. Docker Instalado Debes tener una instalaci\u00f3n funcional de Docker. Puedes verificarlo con: docker --version Paso 1: Configurar los Repositorios de NVIDIA Rocky Linux no sabe d\u00f3nde encontrar el toolkit por defecto. Primero, debemos agregar los repositorios oficiales de NVIDIA a nuestro sistema. sudo dnf config-manager --add-repo https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo Paso 2: Instalar el NVIDIA Container Toolkit Ahora que el sistema conoce el repositorio, puedes instalar el paquete usando dnf. sudo dnf install -y nvidia-container-toolkit Paso 3: Configurar Docker para que Use el Toolkit Una vez instalado, tienes que decirle a Docker que lo utilice como un \"runtime\" para GPUs. El toolkit incluye una herramienta para hacer esto autom\u00e1ticamente. sudo nvidia-ctk runtime configure --runtime = docker Despu\u00e9s de configurar, es crucial reiniciar el servicio de Docker para que aplique los cambios. sudo systemctl restart docker Paso 4: Verificar la Instalaci\u00f3n \u2705 La mejor forma de confirmar que todo funciona es ejecutar un contenedor de prueba que intente acceder a la GPU. El siguiente comando es el est\u00e1ndar para esta prueba: docker run --rm --gpus all nvidia/cuda:12.0.0-base-ubuntu22.04 nvidia-smi Resumen de la evoluci\u00f3n de la arquitectura Opci\u00f3n 1: NGINX \u2192 4 servicios (ideal) Un solo puerto expuesto por NGINX y, detr\u00e1s, 4 servicios accesibles por subpath : - `/` \u2192 n8n - `/grafana/` \u2192 Grafana - `/webui/` \u2192 OpenWebUI - `/qdrant/` \u2192 Qdrant Ventaja: 1 t\u00fanel SSH, una \u00fanica puerta de entrada y URLs limpias. Problemas encontrados WebSockets/SSE de n8n : el proxy requer\u00eda proxy_http_version 1.1 , headers Upgrade/Connection y timeouts altos. Sin eso, fallaban eventos del editor y la UI mostraba \u201cCould not connect to server\u201d. Subpath : Grafana requiere GF_SERVER_ROOT_URL apuntando al subpath y GF_SERVER_SERVE_FROM_SUB_PATH=true . Aun con eso, aparecieron 301/redirects a /grafana y recursos est\u00e1ticos fuera del path. OpenWebUI sirve assets en rutas absolutas; al forzarlo bajo /webui/ faltaban archivos ( /static/splash.png 404) y la UI quedaba en blanco salvo el loader. Opci\u00f3n 2: solamente NGINX -> N8N (transici\u00f3n) 1 puerto \u2192 NGINX \u2192 solo n8n en / . Opci\u00f3n 3: 4 puertos v\u00eda SSH (implementada) Se abren t\u00faneles locales (sin puertos p\u00fablicos): ssh -i ~/.ssh/dorotea-kevin -L 51234 :localhost:51234 daninziata@200.7.141.59 ssh -i ~/.ssh/dorotea-kevin -L 51235 :localhost:51235 daninziata@200.7.141.59 ssh -i ~/.ssh/dorotea-kevin -L 51236 :localhost:51236 daninziata@200.7.141.59 ssh -i ~/.ssh/dorotea-kevin -L 51237 :localhost:51237 daninziata@200.7.141.59 Con esto: http://localhost:51234 \u2192 n8n (pasa por NGINX) http://localhost:51235 \u2192 Grafana http://localhost:51236 \u2192 OpenWebUI http://localhost:51237/dashboard#/collections \u2192 Qdrant (dashboard) Principales diferencias ANTES vs DESPU\u00c9S Ajustes de NGINX para utilizar n8n en la ra\u00edz ( / ) El proxy qued\u00f3 ajustado para WebSockets/SSE y conexiones largas : proxy_http_version 1.1 Upgrade / Connection \"upgrade\" timeouts altos ( proxy_read_timeout , proxy_send_timeout ) proxy_buffering off para server-sent events (SSE) Con estos ajustes, el editor de n8n dej\u00f3 de mostrar errores de conexi\u00f3n y pudo autenticarse y trabajar con normalidad detr\u00e1s del proxy. Cambio de almacenamiento (limitaci\u00f3n de /var ) Contexto. El servidor tiene: /dev/mapper/vg_server-lv_root 79G 49G 30G 62% / /dev/nvme1n1p1 7T 398G 6.2T 6% /home El runtime de Docker por defecto escribe en /var/lib/docker (dentro de / ), lo que r\u00e1pidamente agotaba espacio , especialmente al descargar modelos de Ollama y datos persistentes. Acci\u00f3n. Se reemplazaron vol\u00famenes nombrados por bind mounts hacia /home : Postgres \u2192 /home/daninziata/docker_data/postgres Grafana \u2192 /home/daninziata/docker_data/grafana OpenWebUI \u2192 /home/daninziata/docker_data/openwebui Qdrant \u2192 /home/daninziata/docker_data/qdrant Prometheus \u2192 /home/daninziata/docker_data/prometheus Ollama (modelos) \u2192 /home/daninziata/ollama_data Resultado. Todo lo pesado sale de /var y pasa al disco grande de 7 TB , eliminando errores por falta de espacio. Networking Antes: muchos servicios con ports: publicados hacia el host. Despu\u00e9s: n8n permanece detr\u00e1s de NGINX (publicado en 51234 ). Grafana , OpenWebUI y Qdrant se publican en puertos altos (51235/51236/51237) s\u00f3lo para t\u00faneles SSH . Servicios internos (Postgres, Prometheus, cAdvisor, etc.) usan expose dentro de la red demo y no se publican al host. Persistencia Antes: vol\u00famenes nombrados ( grafana_data , prometheus_data , etc.). Despu\u00e9s: bind mounts a rutas en /home/daninziata/... para cada servicio, y /home/daninziata/ollama_data para los modelos. Nombres/hosts Antes: contenedores cortos ( ollama , qdrant ). Despu\u00e9s: sufijo -k para distinguir el despliegue remoto (ej. ollama-k , qdrant-k , nginx-proxy-k , etc.). Las apps que dependen por URL se actualizaron (ej. OLLAMA_HOST=ollama-k:11434 ). Monitoreo: cAdvisor + Prometheus S\u00edntoma inicial en Prometheus: connect: connection refused al intentar scrapear cadvisor-k:8081/metrics . Causa: El contenedor cAdvisor escucha en 8080 , pero el job de Prometheus apuntaba a 8081 . Arreglo aplicado: En Compose: cadvisor con expose: \"8080\" . En Prometheus: job cadvisor \u2192 targets: ['cadvisor-k:8080'] . Servicio qdrant-search (Flask + Qdrant Client + Ollama) Par\u00e1metros por variables de entorno (en Compose): OLLAMA_URL=http://ollama-k:11434 QDRANT_URL=http://qdrant-k:6333 COLLECTION_NAME=TEST_DOROTEA Importante: Si se cambia el nombre de colecci\u00f3n , hay que reconstruir la imagen para propagar variables al contenedor: docker compose build --no-cache qdrant-search docker compose up -d qdrant-search docker exec -it qdrant-search-k sh cat /app/client_qdrant_flask_docker.py | grep collection_name El microservicio devolv\u00eda Not found: Collection 'prueba_CRAG3' doesn\u2019t exist! aun cuando COLLECTION_NAME=TEST_DOROTEA . Causa : el contenedor estaba corriendo con capa antigua (variables de una build previa). Soluci\u00f3n : rebuild del servicio. Tras eso, las consultas desde n8n y desde la red Docker funcionaron. GPU y modelos (Ollama) Verificaci\u00f3n de uso de GPU con nvidia-smi : durante generaci\u00f3n se observ\u00f3 GPU-Util 100% en la A30 y consumo de VRAM acorde a los modelos cargados. Sin embargo, solo en una de las 2 placas. tambi\u00e9n se observ\u00f3 que Logs de Ollama mostraron loaded CUDA backend , \u201cVRAM available/required\u201d y selecci\u00f3n de GPULayers , confirmando aceleraci\u00f3n CUDA. Mensaje \u201ctruncating input prompt (limit=4096)\u201d : es el l\u00edmite de contexto del modelo en tokens . Si el prompt (incluida la conversaci\u00f3n previa) lo supera, Ollama lo recorta. Mitigaci\u00f3n: prompts m\u00e1s breves o modelos con mayor contexto (cuando est\u00e9n disponibles). Resultados de las pruebas funcionales en Dorotea Objetivo: Validar el funcionamiento integral del stack con aceleraci\u00f3n GPU, recolecci\u00f3n de m\u00e9tricas en Grafana y generaci\u00f3n de embeddings mediante Ollama y Qdrant, utilizando la colecci\u00f3n TEST_DOROTEA como caso de prueba. 1. Pruebas de carga \u2014 Uso extremo de CPU (Profile CPU) Durante la primera etapa de las pruebas se intent\u00f3 ejecutar un perfil intensivo del modelo ( Profile CPU ), pero el proceso satur\u00f3 completamente los n\u00facleos de la m\u00e1quina, llevando el uso de CPU al 100% en todos los hilos. El proceso fue ejecutado desde Ollama ( /usr/bin/ollama runner ) con m\u00faltiples hilos en paralelo, observ\u00e1ndose que cada uno ejecutaba la carga de modelo en puertos independientes ( --port 33137 ). Este test qued\u00f3 en ejecuci\u00f3n continua sin completar, por lo que se detuvo manualmente para evitar degradar el rendimiento del resto de los contenedores. 2. Monitoreo con Grafana y cAdvisor Durante la ejecuci\u00f3n se dej\u00f3 Grafana activo con los dashboards de Prometheus y cAdvisor recolectando m\u00e9tricas en tiempo real de todos los contenedores. El monitoreo permiti\u00f3 observar: N\u00famero de contenedores activos: 13 Procesos m\u00e1s demandantes de CPU: grobid-k (36.6%) cadvisor-k (10.9%) pgadmin-k , n8n-k , prometheus-k en menor proporci\u00f3n Consumo de memoria: promedio general bajo, sin indicios de swapping ni saturaci\u00f3n. Estos datos validan que el entorno completo puede ejecutarse de manera estable durante largos periodos, incluso con procesos intensivos como el parsing de papers o la generaci\u00f3n de embeddings. 3. Carga de datos \u2014 Creaci\u00f3n de la colecci\u00f3n TEST_DOROTEA Tras estabilizar el entorno, se procedi\u00f3 a realizar una carga de 10 papers cient\u00edficos en formato JSON procesados por el pipeline Grobid \u2192 Ollama \u2192 Qdrant. Duraci\u00f3n total del proceso: aproximadamente 2 horas . Esto sugiere que el cuello de botella podr\u00eda encontrarse en: Procesamiento de embeddings dentro de Ollama ( nomic-embed-text ). Paralelizaci\u00f3n limitada o falta de optimizaci\u00f3n en la inserci\u00f3n de datos en Qdrant. Se recomienda analizar en profundidad la arquitectura de Ollama (modelos, batch size, n\u00famero de workers) para reducir los tiempos de carga en pruebas futuras. Resultado: La colecci\u00f3n TEST_DOROTEA se cre\u00f3 correctamente, con estado green y 554 puntos vectoriales distribuidos en 8 segmentos . 4. Visualizaci\u00f3n de la colecci\u00f3n en Qdrant La exploraci\u00f3n de la colecci\u00f3n en la interfaz de Qdrant mostr\u00f3 una distribuci\u00f3n coherente de los embeddings en el espacio vectorial, confirmando que los puntos fueron generados correctamente y los metadatos (contenido, fuente, etc.) se mantienen accesibles. 5. Pruebas de b\u00fasqueda sem\u00e1ntica Una vez creada la colecci\u00f3n, se probaron distintas consultas al agente a trav\u00e9s de qdrant-search-k , verificando que las respuestas generadas correspond\u00edan a la sem\u00e1ntica de las preguntas y retornaban papers relevantes. \ud83d\udd39 Consulta 1 Pregunta: \u00bfQu\u00e9 trabajos hablan sobre Kubernetes o autoscaling en contenedores? Respuesta del agente: No specific information was found in the texts to answer the question regarding the types of jobs (trabajos) related to Kubernetes or autoscaling in containers. Recommended Papers: Adaptive AI-based auto-scaling for Kubernetes Predictive Hybrid Autoscaling for Containerized Applications Time series forecasting-based Kubernetes autoscaling using Facebook Prophet and Long Short-Term Memory \ud83d\udd39 Consulta 2 Pregunta: Dame referencias sobre los papers que mencionan MLOps o pipelines de machine learning. Respuesta del agente: Here are the references to papers that mention MLOps or pipelines in machine learning: \"MLOps: A Taxonomy and a Methodology\" \"Towards MLOps: A Case Study of ML Pipeline Platform\" \"MLOps: A Taxonomy and a Methodology\" \"MLOps: A Taxonomy and a Methodology\" \"MLOps: A Taxonomy and a Methodology\" These papers provide an overview of the MLOps methodology, its importance, and its application in industrial environments. They also discuss various aspects of MLOps, including pipelines, DevOps principles, and benchmarking tools. The papers also mention other relevant topics such as: Machine Learning Operations (MLOps) ML pipeline platforms CI/CD routines Automated ML triggering Model registry storage Monitoring and performance You can find more information about these papers by accessing the references provided in the database results. Recommended Papers: MLOps: A Taxonomy and a Methodology Towards MLOps: A Case Study of ML Pipeline Platform \ud83d\udd39 Consulta 3 Pregunta: \u00bfQu\u00e9 papers analizan la eficiencia energ\u00e9tica o el consumo de energ\u00eda en la nube? Respuesta del agente: \"Energy efficiency is always a major concern in cloud computing and when it comes to the application level many recommendations have been made to optimise energy usage at the system level.\" \"No specific information was found in the texts to answer the question about papers that analyze the efficiency of energy consumption or energy consumption in the nube, specifically about papers with titles that include \"Energy efficiency in cloud computing data centers\" and \"Energy efficiency\" or mention \"energy consumption\".\" Recommended Papers: Energy efficiency in cloud computing data centers: a survey on software technologies Machine Learning-based Orchestration of Containers: A Taxonomy and Future Directions \ud83d\udd39 Consulta 4 Pregunta: \u00bfCu\u00e1les de los papers hacen una revisi\u00f3n o resumen del estado del arte sobre tecnolog\u00edas de contenedores? Respuesta del agente: No specific information was found in the texts to answer the question about papers that analyze the efficiency of energy consumption or energy consumption in the nube. Recommended Papers: Container Technologies for ARM Architecture: A Comprehensive Survey of the State-of-the-Art A novel energy-aware resource management technique using joint VM and container consolidation approach for green computing in cloud data centers \ud83d\udd39 Consulta 5 Pregunta: \u00bfQu\u00e9 trabajos proponen nuevos algoritmos o m\u00e9todos para mejorar la elasticidad o el escalado en la nube? Respuesta del agente: \"No specific information was found in the texts to answer the question about new algorithms or methods for improving elasticity or scaling in the cloud.\" Recommended Papers: A New Infrastructure Elasticity Control Algorithm for Containerized Cloud Adaptive AI-based auto-scaling for Kubernetes Time series forecasting-based Kubernetes autoscaling using Facebook Prophet and Long Short-Term Memory Test usando OpenwebUI + Preguntas en ingl\u00e9s Una vez dentro de OpenwebUI, el primer paso es configurar el Pipe que se utilizar\u00e1. Para ello, sigue los pasos explicados en el archivo 01_implementacion-final.md . La \u00fanica diferencia en este caso fue el c\u00f3digo empleado para configurar el Pipe, que se encuentra en [ DOCUMENTACION/pps-kevin_haponiuk-2025/src/self-hosted-ai-starter-kit/n8n-pipeline-function-dorotea.py ]. Las preguntas realizadas fueron las mismas que en pruebas anteriores, pero esta vez en ingl\u00e9s. Which papers discuss Kubernetes or autoscaling in containers? Provide references to papers that mention MLOps or machine learning pipelines. En este caso, se realiz\u00f3 la misma pregunta por segunda vez, ya que la primera respuesta fue algo imprecisa. Esto permiti\u00f3 observar c\u00f3mo se comporta el sistema al repetir una consulta. Which papers analyze energy efficiency or energy consumption in the cloud? Which papers provide a review or state-of-the-art summary on container technologies? Which papers propose new algorithms or methods to improve elasticity or scaling in the cloud? Es importante destacar que las respuestas del agente mejoraron significativamente al realizar las preguntas en ingl\u00e9s. Esto se debe a que el modelo de Ollama est\u00e1 entrenado principalmente en ingl\u00e9s, lo que optimiza la calidad y precisi\u00f3n de las respuestas. Conclusiones El pipeline completo oper\u00f3 de manera estable , abarcando desde la extracci\u00f3n y an\u00e1lisis de los papers hasta la indexaci\u00f3n vectorial y la consulta sem\u00e1ntica. El rendimiento general fue satisfactorio , aunque la carga inicial de datos result\u00f3 lenta (\u22482 horas para 10 documentos) , indicando posibles cuellos de botella en el procesamiento de embeddings o la inserci\u00f3n en Qdrant. Las respuestas del agente fueron sem\u00e1nticamente relevantes , con sugerencias precisas y papers correctamente asociados, validando el correcto funcionamiento de los embeddings y la b\u00fasqueda vectorial en Qdrant. Adem\u00e1s, se observ\u00f3 que el agente mejora significativamente sus respuestas cuando las preguntas se formulan en ingl\u00e9s, debido a que el modelo de Ollama est\u00e1 entrenado principalmente en este idioma. Esto optimiza la calidad y precisi\u00f3n de las respuestas, haciendo que las consultas sean m\u00e1s efectivas. Se recomienda explorar optimizaciones en: Uso de GPU en Ollama : mejorar la paralelizaci\u00f3n durante la generaci\u00f3n de embeddings. Segmentaci\u00f3n del proceso de carga : dividir tareas para reducir tiempos. Modelos alternativos : evaluar arquitecturas m\u00e1s eficientes para embeddings masivos. La infraestructura desplegada en Dorotea demostr\u00f3 ser robusta, con capacidad de procesamiento adecuada y un rendimiento aceptable para pruebas de investigaci\u00f3n y desarrollo, as\u00ed como para futuras escalas. Trabajo futuro Migrar a la Opci\u00f3n 1 : Consolidar los servicios detr\u00e1s de un \u00fanico puerto y 4 subpaths en NGINX para simplificar el acceso y mejorar la organizaci\u00f3n. Fortalecer la seguridad : Implementar autenticaci\u00f3n en NGINX o integrar un sistema de inicio de sesi\u00f3n \u00fanico (SSO). Configurar TLS end-to-end si se planea exponer los servicios m\u00e1s all\u00e1 del t\u00fanel SSH (actualmente suficiente para pruebas internas). Optimizar el pipeline : Analizar el uso de GPU en Ollama para maximizar la eficiencia. Evaluar herramientas o t\u00e9cnicas para acelerar la carga de datos en Qdrant. Considerar modelos con mayor capacidad de contexto para mejorar las respuestas sem\u00e1nticas. Monitoreo avanzado : Ampliar los dashboards en Grafana para incluir m\u00e9tricas espec\u00edficas de GPU y rendimiento de embeddings. Automatizar alertas para detectar posibles cuellos de botella o fallos en tiempo real.","title":"Implementaci\u00f3n en Dorotea"},{"location":"implementacion/02_implementacion-dorotea/#prueba-en-dorotea-ajustes-de-arquitectura-y-codigo","text":"Objetivo. Salir del entorno local y validar el stack en un servidor remoto de altas prestaciones (Dorotea) sin exponer puertos p\u00fablicos; mantener acceso seguro v\u00eda SSH y dejar todo listo para pruebas de carga y funcionales.","title":"Prueba en Dorotea \u2014 Ajustes de arquitectura y c\u00f3digo"},{"location":"implementacion/02_implementacion-dorotea/#requisitos-previos-drivers-de-nvidia-instalados-en-rocky-linux","text":"El servidor debe tener los drivers propietarios de NVIDIA funcionando correctamente. Para verificarlo, ejecuta este comando: nvidia-smi Si ves una tabla con los detalles de tu GPU, est\u00e1s listo. Si te da un error, necesitas instalar los drivers primero.","title":"Requisitos Previos: Drivers de NVIDIA Instalados en Rocky Linux"},{"location":"implementacion/02_implementacion-dorotea/#docker-instalado","text":"Debes tener una instalaci\u00f3n funcional de Docker. Puedes verificarlo con: docker --version","title":"Docker Instalado"},{"location":"implementacion/02_implementacion-dorotea/#paso-1-configurar-los-repositorios-de-nvidia","text":"Rocky Linux no sabe d\u00f3nde encontrar el toolkit por defecto. Primero, debemos agregar los repositorios oficiales de NVIDIA a nuestro sistema. sudo dnf config-manager --add-repo https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo","title":"Paso 1: Configurar los Repositorios de NVIDIA"},{"location":"implementacion/02_implementacion-dorotea/#paso-2-instalar-el-nvidia-container-toolkit","text":"Ahora que el sistema conoce el repositorio, puedes instalar el paquete usando dnf. sudo dnf install -y nvidia-container-toolkit","title":"Paso 2: Instalar el NVIDIA Container Toolkit"},{"location":"implementacion/02_implementacion-dorotea/#paso-3-configurar-docker-para-que-use-el-toolkit","text":"Una vez instalado, tienes que decirle a Docker que lo utilice como un \"runtime\" para GPUs. El toolkit incluye una herramienta para hacer esto autom\u00e1ticamente. sudo nvidia-ctk runtime configure --runtime = docker Despu\u00e9s de configurar, es crucial reiniciar el servicio de Docker para que aplique los cambios. sudo systemctl restart docker","title":"Paso 3: Configurar Docker para que Use el Toolkit"},{"location":"implementacion/02_implementacion-dorotea/#paso-4-verificar-la-instalacion","text":"La mejor forma de confirmar que todo funciona es ejecutar un contenedor de prueba que intente acceder a la GPU. El siguiente comando es el est\u00e1ndar para esta prueba: docker run --rm --gpus all nvidia/cuda:12.0.0-base-ubuntu22.04 nvidia-smi","title":"Paso 4: Verificar la Instalaci\u00f3n \u2705"},{"location":"implementacion/02_implementacion-dorotea/#resumen-de-la-evolucion-de-la-arquitectura","text":"","title":"Resumen de la evoluci\u00f3n de la arquitectura"},{"location":"implementacion/02_implementacion-dorotea/#opcion-1-nginx-4-servicios-ideal","text":"Un solo puerto expuesto por NGINX y, detr\u00e1s, 4 servicios accesibles por subpath : - `/` \u2192 n8n - `/grafana/` \u2192 Grafana - `/webui/` \u2192 OpenWebUI - `/qdrant/` \u2192 Qdrant Ventaja: 1 t\u00fanel SSH, una \u00fanica puerta de entrada y URLs limpias. Problemas encontrados WebSockets/SSE de n8n : el proxy requer\u00eda proxy_http_version 1.1 , headers Upgrade/Connection y timeouts altos. Sin eso, fallaban eventos del editor y la UI mostraba \u201cCould not connect to server\u201d. Subpath : Grafana requiere GF_SERVER_ROOT_URL apuntando al subpath y GF_SERVER_SERVE_FROM_SUB_PATH=true . Aun con eso, aparecieron 301/redirects a /grafana y recursos est\u00e1ticos fuera del path. OpenWebUI sirve assets en rutas absolutas; al forzarlo bajo /webui/ faltaban archivos ( /static/splash.png 404) y la UI quedaba en blanco salvo el loader.","title":"Opci\u00f3n 1: NGINX \u2192 4 servicios (ideal)"},{"location":"implementacion/02_implementacion-dorotea/#opcion-2-solamente-nginx-n8n-transicion","text":"1 puerto \u2192 NGINX \u2192 solo n8n en / .","title":"Opci\u00f3n 2: solamente NGINX -&gt; N8N (transici\u00f3n)"},{"location":"implementacion/02_implementacion-dorotea/#opcion-3-4-puertos-via-ssh-implementada","text":"Se abren t\u00faneles locales (sin puertos p\u00fablicos): ssh -i ~/.ssh/dorotea-kevin -L 51234 :localhost:51234 daninziata@200.7.141.59 ssh -i ~/.ssh/dorotea-kevin -L 51235 :localhost:51235 daninziata@200.7.141.59 ssh -i ~/.ssh/dorotea-kevin -L 51236 :localhost:51236 daninziata@200.7.141.59 ssh -i ~/.ssh/dorotea-kevin -L 51237 :localhost:51237 daninziata@200.7.141.59 Con esto: http://localhost:51234 \u2192 n8n (pasa por NGINX) http://localhost:51235 \u2192 Grafana http://localhost:51236 \u2192 OpenWebUI http://localhost:51237/dashboard#/collections \u2192 Qdrant (dashboard)","title":"Opci\u00f3n 3: 4 puertos v\u00eda SSH (implementada)"},{"location":"implementacion/02_implementacion-dorotea/#principales-diferencias-antes-vs-despues","text":"","title":"Principales diferencias ANTES vs DESPU\u00c9S"},{"location":"implementacion/02_implementacion-dorotea/#ajustes-de-nginx-para-utilizar-n8n-en-la-raiz","text":"El proxy qued\u00f3 ajustado para WebSockets/SSE y conexiones largas : proxy_http_version 1.1 Upgrade / Connection \"upgrade\" timeouts altos ( proxy_read_timeout , proxy_send_timeout ) proxy_buffering off para server-sent events (SSE) Con estos ajustes, el editor de n8n dej\u00f3 de mostrar errores de conexi\u00f3n y pudo autenticarse y trabajar con normalidad detr\u00e1s del proxy.","title":"Ajustes de NGINX para utilizar n8n en la ra\u00edz (/)"},{"location":"implementacion/02_implementacion-dorotea/#cambio-de-almacenamiento-limitacion-de-var","text":"Contexto. El servidor tiene: /dev/mapper/vg_server-lv_root 79G 49G 30G 62% / /dev/nvme1n1p1 7T 398G 6.2T 6% /home El runtime de Docker por defecto escribe en /var/lib/docker (dentro de / ), lo que r\u00e1pidamente agotaba espacio , especialmente al descargar modelos de Ollama y datos persistentes. Acci\u00f3n. Se reemplazaron vol\u00famenes nombrados por bind mounts hacia /home : Postgres \u2192 /home/daninziata/docker_data/postgres Grafana \u2192 /home/daninziata/docker_data/grafana OpenWebUI \u2192 /home/daninziata/docker_data/openwebui Qdrant \u2192 /home/daninziata/docker_data/qdrant Prometheus \u2192 /home/daninziata/docker_data/prometheus Ollama (modelos) \u2192 /home/daninziata/ollama_data Resultado. Todo lo pesado sale de /var y pasa al disco grande de 7 TB , eliminando errores por falta de espacio.","title":"Cambio de almacenamiento (limitaci\u00f3n de /var)"},{"location":"implementacion/02_implementacion-dorotea/#networking","text":"Antes: muchos servicios con ports: publicados hacia el host. Despu\u00e9s: n8n permanece detr\u00e1s de NGINX (publicado en 51234 ). Grafana , OpenWebUI y Qdrant se publican en puertos altos (51235/51236/51237) s\u00f3lo para t\u00faneles SSH . Servicios internos (Postgres, Prometheus, cAdvisor, etc.) usan expose dentro de la red demo y no se publican al host.","title":"Networking"},{"location":"implementacion/02_implementacion-dorotea/#persistencia","text":"Antes: vol\u00famenes nombrados ( grafana_data , prometheus_data , etc.). Despu\u00e9s: bind mounts a rutas en /home/daninziata/... para cada servicio, y /home/daninziata/ollama_data para los modelos.","title":"Persistencia"},{"location":"implementacion/02_implementacion-dorotea/#nombreshosts","text":"Antes: contenedores cortos ( ollama , qdrant ). Despu\u00e9s: sufijo -k para distinguir el despliegue remoto (ej. ollama-k , qdrant-k , nginx-proxy-k , etc.). Las apps que dependen por URL se actualizaron (ej. OLLAMA_HOST=ollama-k:11434 ).","title":"Nombres/hosts"},{"location":"implementacion/02_implementacion-dorotea/#monitoreo-cadvisor-prometheus","text":"S\u00edntoma inicial en Prometheus: connect: connection refused al intentar scrapear cadvisor-k:8081/metrics . Causa: El contenedor cAdvisor escucha en 8080 , pero el job de Prometheus apuntaba a 8081 . Arreglo aplicado: En Compose: cadvisor con expose: \"8080\" . En Prometheus: job cadvisor \u2192 targets: ['cadvisor-k:8080'] .","title":"Monitoreo: cAdvisor + Prometheus"},{"location":"implementacion/02_implementacion-dorotea/#servicio-qdrant-search-flask-qdrant-client-ollama","text":"Par\u00e1metros por variables de entorno (en Compose): OLLAMA_URL=http://ollama-k:11434 QDRANT_URL=http://qdrant-k:6333 COLLECTION_NAME=TEST_DOROTEA Importante: Si se cambia el nombre de colecci\u00f3n , hay que reconstruir la imagen para propagar variables al contenedor: docker compose build --no-cache qdrant-search docker compose up -d qdrant-search docker exec -it qdrant-search-k sh cat /app/client_qdrant_flask_docker.py | grep collection_name El microservicio devolv\u00eda Not found: Collection 'prueba_CRAG3' doesn\u2019t exist! aun cuando COLLECTION_NAME=TEST_DOROTEA . Causa : el contenedor estaba corriendo con capa antigua (variables de una build previa). Soluci\u00f3n : rebuild del servicio. Tras eso, las consultas desde n8n y desde la red Docker funcionaron.","title":"Servicio qdrant-search (Flask + Qdrant Client + Ollama)"},{"location":"implementacion/02_implementacion-dorotea/#gpu-y-modelos-ollama","text":"Verificaci\u00f3n de uso de GPU con nvidia-smi : durante generaci\u00f3n se observ\u00f3 GPU-Util 100% en la A30 y consumo de VRAM acorde a los modelos cargados. Sin embargo, solo en una de las 2 placas. tambi\u00e9n se observ\u00f3 que Logs de Ollama mostraron loaded CUDA backend , \u201cVRAM available/required\u201d y selecci\u00f3n de GPULayers , confirmando aceleraci\u00f3n CUDA. Mensaje \u201ctruncating input prompt (limit=4096)\u201d : es el l\u00edmite de contexto del modelo en tokens . Si el prompt (incluida la conversaci\u00f3n previa) lo supera, Ollama lo recorta. Mitigaci\u00f3n: prompts m\u00e1s breves o modelos con mayor contexto (cuando est\u00e9n disponibles).","title":"GPU y modelos (Ollama)"},{"location":"implementacion/02_implementacion-dorotea/#resultados-de-las-pruebas-funcionales-en-dorotea","text":"Objetivo: Validar el funcionamiento integral del stack con aceleraci\u00f3n GPU, recolecci\u00f3n de m\u00e9tricas en Grafana y generaci\u00f3n de embeddings mediante Ollama y Qdrant, utilizando la colecci\u00f3n TEST_DOROTEA como caso de prueba.","title":"Resultados de las pruebas funcionales en Dorotea"},{"location":"implementacion/02_implementacion-dorotea/#1-pruebas-de-carga-uso-extremo-de-cpu-profile-cpu","text":"Durante la primera etapa de las pruebas se intent\u00f3 ejecutar un perfil intensivo del modelo ( Profile CPU ), pero el proceso satur\u00f3 completamente los n\u00facleos de la m\u00e1quina, llevando el uso de CPU al 100% en todos los hilos. El proceso fue ejecutado desde Ollama ( /usr/bin/ollama runner ) con m\u00faltiples hilos en paralelo, observ\u00e1ndose que cada uno ejecutaba la carga de modelo en puertos independientes ( --port 33137 ). Este test qued\u00f3 en ejecuci\u00f3n continua sin completar, por lo que se detuvo manualmente para evitar degradar el rendimiento del resto de los contenedores.","title":"1. Pruebas de carga \u2014 Uso extremo de CPU (Profile CPU)"},{"location":"implementacion/02_implementacion-dorotea/#2-monitoreo-con-grafana-y-cadvisor","text":"Durante la ejecuci\u00f3n se dej\u00f3 Grafana activo con los dashboards de Prometheus y cAdvisor recolectando m\u00e9tricas en tiempo real de todos los contenedores. El monitoreo permiti\u00f3 observar: N\u00famero de contenedores activos: 13 Procesos m\u00e1s demandantes de CPU: grobid-k (36.6%) cadvisor-k (10.9%) pgadmin-k , n8n-k , prometheus-k en menor proporci\u00f3n Consumo de memoria: promedio general bajo, sin indicios de swapping ni saturaci\u00f3n. Estos datos validan que el entorno completo puede ejecutarse de manera estable durante largos periodos, incluso con procesos intensivos como el parsing de papers o la generaci\u00f3n de embeddings.","title":"2. Monitoreo con Grafana y cAdvisor"},{"location":"implementacion/02_implementacion-dorotea/#3-carga-de-datos-creacion-de-la-coleccion-test_dorotea","text":"Tras estabilizar el entorno, se procedi\u00f3 a realizar una carga de 10 papers cient\u00edficos en formato JSON procesados por el pipeline Grobid \u2192 Ollama \u2192 Qdrant. Duraci\u00f3n total del proceso: aproximadamente 2 horas . Esto sugiere que el cuello de botella podr\u00eda encontrarse en: Procesamiento de embeddings dentro de Ollama ( nomic-embed-text ). Paralelizaci\u00f3n limitada o falta de optimizaci\u00f3n en la inserci\u00f3n de datos en Qdrant. Se recomienda analizar en profundidad la arquitectura de Ollama (modelos, batch size, n\u00famero de workers) para reducir los tiempos de carga en pruebas futuras.","title":"3. Carga de datos \u2014 Creaci\u00f3n de la colecci\u00f3n TEST_DOROTEA"},{"location":"implementacion/02_implementacion-dorotea/#resultado","text":"La colecci\u00f3n TEST_DOROTEA se cre\u00f3 correctamente, con estado green y 554 puntos vectoriales distribuidos en 8 segmentos .","title":"Resultado:"},{"location":"implementacion/02_implementacion-dorotea/#4-visualizacion-de-la-coleccion-en-qdrant","text":"La exploraci\u00f3n de la colecci\u00f3n en la interfaz de Qdrant mostr\u00f3 una distribuci\u00f3n coherente de los embeddings en el espacio vectorial, confirmando que los puntos fueron generados correctamente y los metadatos (contenido, fuente, etc.) se mantienen accesibles.","title":"4. Visualizaci\u00f3n de la colecci\u00f3n en Qdrant"},{"location":"implementacion/02_implementacion-dorotea/#5-pruebas-de-busqueda-semantica","text":"Una vez creada la colecci\u00f3n, se probaron distintas consultas al agente a trav\u00e9s de qdrant-search-k , verificando que las respuestas generadas correspond\u00edan a la sem\u00e1ntica de las preguntas y retornaban papers relevantes.","title":"5. Pruebas de b\u00fasqueda sem\u00e1ntica"},{"location":"implementacion/02_implementacion-dorotea/#consulta-1","text":"Pregunta: \u00bfQu\u00e9 trabajos hablan sobre Kubernetes o autoscaling en contenedores? Respuesta del agente: No specific information was found in the texts to answer the question regarding the types of jobs (trabajos) related to Kubernetes or autoscaling in containers. Recommended Papers: Adaptive AI-based auto-scaling for Kubernetes Predictive Hybrid Autoscaling for Containerized Applications Time series forecasting-based Kubernetes autoscaling using Facebook Prophet and Long Short-Term Memory","title":"\ud83d\udd39 Consulta 1"},{"location":"implementacion/02_implementacion-dorotea/#consulta-2","text":"Pregunta: Dame referencias sobre los papers que mencionan MLOps o pipelines de machine learning. Respuesta del agente: Here are the references to papers that mention MLOps or pipelines in machine learning: \"MLOps: A Taxonomy and a Methodology\" \"Towards MLOps: A Case Study of ML Pipeline Platform\" \"MLOps: A Taxonomy and a Methodology\" \"MLOps: A Taxonomy and a Methodology\" \"MLOps: A Taxonomy and a Methodology\" These papers provide an overview of the MLOps methodology, its importance, and its application in industrial environments. They also discuss various aspects of MLOps, including pipelines, DevOps principles, and benchmarking tools. The papers also mention other relevant topics such as: Machine Learning Operations (MLOps) ML pipeline platforms CI/CD routines Automated ML triggering Model registry storage Monitoring and performance You can find more information about these papers by accessing the references provided in the database results. Recommended Papers: MLOps: A Taxonomy and a Methodology Towards MLOps: A Case Study of ML Pipeline Platform","title":"\ud83d\udd39 Consulta 2"},{"location":"implementacion/02_implementacion-dorotea/#consulta-3","text":"Pregunta: \u00bfQu\u00e9 papers analizan la eficiencia energ\u00e9tica o el consumo de energ\u00eda en la nube? Respuesta del agente: \"Energy efficiency is always a major concern in cloud computing and when it comes to the application level many recommendations have been made to optimise energy usage at the system level.\" \"No specific information was found in the texts to answer the question about papers that analyze the efficiency of energy consumption or energy consumption in the nube, specifically about papers with titles that include \"Energy efficiency in cloud computing data centers\" and \"Energy efficiency\" or mention \"energy consumption\".\" Recommended Papers: Energy efficiency in cloud computing data centers: a survey on software technologies Machine Learning-based Orchestration of Containers: A Taxonomy and Future Directions","title":"\ud83d\udd39 Consulta 3"},{"location":"implementacion/02_implementacion-dorotea/#consulta-4","text":"Pregunta: \u00bfCu\u00e1les de los papers hacen una revisi\u00f3n o resumen del estado del arte sobre tecnolog\u00edas de contenedores? Respuesta del agente: No specific information was found in the texts to answer the question about papers that analyze the efficiency of energy consumption or energy consumption in the nube. Recommended Papers: Container Technologies for ARM Architecture: A Comprehensive Survey of the State-of-the-Art A novel energy-aware resource management technique using joint VM and container consolidation approach for green computing in cloud data centers","title":"\ud83d\udd39 Consulta 4"},{"location":"implementacion/02_implementacion-dorotea/#consulta-5","text":"Pregunta: \u00bfQu\u00e9 trabajos proponen nuevos algoritmos o m\u00e9todos para mejorar la elasticidad o el escalado en la nube? Respuesta del agente: \"No specific information was found in the texts to answer the question about new algorithms or methods for improving elasticity or scaling in the cloud.\" Recommended Papers: A New Infrastructure Elasticity Control Algorithm for Containerized Cloud Adaptive AI-based auto-scaling for Kubernetes Time series forecasting-based Kubernetes autoscaling using Facebook Prophet and Long Short-Term Memory","title":"\ud83d\udd39 Consulta 5"},{"location":"implementacion/02_implementacion-dorotea/#test-usando-openwebui-preguntas-en-ingles","text":"Una vez dentro de OpenwebUI, el primer paso es configurar el Pipe que se utilizar\u00e1. Para ello, sigue los pasos explicados en el archivo 01_implementacion-final.md . La \u00fanica diferencia en este caso fue el c\u00f3digo empleado para configurar el Pipe, que se encuentra en [ DOCUMENTACION/pps-kevin_haponiuk-2025/src/self-hosted-ai-starter-kit/n8n-pipeline-function-dorotea.py ]. Las preguntas realizadas fueron las mismas que en pruebas anteriores, pero esta vez en ingl\u00e9s. Which papers discuss Kubernetes or autoscaling in containers? Provide references to papers that mention MLOps or machine learning pipelines. En este caso, se realiz\u00f3 la misma pregunta por segunda vez, ya que la primera respuesta fue algo imprecisa. Esto permiti\u00f3 observar c\u00f3mo se comporta el sistema al repetir una consulta. Which papers analyze energy efficiency or energy consumption in the cloud? Which papers provide a review or state-of-the-art summary on container technologies? Which papers propose new algorithms or methods to improve elasticity or scaling in the cloud? Es importante destacar que las respuestas del agente mejoraron significativamente al realizar las preguntas en ingl\u00e9s. Esto se debe a que el modelo de Ollama est\u00e1 entrenado principalmente en ingl\u00e9s, lo que optimiza la calidad y precisi\u00f3n de las respuestas.","title":"Test usando OpenwebUI + Preguntas en ingl\u00e9s"},{"location":"implementacion/02_implementacion-dorotea/#conclusiones","text":"El pipeline completo oper\u00f3 de manera estable , abarcando desde la extracci\u00f3n y an\u00e1lisis de los papers hasta la indexaci\u00f3n vectorial y la consulta sem\u00e1ntica. El rendimiento general fue satisfactorio , aunque la carga inicial de datos result\u00f3 lenta (\u22482 horas para 10 documentos) , indicando posibles cuellos de botella en el procesamiento de embeddings o la inserci\u00f3n en Qdrant. Las respuestas del agente fueron sem\u00e1nticamente relevantes , con sugerencias precisas y papers correctamente asociados, validando el correcto funcionamiento de los embeddings y la b\u00fasqueda vectorial en Qdrant. Adem\u00e1s, se observ\u00f3 que el agente mejora significativamente sus respuestas cuando las preguntas se formulan en ingl\u00e9s, debido a que el modelo de Ollama est\u00e1 entrenado principalmente en este idioma. Esto optimiza la calidad y precisi\u00f3n de las respuestas, haciendo que las consultas sean m\u00e1s efectivas. Se recomienda explorar optimizaciones en: Uso de GPU en Ollama : mejorar la paralelizaci\u00f3n durante la generaci\u00f3n de embeddings. Segmentaci\u00f3n del proceso de carga : dividir tareas para reducir tiempos. Modelos alternativos : evaluar arquitecturas m\u00e1s eficientes para embeddings masivos. La infraestructura desplegada en Dorotea demostr\u00f3 ser robusta, con capacidad de procesamiento adecuada y un rendimiento aceptable para pruebas de investigaci\u00f3n y desarrollo, as\u00ed como para futuras escalas.","title":"Conclusiones"},{"location":"implementacion/02_implementacion-dorotea/#trabajo-futuro","text":"Migrar a la Opci\u00f3n 1 : Consolidar los servicios detr\u00e1s de un \u00fanico puerto y 4 subpaths en NGINX para simplificar el acceso y mejorar la organizaci\u00f3n. Fortalecer la seguridad : Implementar autenticaci\u00f3n en NGINX o integrar un sistema de inicio de sesi\u00f3n \u00fanico (SSO). Configurar TLS end-to-end si se planea exponer los servicios m\u00e1s all\u00e1 del t\u00fanel SSH (actualmente suficiente para pruebas internas). Optimizar el pipeline : Analizar el uso de GPU en Ollama para maximizar la eficiencia. Evaluar herramientas o t\u00e9cnicas para acelerar la carga de datos en Qdrant. Considerar modelos con mayor capacidad de contexto para mejorar las respuestas sem\u00e1nticas. Monitoreo avanzado : Ampliar los dashboards en Grafana para incluir m\u00e9tricas espec\u00edficas de GPU y rendimiento de embeddings. Automatizar alertas para detectar posibles cuellos de botella o fallos en tiempo real.","title":"Trabajo futuro"},{"location":"implementacion/extra_00_problematica-agent/","text":"Generalmente es mucho m\u00e1s com\u00fan encontrar este tipo de arquitectura en YouTube o foros con APIs conectadas a alg\u00fan modelo, por ejemplo ChatGPT, que termina funcionando bien (se termina omitiendo la herramienta que se llama Infopapers y se conecta directamente a la base de datos). Sin embargo, el objetivo de este proyecto es realizar todo de manera local. Para esto, N8N en las herramientas requiere este tipo de arquitectura, es decir, conectando Infopapers que requiere poner un modelo m\u00e1s de Ollama. Se estuvo probando con distintos modelos y distintos prompts, pero no se pudo solucionar la problem\u00e1tica. Lo que termina haciendo es que, una vez recibida la query, al momento de realizar la b\u00fasqueda en la base de datos funciona correctamente y devuelve bien, pero el modelo que est\u00e1 en Infopapers termina generando una respuesta que le entrega al siguiente modelo del AI Agent, causando un doble procesamiento que termina afectando la respuesta y no funcionando como se espera. Por lo tanto, se busc\u00f3 otra forma de armar la arquitectura, sabiendo que la base de datos est\u00e1 respondiendo bien pero la problem\u00e1tica est\u00e1 luego de la b\u00fasqueda en la base de datos.","title":"Extra 00 problematica agent"},{"location":"investigacion/01_llm/","text":"\u00bfC\u00f3mo funciona un Modelo de Lenguaje Generativo? \ud83e\udde0\ud83d\udcac Para poder desarrollar una soluci\u00f3n efectiva, es fundamental comprender qu\u00e9 ocurre detr\u00e1s de escena cuando le hacemos una solicitud a una inteligencia artificial de texto generativo como puede ser ChatGPT. En esta secci\u00f3n vamos a desglosar y explicar, de forma simple y conceptual, los elementos esenciales que conforman un modelo de lenguaje generativo. \u00bfQu\u00e9 es Machine Learning? El Machine Learning es una rama de la inteligencia artificial que permite a las computadoras aprender de los datos sin que sea necesario programarlas expl\u00edcitamente para cada tarea. Una definici\u00f3n cl\u00e1sica lo describe as\u00ed: Machine Learning es el campo de estudio que da a las computadoras la capacidad de aprender sin ser expl\u00edcitamente programadas. \u2014 Arthur Samuel, 1959 Otra, m\u00e1s enfocada en su aplicaci\u00f3n, dice: Un programa aprende de la experiencia (E), con respecto a una tarea (T) y una medida de rendimiento (P), si mejora su rendimiento en T, medido por P, gracias a E. \u2014 Tom Mitchell, 1997 Un ejemplo cotidiano Tomemos como ejemplo el filtro de spam . Este sistema analiza miles de correos que los usuarios marcaron como spam o no spam, y con eso aprende a detectar patrones para clasificar correctamente nuevos correos. Aqu\u00ed: Tarea (T): Clasificar correos como spam o no spam. Experiencia (E): Correos etiquetados por usuarios. Rendimiento (P): Porcentaje de correos clasificados correctamente (precisi\u00f3n o accuracy ). Con suficiente experiencia, el sistema mejora su desempe\u00f1o en la tarea, sin necesidad de reglas escritas por humanos. Tipos de aprendizaje en Machine Learning Aprendizaje Supervisado : Es como aprender a tocar el piano con un profesor. El algoritmo se entrena con datos etiquetados, es decir, con las respuestas correctas ya conocidas. Aprende comparando sus resultados con las respuestas reales y ajustando su comportamiento. Se utiliza, por ejemplo, en sistemas que filtran correos spam. Aprendizaje No Supervisado : Similar a armar un rompecabezas sin saber c\u00f3mo deber\u00eda verse al final. No se proporcionan etiquetas ni respuestas, y el algoritmo debe descubrir patrones o agrupaciones por s\u00ed mismo. Un uso com\u00fan es la segmentaci\u00f3n de clientes en marketing. Aprendizaje por Refuerzo : Como entrenar a una mascota con premios y castigos. El algoritmo aprende a trav\u00e9s de prueba y error, recibiendo recompensas por buenas acciones y penalizaciones por errores. Es muy usado en videojuegos y rob\u00f3tica, donde el sistema mejora con la experiencia. Figura 1: Tipos de aprendizajes en machine learning \u00bfQu\u00e9 es una red neuronal? Las redes neuronales artificiales ( Artificial Neural Networks , o ANN) son modelos matem\u00e1ticos inspirados en el funcionamiento del cerebro humano . Por medio de una colecci\u00f3n de unidades llamadas neuronas artificiales , organizadas en capas y conectadas entre s\u00ed, que aprende a realizar tareas complejas mediante la experiencia , es decir, ajustando sus par\u00e1metros a partir de los datos de entrenamiento. Gracias a esta capacidad, pueden realizar tareas como clasificar im\u00e1genes, reconocer voz o jugar videojuegos, todo esto aprendiendo a partir de datos . Inspiraci\u00f3n biol\u00f3gica Una neurona biol\u00f3gica es una c\u00e9lula del cerebro que recibe se\u00f1ales de otras neuronas a trav\u00e9s de estructuras llamadas dendritas , procesa esa informaci\u00f3n en el cuerpo celular, y transmite se\u00f1ales hacia otras neuronas a trav\u00e9s del ax\u00f3n . Cuando recibe suficientes est\u00edmulos, la neurona \"dispara\" una se\u00f1al el\u00e9ctrica. Figura 2: Una neurona biol\u00f3gica Figura 3: Representaci\u00f3n de una red neuronal humana De forma an\u00e1loga, una neurona artificial recibe m\u00faltiples entradas num\u00e9ricas (como los datos de una imagen o un texto), las combina mediante una funci\u00f3n matem\u00e1tica y produce una salida. Las conexiones entre neuronas se representan con pesos, que determinan la importancia de cada entrada. Al entrenar la red, estos pesos se ajustan para mejorar el desempe\u00f1o del modelo. Figura 4: Representaci\u00f3n conceptual de una red neuronal artificial Un poco de historia Las primeras redes neuronales fueron propuestas en 1943 por McCulloch y Pitts , quienes crearon un modelo muy simplificado de neurona artificial capaz de realizar c\u00e1lculos l\u00f3gicos (como AND, OR y NOT). Este fue el punto de partida para una larga evoluci\u00f3n. Durante d\u00e9cadas, las redes neuronales pasaron por varios ciclos de entusiasmo y olvido, hasta que en la \u00faltima d\u00e9cada resurgieron con fuerza, gracias a: la disponibilidad de grandes cantidades de datos (Big Data), el aumento exponencial de la capacidad de c\u00f3mputo (especialmente con GPUs y TPUs), y las mejoras en los algoritmos de entrenamiento (como el descenso por gradiente y sus variantes). Todo esto permiti\u00f3 entrenar redes neuronales m\u00e1s profundas y complejas, dando lugar al auge de ia que hoy presenciamos. \u00bfQu\u00e9 es un LLM? Un LLM ( Large Language Model , o Modelo de Lenguaje Grande ) es un tipo de modelo de inteligencia artificial entrenado para comprender, generar y trabajar con lenguaje natural de manera fluida. Estos modelos est\u00e1n basados en redes neuronales profundas, especialmente en arquitecturas conocidas como transformers (explicado m\u00e1s adelante), que le permite procesar y generar texto a gran escala. Lo que hace especial a un LLM es la enorme cantidad de datos con los que ha sido entrenado (normalmente textos provenientes de libros, art\u00edculos, p\u00e1ginas web y otros contenidos p\u00fablicos) y la gran cantidad de par\u00e1metros que posee, muchas veces llegando a miles de millones de conexiones . Gracias a esto, los LLM pueden: Redactar textos coherentes y contextuales. Traducir entre distintos idiomas. Resumir documentos extensos. Responder preguntas de forma informada. Asistir en programaci\u00f3n, an\u00e1lisis de datos, y mucho m\u00e1s. \u00bfQu\u00e9 es el lenguaje natural y c\u00f3mo puede interpretarlo la IA? El lenguaje natural es la forma de comunicaci\u00f3n que usamos los seres humanos en nuestra vida cotidiana, como el espa\u00f1ol, ingl\u00e9s, franc\u00e9s, etc. Se caracteriza por ser rico, flexible y ambiguo , y est\u00e1 lleno de matices culturales, contextuales y emocionales. En el \u00e1mbito de la computaci\u00f3n, cuando se habla de procesamiento de lenguaje natural (NLP \"Natural Language Processing\", por sus siglas en ingl\u00e9s) , se hace referencia a la capacidad de una m\u00e1quina para entender, interpretar, generar y responder a textos o conversaciones escritos u orales del mismo modo en que lo har\u00eda una persona. Cuando escribimos un texto como: \"El gato duerme en el sof\u00e1\" Este texto pasa por varios pasos para que la IA lo entienda: 1. Codificaci\u00f3n num\u00e9rica Lo primero que debemos hacer es convertir las palabras en un formato que la computadora pueda entender , como por ejemplo una codificaci\u00f3n num\u00e9rica. Una forma sencilla de hacerlo es utilizando c\u00f3digos como ASCII , donde a cada car\u00e1cter se le asigna un n\u00famero espec\u00edfico. Figura 5: Proceso de codificaci\u00f3n 2. Tokenizaci\u00f3n A nuestra red neuronal la entrenamos aliment\u00e1ndola con una gran cantidad de datos ya codificados . A medida que procesa esta informaci\u00f3n, aprende a reconocer patrones repetitivos en el lenguaje. Estos patrones, conocidos como tokens , se van organizando y catalogando en una especie de vocabulario interno o lista, que luego usar\u00e1 para comprender y generar texto. Figura 6: Red neuronal encontrando patrones \ud83d\udd0d Aclaraci\u00f3n: un token no es necesariamente una palabra. Puede ser una palabra completa, una parte de una palabra, o incluso un conjunto de palabras o s\u00edmbolos, dependiendo del modelo y del sistema de tokenizaci\u00f3n utilizado. 3. Compresi\u00f3n Una vez que nuestra red neuronal ha identificado los patrones en los datos, el siguiente paso es representar cada token mediante un n\u00famero o identificador \u00fanico (ID) . Esta conversi\u00f3n permite simplificar el procesamiento y optimizar el almacenamiento , ya que trabajar con n\u00fameros es mucho m\u00e1s eficiente que hacerlo directamente con texto. Por ejemplo: Token Token completo ID hola 72 111 108 97 1923 hace 104 97 99 101 1234 fr\u00edo 102 114 237 111 6543 De esta forma, cuando la red procesa una frase como \"hola hace fr\u00edo\", en realidad est\u00e1 trabajando internamente con una secuencia de n\u00fameros: 1923, 1234, 6543 . Figura 7: Representaci\u00f3n mediante ID 4. Marcadores / Embedding Es importante entender que el modelo no \"sabe\" lo que significa cada palabra como lo har\u00eda una persona. No tiene una comprensi\u00f3n sem\u00e1ntica real de los t\u00e9rminos. Sin embargo, s\u00ed puede identificar qu\u00e9 tokens est\u00e1n relacionados entre s\u00ed gracias a c\u00f3mo se organizan en el espacio de embedding. Por ejemplo, puede aprender que el token \"la\" aparece frecuentemente cerca de \"reina\" , y por eso los ubica cerca en el espacio vectorial . Esa proximidad representa una relaci\u00f3n estad\u00edstica, no un entendimiento real del lenguaje. Esta capacidad de asociar t\u00e9rminos seg\u00fan su contexto permite que el modelo genere texto coherente y mantenga el sentido general, aunque no comprenda el significado como lo har\u00edamos nosotros. Figura 8: Ejemplo de relaci\u00f3n De esta manera, puedo encontrar relaciones entre los token y agregar una marca en cada ocasi\u00f3n que tengan relaci\u00f3n entre ellos: Figura 9: marcaci\u00f3n entre tokens la idea es seguir sumando marcadores entre distintos token, de esta manera el modelo aunque no sabe exactamente qu\u00e9 significan, s\u00ed saber como clasificarlos. Figura x: Relaci\u00f3n entre tokens Esto se puede extender hasta cientos o miles de marcadores (dimensiones), formando lo que podr\u00edamos imaginar como un tipo de ADN de la palabra (o del token, en general). Cada token se representa mediante un vector num\u00e9rico de gran dimensi\u00f3n , donde cada posici\u00f3n del vector captura una caracter\u00edstica o relaci\u00f3n aprendida del contexto . As\u00ed, dos tokens con significados o usos similares tendr\u00e1n vectores parecidos. \ud83d\udccc Nota: aunque en este caso hablamos de \"token = palabra\", un token tambi\u00e9n puede ser parte de una palabra o incluso una secuencia de palabras, seg\u00fan el modelo de tokenizaci\u00f3n utilizado. Figura 10: Relaci\u00f3n entre tokens Otra forma de representar estos vectores es imaginarlos en un espacio tridimensional . En la pr\u00e1ctica, cada marcador corresponde a una dimensi\u00f3n, por lo que los vectores pueden tener cientos o miles de dimensiones . Sin embargo, al proyectarlos en 3D podemos visualizar mejor la relaci\u00f3n de cercan\u00eda entre tokens . Esta idea de \"cercan\u00eda sem\u00e1ntica\" nos permite ver, por ejemplo, que tokens como \"rey\" y \"reina\" est\u00e1n m\u00e1s cerca entre s\u00ed que de \"auto\" . Esta representaci\u00f3n permite entender c\u00f3mo el modelo \"comprende\" las relaciones entre conceptos sin saber realmente qu\u00e9 significan. El siguiente ejemplo fue obtenido de esta herramienta de visualizaci\u00f3n de embeddings de TensorFlow , donde pod\u00e9s explorar estas relaciones en 2D o 3D de forma interactiva. Figura 11: Representaci\u00f3n tridimensional embedding En este caso, podemos ver que palabras cercanas a \"argentina\" son: Figura 12: Puntos m\u00e1s cercanos a la palabra \"argentina\" Esto permite realizar operaciones matem\u00e1ticas entre vectores que representan palabras, tambi\u00e9n conocidos como embeddings . Gracias a esta propiedad, se pueden hacer analog\u00edas sem\u00e1nticas, como por ejemplo: \"CORONA\" + \"HOMBRE\" \u2248 \"REY\" \"REY\" - \"HOMBRE\" + \"MUJER\" \u2248 \"REINA\" Estos c\u00e1lculos se basan en relaciones sem\u00e1nticas aprendidas por el modelo a partir de grandes cantidades de texto. As\u00ed, los embeddings no solo capturan el significado de las palabras, sino tambi\u00e9n sus relaciones contextuales. 5. Normalizaci\u00f3n y Lematizaci\u00f3n Cuando trabajamos con texto, especialmente al darle instrucciones a una IA, hay muchos t\u00e9rminos que no aportan valor significativo. Palabras como \"la\", \"lo\", \"el\", \"y\", \"con\" , o incluso los signos de puntuaci\u00f3n , suelen ser irrelevantes para el an\u00e1lisis, ya que aparecen en casi todos los contextos y no cambian el significado central de una frase. Para simplificar y optimizar el procesamiento, lo primero que hacemos es limpiar los textos antes de pasarlos al sistema. Veamos un ejemplo con la frase: \"El gato est\u00e1 durmiendo en el sof\u00e1.\" Frase original: \"El gato est\u00e1 durmiendo en el sof\u00e1.\" Eliminaci\u00f3n de palabras irrelevantes y signos de puntuaci\u00f3n: \"gato est\u00e1 durmiendo sof\u00e1\" Lematizaci\u00f3n (reducci\u00f3n de las palabras a su forma base): \"gato es dormir sof\u00e1\" Tokenizaci\u00f3n (divisi\u00f3n en unidades b\u00e1sicas): \"gato\", \"dormir\", \"sof\u00e1\" Esta frase queda comprimida y m\u00e1s eficiente para el procesamiento por parte del modelo. En lugar de trabajar con muchas palabras y tokens innecesarios, esta versi\u00f3n reduce el texto a su m\u00ednima expresi\u00f3n \u00fatil , acelerando la b\u00fasqueda y el an\u00e1lisis dentro del modelo. Modelos como ChatGPT preparan internamente nuestra entrada de esta forma para compararla con representaciones (embeddings) similares y generar una respuesta adecuada. Este proceso implica una p\u00e9rdida de informaci\u00f3n , ya que comprimimos el mensaje original. Sin embargo, esto no es algo negativo: la IA no repite textualmente, sino que genera una respuesta original basada en la comprensi\u00f3n del contenido. Un buen ejemplo ser\u00eda escuchar una clase y al d\u00eda siguiente explicarle el concepto a alguien m\u00e1s: no recordamos todas las palabras exactas, pero s\u00ed entendemos y transmitimos la idea principal. 6. Sampling El sampling (muestreo) es una t\u00e9cnica que permite al modelo generar respuestas de forma m\u00e1s variada y creativa , introduciendo un componente aleatorio en el proceso de generaci\u00f3n de texto. En lugar de elegir siempre la palabra m\u00e1s probable seg\u00fan el modelo, el sampling permite explorar otras opciones que, aunque sean ligeramente menos probables, siguen siendo coherentes dentro del contexto. Es como moverse ligeramente dentro del espacio de embeddings , eligiendo caminos diferentes que llevan a respuestas distintas pero igualmente v\u00e1lidas. Esto evita que el modelo siempre d\u00e9 las mismas respuestas ante los mismos inputs y permite generar resultados m\u00e1s naturales y diversos , sin perder el sentido general de la conversaci\u00f3n. Evoluci\u00f3n de las Redes Neuronales Las redes neuronales tradicionales tienen una gran limitaci\u00f3n: no tienen memoria . Es decir, procesan cada entrada (input) de manera aislada, sin recordar lo que pas\u00f3 antes. Pero, \u00bfc\u00f3mo puede una red entender el contexto de una conversaci\u00f3n o el significado completo de una frase si solo ve una palabra a la vez? Redes Neuronales Recurrentes (RNN) Para resolver este problema apareci\u00f3 una arquitectura llamada Red Neuronal Recurrente (RNN), que introduce la idea de \"memoria\" al alimentar el resultado de un paso como entrada del siguiente. Procesan palabra por palabra en secuencia . Cada palabra se analiza considerando lo anterior. Problema: pierden contexto cuando el texto es muy largo. No permiten paralelizar , por lo que el entrenamiento es m\u00e1s lento. Figura 13: Ejemplo de RNN LSTM (Long Short-Term Memory) Las LSTM son una mejora de las RNN tradicionales. Introducen una especie de \"filtro de memoria\" que decide qu\u00e9 informaci\u00f3n mantener y cu\u00e1l olvidar. Manejan mejor el contexto de largo plazo. Son m\u00e1s precisas que las RNN simples. Aun as\u00ed, siguen siendo secuenciales y no paralelizables , lo que limita su eficiencia. El Gran Cambio: Transformers El gran salto lleg\u00f3 en 2017 con el paper \u201cAttention is All You Need\u201d , que introdujo el modelo Transformer . Esta arquitectura cambi\u00f3 por completo la forma en que las redes procesan secuencias. \u00bfQu\u00e9 aportan los Transformers? Utilizan un mecanismo llamado Self-Attention para identificar la importancia de cada palabra en relaci\u00f3n con todas las dem\u00e1s, en paralelo . Permiten analizar todas las palabras al mismo tiempo , no de forma secuencial. Esto facilita el entrenamiento y mejora la comprensi\u00f3n del contexto completo . Son altamente paralelizables , lo que acelera enormemente el proceso. Ejemplo con Self-Attention En la frase: \u201cLa vida es bella, as\u00ed que v\u00edvela cada d\u00eda.\u201d El modelo analiza cada palabra en relaci\u00f3n con todas las dem\u00e1s. Por ejemplo: \"la\" con respecto a \"vida\", \"bella\", \"v\u00edvela\", etc. \"vida\" con respecto a todas las dem\u00e1s tambi\u00e9n. Figura 14: Ejemplo de uso de transformers Figura 15: Ejemplo de paralelizaci\u00f3n Esto permite entender cu\u00e1les son las palabras m\u00e1s importantes para construir el significado global de la frase, incluso si est\u00e1n separadas por otras palabras. Gracias a los Transformers, hoy podemos contar con modelos como GPT, BERT y otros LLM (Large Language Models) que comprenden mejor el lenguaje y generan respuestas m\u00e1s coherentes, r\u00e1pidas y \u00fatiles. Bibliograf\u00eda: https://www.youtube.com/watch?v=FdZ8LKiJBhQ&list=PL3ei_Xb7-ic5pkJDTplPxWvE8t13mm19W&index=8 https://www.ntiva.com/blog/what-is-machine-learning Aur\u00e9lien G\u00e9ron - Hands-On Machine Learning with Scikit-Learn, Keras and TensorFlow http://personal.cimat.mx:8181/~mrivera/cursos/aprendizaje_profundo/RNN_LTSM/introduccion_rnn.html https://visajourneypro.com/ https://projector.tensorflow.org/ https://arxiv.org/abs/1706.03762","title":"Conceptos Previos"},{"location":"investigacion/01_llm/#como-funciona-un-modelo-de-lenguaje-generativo","text":"Para poder desarrollar una soluci\u00f3n efectiva, es fundamental comprender qu\u00e9 ocurre detr\u00e1s de escena cuando le hacemos una solicitud a una inteligencia artificial de texto generativo como puede ser ChatGPT. En esta secci\u00f3n vamos a desglosar y explicar, de forma simple y conceptual, los elementos esenciales que conforman un modelo de lenguaje generativo.","title":"\u00bfC\u00f3mo funciona un Modelo de Lenguaje Generativo? \ud83e\udde0\ud83d\udcac"},{"location":"investigacion/01_llm/#que-es-machine-learning","text":"El Machine Learning es una rama de la inteligencia artificial que permite a las computadoras aprender de los datos sin que sea necesario programarlas expl\u00edcitamente para cada tarea. Una definici\u00f3n cl\u00e1sica lo describe as\u00ed: Machine Learning es el campo de estudio que da a las computadoras la capacidad de aprender sin ser expl\u00edcitamente programadas. \u2014 Arthur Samuel, 1959 Otra, m\u00e1s enfocada en su aplicaci\u00f3n, dice: Un programa aprende de la experiencia (E), con respecto a una tarea (T) y una medida de rendimiento (P), si mejora su rendimiento en T, medido por P, gracias a E. \u2014 Tom Mitchell, 1997","title":"\u00bfQu\u00e9 es Machine Learning?"},{"location":"investigacion/01_llm/#un-ejemplo-cotidiano","text":"Tomemos como ejemplo el filtro de spam . Este sistema analiza miles de correos que los usuarios marcaron como spam o no spam, y con eso aprende a detectar patrones para clasificar correctamente nuevos correos. Aqu\u00ed: Tarea (T): Clasificar correos como spam o no spam. Experiencia (E): Correos etiquetados por usuarios. Rendimiento (P): Porcentaje de correos clasificados correctamente (precisi\u00f3n o accuracy ). Con suficiente experiencia, el sistema mejora su desempe\u00f1o en la tarea, sin necesidad de reglas escritas por humanos.","title":"Un ejemplo cotidiano"},{"location":"investigacion/01_llm/#tipos-de-aprendizaje-en-machine-learning","text":"Aprendizaje Supervisado : Es como aprender a tocar el piano con un profesor. El algoritmo se entrena con datos etiquetados, es decir, con las respuestas correctas ya conocidas. Aprende comparando sus resultados con las respuestas reales y ajustando su comportamiento. Se utiliza, por ejemplo, en sistemas que filtran correos spam. Aprendizaje No Supervisado : Similar a armar un rompecabezas sin saber c\u00f3mo deber\u00eda verse al final. No se proporcionan etiquetas ni respuestas, y el algoritmo debe descubrir patrones o agrupaciones por s\u00ed mismo. Un uso com\u00fan es la segmentaci\u00f3n de clientes en marketing. Aprendizaje por Refuerzo : Como entrenar a una mascota con premios y castigos. El algoritmo aprende a trav\u00e9s de prueba y error, recibiendo recompensas por buenas acciones y penalizaciones por errores. Es muy usado en videojuegos y rob\u00f3tica, donde el sistema mejora con la experiencia. Figura 1: Tipos de aprendizajes en machine learning","title":"Tipos de aprendizaje en Machine Learning"},{"location":"investigacion/01_llm/#que-es-una-red-neuronal","text":"Las redes neuronales artificiales ( Artificial Neural Networks , o ANN) son modelos matem\u00e1ticos inspirados en el funcionamiento del cerebro humano . Por medio de una colecci\u00f3n de unidades llamadas neuronas artificiales , organizadas en capas y conectadas entre s\u00ed, que aprende a realizar tareas complejas mediante la experiencia , es decir, ajustando sus par\u00e1metros a partir de los datos de entrenamiento. Gracias a esta capacidad, pueden realizar tareas como clasificar im\u00e1genes, reconocer voz o jugar videojuegos, todo esto aprendiendo a partir de datos .","title":"\u00bfQu\u00e9 es una red neuronal?"},{"location":"investigacion/01_llm/#inspiracion-biologica","text":"Una neurona biol\u00f3gica es una c\u00e9lula del cerebro que recibe se\u00f1ales de otras neuronas a trav\u00e9s de estructuras llamadas dendritas , procesa esa informaci\u00f3n en el cuerpo celular, y transmite se\u00f1ales hacia otras neuronas a trav\u00e9s del ax\u00f3n . Cuando recibe suficientes est\u00edmulos, la neurona \"dispara\" una se\u00f1al el\u00e9ctrica. Figura 2: Una neurona biol\u00f3gica Figura 3: Representaci\u00f3n de una red neuronal humana De forma an\u00e1loga, una neurona artificial recibe m\u00faltiples entradas num\u00e9ricas (como los datos de una imagen o un texto), las combina mediante una funci\u00f3n matem\u00e1tica y produce una salida. Las conexiones entre neuronas se representan con pesos, que determinan la importancia de cada entrada. Al entrenar la red, estos pesos se ajustan para mejorar el desempe\u00f1o del modelo. Figura 4: Representaci\u00f3n conceptual de una red neuronal artificial","title":"Inspiraci\u00f3n biol\u00f3gica"},{"location":"investigacion/01_llm/#un-poco-de-historia","text":"Las primeras redes neuronales fueron propuestas en 1943 por McCulloch y Pitts , quienes crearon un modelo muy simplificado de neurona artificial capaz de realizar c\u00e1lculos l\u00f3gicos (como AND, OR y NOT). Este fue el punto de partida para una larga evoluci\u00f3n. Durante d\u00e9cadas, las redes neuronales pasaron por varios ciclos de entusiasmo y olvido, hasta que en la \u00faltima d\u00e9cada resurgieron con fuerza, gracias a: la disponibilidad de grandes cantidades de datos (Big Data), el aumento exponencial de la capacidad de c\u00f3mputo (especialmente con GPUs y TPUs), y las mejoras en los algoritmos de entrenamiento (como el descenso por gradiente y sus variantes). Todo esto permiti\u00f3 entrenar redes neuronales m\u00e1s profundas y complejas, dando lugar al auge de ia que hoy presenciamos.","title":"Un poco de historia"},{"location":"investigacion/01_llm/#que-es-un-llm","text":"Un LLM ( Large Language Model , o Modelo de Lenguaje Grande ) es un tipo de modelo de inteligencia artificial entrenado para comprender, generar y trabajar con lenguaje natural de manera fluida. Estos modelos est\u00e1n basados en redes neuronales profundas, especialmente en arquitecturas conocidas como transformers (explicado m\u00e1s adelante), que le permite procesar y generar texto a gran escala. Lo que hace especial a un LLM es la enorme cantidad de datos con los que ha sido entrenado (normalmente textos provenientes de libros, art\u00edculos, p\u00e1ginas web y otros contenidos p\u00fablicos) y la gran cantidad de par\u00e1metros que posee, muchas veces llegando a miles de millones de conexiones . Gracias a esto, los LLM pueden: Redactar textos coherentes y contextuales. Traducir entre distintos idiomas. Resumir documentos extensos. Responder preguntas de forma informada. Asistir en programaci\u00f3n, an\u00e1lisis de datos, y mucho m\u00e1s.","title":"\u00bfQu\u00e9 es un LLM?"},{"location":"investigacion/01_llm/#que-es-el-lenguaje-natural-y-como-puede-interpretarlo-la-ia","text":"El lenguaje natural es la forma de comunicaci\u00f3n que usamos los seres humanos en nuestra vida cotidiana, como el espa\u00f1ol, ingl\u00e9s, franc\u00e9s, etc. Se caracteriza por ser rico, flexible y ambiguo , y est\u00e1 lleno de matices culturales, contextuales y emocionales. En el \u00e1mbito de la computaci\u00f3n, cuando se habla de procesamiento de lenguaje natural (NLP \"Natural Language Processing\", por sus siglas en ingl\u00e9s) , se hace referencia a la capacidad de una m\u00e1quina para entender, interpretar, generar y responder a textos o conversaciones escritos u orales del mismo modo en que lo har\u00eda una persona. Cuando escribimos un texto como: \"El gato duerme en el sof\u00e1\" Este texto pasa por varios pasos para que la IA lo entienda:","title":"\u00bfQu\u00e9 es el lenguaje natural y c\u00f3mo puede interpretarlo la IA?"},{"location":"investigacion/01_llm/#1-codificacion-numerica","text":"Lo primero que debemos hacer es convertir las palabras en un formato que la computadora pueda entender , como por ejemplo una codificaci\u00f3n num\u00e9rica. Una forma sencilla de hacerlo es utilizando c\u00f3digos como ASCII , donde a cada car\u00e1cter se le asigna un n\u00famero espec\u00edfico. Figura 5: Proceso de codificaci\u00f3n","title":"1. Codificaci\u00f3n num\u00e9rica"},{"location":"investigacion/01_llm/#2-tokenizacion","text":"A nuestra red neuronal la entrenamos aliment\u00e1ndola con una gran cantidad de datos ya codificados . A medida que procesa esta informaci\u00f3n, aprende a reconocer patrones repetitivos en el lenguaje. Estos patrones, conocidos como tokens , se van organizando y catalogando en una especie de vocabulario interno o lista, que luego usar\u00e1 para comprender y generar texto. Figura 6: Red neuronal encontrando patrones \ud83d\udd0d Aclaraci\u00f3n: un token no es necesariamente una palabra. Puede ser una palabra completa, una parte de una palabra, o incluso un conjunto de palabras o s\u00edmbolos, dependiendo del modelo y del sistema de tokenizaci\u00f3n utilizado.","title":"2. Tokenizaci\u00f3n"},{"location":"investigacion/01_llm/#3-compresion","text":"Una vez que nuestra red neuronal ha identificado los patrones en los datos, el siguiente paso es representar cada token mediante un n\u00famero o identificador \u00fanico (ID) . Esta conversi\u00f3n permite simplificar el procesamiento y optimizar el almacenamiento , ya que trabajar con n\u00fameros es mucho m\u00e1s eficiente que hacerlo directamente con texto. Por ejemplo: Token Token completo ID hola 72 111 108 97 1923 hace 104 97 99 101 1234 fr\u00edo 102 114 237 111 6543 De esta forma, cuando la red procesa una frase como \"hola hace fr\u00edo\", en realidad est\u00e1 trabajando internamente con una secuencia de n\u00fameros: 1923, 1234, 6543 . Figura 7: Representaci\u00f3n mediante ID","title":"3. Compresi\u00f3n"},{"location":"investigacion/01_llm/#4-marcadores-embedding","text":"Es importante entender que el modelo no \"sabe\" lo que significa cada palabra como lo har\u00eda una persona. No tiene una comprensi\u00f3n sem\u00e1ntica real de los t\u00e9rminos. Sin embargo, s\u00ed puede identificar qu\u00e9 tokens est\u00e1n relacionados entre s\u00ed gracias a c\u00f3mo se organizan en el espacio de embedding. Por ejemplo, puede aprender que el token \"la\" aparece frecuentemente cerca de \"reina\" , y por eso los ubica cerca en el espacio vectorial . Esa proximidad representa una relaci\u00f3n estad\u00edstica, no un entendimiento real del lenguaje. Esta capacidad de asociar t\u00e9rminos seg\u00fan su contexto permite que el modelo genere texto coherente y mantenga el sentido general, aunque no comprenda el significado como lo har\u00edamos nosotros. Figura 8: Ejemplo de relaci\u00f3n De esta manera, puedo encontrar relaciones entre los token y agregar una marca en cada ocasi\u00f3n que tengan relaci\u00f3n entre ellos: Figura 9: marcaci\u00f3n entre tokens la idea es seguir sumando marcadores entre distintos token, de esta manera el modelo aunque no sabe exactamente qu\u00e9 significan, s\u00ed saber como clasificarlos. Figura x: Relaci\u00f3n entre tokens Esto se puede extender hasta cientos o miles de marcadores (dimensiones), formando lo que podr\u00edamos imaginar como un tipo de ADN de la palabra (o del token, en general). Cada token se representa mediante un vector num\u00e9rico de gran dimensi\u00f3n , donde cada posici\u00f3n del vector captura una caracter\u00edstica o relaci\u00f3n aprendida del contexto . As\u00ed, dos tokens con significados o usos similares tendr\u00e1n vectores parecidos. \ud83d\udccc Nota: aunque en este caso hablamos de \"token = palabra\", un token tambi\u00e9n puede ser parte de una palabra o incluso una secuencia de palabras, seg\u00fan el modelo de tokenizaci\u00f3n utilizado. Figura 10: Relaci\u00f3n entre tokens Otra forma de representar estos vectores es imaginarlos en un espacio tridimensional . En la pr\u00e1ctica, cada marcador corresponde a una dimensi\u00f3n, por lo que los vectores pueden tener cientos o miles de dimensiones . Sin embargo, al proyectarlos en 3D podemos visualizar mejor la relaci\u00f3n de cercan\u00eda entre tokens . Esta idea de \"cercan\u00eda sem\u00e1ntica\" nos permite ver, por ejemplo, que tokens como \"rey\" y \"reina\" est\u00e1n m\u00e1s cerca entre s\u00ed que de \"auto\" . Esta representaci\u00f3n permite entender c\u00f3mo el modelo \"comprende\" las relaciones entre conceptos sin saber realmente qu\u00e9 significan. El siguiente ejemplo fue obtenido de esta herramienta de visualizaci\u00f3n de embeddings de TensorFlow , donde pod\u00e9s explorar estas relaciones en 2D o 3D de forma interactiva. Figura 11: Representaci\u00f3n tridimensional embedding En este caso, podemos ver que palabras cercanas a \"argentina\" son: Figura 12: Puntos m\u00e1s cercanos a la palabra \"argentina\" Esto permite realizar operaciones matem\u00e1ticas entre vectores que representan palabras, tambi\u00e9n conocidos como embeddings . Gracias a esta propiedad, se pueden hacer analog\u00edas sem\u00e1nticas, como por ejemplo: \"CORONA\" + \"HOMBRE\" \u2248 \"REY\" \"REY\" - \"HOMBRE\" + \"MUJER\" \u2248 \"REINA\" Estos c\u00e1lculos se basan en relaciones sem\u00e1nticas aprendidas por el modelo a partir de grandes cantidades de texto. As\u00ed, los embeddings no solo capturan el significado de las palabras, sino tambi\u00e9n sus relaciones contextuales.","title":"4. Marcadores / Embedding"},{"location":"investigacion/01_llm/#5-normalizacion-y-lematizacion","text":"Cuando trabajamos con texto, especialmente al darle instrucciones a una IA, hay muchos t\u00e9rminos que no aportan valor significativo. Palabras como \"la\", \"lo\", \"el\", \"y\", \"con\" , o incluso los signos de puntuaci\u00f3n , suelen ser irrelevantes para el an\u00e1lisis, ya que aparecen en casi todos los contextos y no cambian el significado central de una frase. Para simplificar y optimizar el procesamiento, lo primero que hacemos es limpiar los textos antes de pasarlos al sistema. Veamos un ejemplo con la frase: \"El gato est\u00e1 durmiendo en el sof\u00e1.\" Frase original: \"El gato est\u00e1 durmiendo en el sof\u00e1.\" Eliminaci\u00f3n de palabras irrelevantes y signos de puntuaci\u00f3n: \"gato est\u00e1 durmiendo sof\u00e1\" Lematizaci\u00f3n (reducci\u00f3n de las palabras a su forma base): \"gato es dormir sof\u00e1\" Tokenizaci\u00f3n (divisi\u00f3n en unidades b\u00e1sicas): \"gato\", \"dormir\", \"sof\u00e1\" Esta frase queda comprimida y m\u00e1s eficiente para el procesamiento por parte del modelo. En lugar de trabajar con muchas palabras y tokens innecesarios, esta versi\u00f3n reduce el texto a su m\u00ednima expresi\u00f3n \u00fatil , acelerando la b\u00fasqueda y el an\u00e1lisis dentro del modelo. Modelos como ChatGPT preparan internamente nuestra entrada de esta forma para compararla con representaciones (embeddings) similares y generar una respuesta adecuada. Este proceso implica una p\u00e9rdida de informaci\u00f3n , ya que comprimimos el mensaje original. Sin embargo, esto no es algo negativo: la IA no repite textualmente, sino que genera una respuesta original basada en la comprensi\u00f3n del contenido. Un buen ejemplo ser\u00eda escuchar una clase y al d\u00eda siguiente explicarle el concepto a alguien m\u00e1s: no recordamos todas las palabras exactas, pero s\u00ed entendemos y transmitimos la idea principal.","title":"5. Normalizaci\u00f3n y Lematizaci\u00f3n"},{"location":"investigacion/01_llm/#6-sampling","text":"El sampling (muestreo) es una t\u00e9cnica que permite al modelo generar respuestas de forma m\u00e1s variada y creativa , introduciendo un componente aleatorio en el proceso de generaci\u00f3n de texto. En lugar de elegir siempre la palabra m\u00e1s probable seg\u00fan el modelo, el sampling permite explorar otras opciones que, aunque sean ligeramente menos probables, siguen siendo coherentes dentro del contexto. Es como moverse ligeramente dentro del espacio de embeddings , eligiendo caminos diferentes que llevan a respuestas distintas pero igualmente v\u00e1lidas. Esto evita que el modelo siempre d\u00e9 las mismas respuestas ante los mismos inputs y permite generar resultados m\u00e1s naturales y diversos , sin perder el sentido general de la conversaci\u00f3n.","title":"6. Sampling"},{"location":"investigacion/01_llm/#evolucion-de-las-redes-neuronales","text":"Las redes neuronales tradicionales tienen una gran limitaci\u00f3n: no tienen memoria . Es decir, procesan cada entrada (input) de manera aislada, sin recordar lo que pas\u00f3 antes. Pero, \u00bfc\u00f3mo puede una red entender el contexto de una conversaci\u00f3n o el significado completo de una frase si solo ve una palabra a la vez?","title":"Evoluci\u00f3n de las Redes Neuronales"},{"location":"investigacion/01_llm/#redes-neuronales-recurrentes-rnn","text":"Para resolver este problema apareci\u00f3 una arquitectura llamada Red Neuronal Recurrente (RNN), que introduce la idea de \"memoria\" al alimentar el resultado de un paso como entrada del siguiente. Procesan palabra por palabra en secuencia . Cada palabra se analiza considerando lo anterior. Problema: pierden contexto cuando el texto es muy largo. No permiten paralelizar , por lo que el entrenamiento es m\u00e1s lento. Figura 13: Ejemplo de RNN","title":"Redes Neuronales Recurrentes (RNN)"},{"location":"investigacion/01_llm/#lstm-long-short-term-memory","text":"Las LSTM son una mejora de las RNN tradicionales. Introducen una especie de \"filtro de memoria\" que decide qu\u00e9 informaci\u00f3n mantener y cu\u00e1l olvidar. Manejan mejor el contexto de largo plazo. Son m\u00e1s precisas que las RNN simples. Aun as\u00ed, siguen siendo secuenciales y no paralelizables , lo que limita su eficiencia.","title":"LSTM (Long Short-Term Memory)"},{"location":"investigacion/01_llm/#el-gran-cambio-transformers","text":"El gran salto lleg\u00f3 en 2017 con el paper \u201cAttention is All You Need\u201d , que introdujo el modelo Transformer . Esta arquitectura cambi\u00f3 por completo la forma en que las redes procesan secuencias.","title":"El Gran Cambio: Transformers"},{"location":"investigacion/01_llm/#que-aportan-los-transformers","text":"Utilizan un mecanismo llamado Self-Attention para identificar la importancia de cada palabra en relaci\u00f3n con todas las dem\u00e1s, en paralelo . Permiten analizar todas las palabras al mismo tiempo , no de forma secuencial. Esto facilita el entrenamiento y mejora la comprensi\u00f3n del contexto completo . Son altamente paralelizables , lo que acelera enormemente el proceso.","title":"\u00bfQu\u00e9 aportan los Transformers?"},{"location":"investigacion/01_llm/#ejemplo-con-self-attention","text":"En la frase: \u201cLa vida es bella, as\u00ed que v\u00edvela cada d\u00eda.\u201d El modelo analiza cada palabra en relaci\u00f3n con todas las dem\u00e1s. Por ejemplo: \"la\" con respecto a \"vida\", \"bella\", \"v\u00edvela\", etc. \"vida\" con respecto a todas las dem\u00e1s tambi\u00e9n. Figura 14: Ejemplo de uso de transformers Figura 15: Ejemplo de paralelizaci\u00f3n Esto permite entender cu\u00e1les son las palabras m\u00e1s importantes para construir el significado global de la frase, incluso si est\u00e1n separadas por otras palabras. Gracias a los Transformers, hoy podemos contar con modelos como GPT, BERT y otros LLM (Large Language Models) que comprenden mejor el lenguaje y generan respuestas m\u00e1s coherentes, r\u00e1pidas y \u00fatiles. Bibliograf\u00eda: https://www.youtube.com/watch?v=FdZ8LKiJBhQ&list=PL3ei_Xb7-ic5pkJDTplPxWvE8t13mm19W&index=8 https://www.ntiva.com/blog/what-is-machine-learning Aur\u00e9lien G\u00e9ron - Hands-On Machine Learning with Scikit-Learn, Keras and TensorFlow http://personal.cimat.mx:8181/~mrivera/cursos/aprendizaje_profundo/RNN_LTSM/introduccion_rnn.html https://visajourneypro.com/ https://projector.tensorflow.org/ https://arxiv.org/abs/1706.03762","title":"Ejemplo con Self-Attention"},{"location":"investigacion/02_metod-invest/","text":"\u00bfQu\u00e9 es una Metodolog\u00eda de Investigaci\u00f3n? Una metodolog\u00eda de investigaci\u00f3n es el conjunto de principios, procedimientos y t\u00e9cnicas que gu\u00edan el proceso de investigaci\u00f3n cient\u00edfica. Es un marco estructurado que permite a los investigadores planificar, organizar y ejecutar un estudio de forma sistem\u00e1tica y rigurosa , con el objetivo de: Responder preguntas Resolver problemas Generar nuevo conocimiento El objetivo principal es asegurar que el proceso sea replicable, v\u00e1lido y confiable , adapt\u00e1ndose tanto al tipo de investigaci\u00f3n como al \u00e1rea de estudio. A continuaci\u00f3n, se presenta un mapa metodol\u00f3gico de investigaci\u00f3n: Figura 1: \"Mapa metodol\u00f3gico de investigaci\u00f3n\" El mapa presenta una secuencia estructurada de etapas para el desarrollo de un proceso de investigaci\u00f3n completo. En el presente trabajo, se profundizar\u00e1 principalmente en los tres primeros bloques metodol\u00f3gicos. El primer bloque corresponde a la identificaci\u00f3n del problema, etapa fundamental donde se establece el problema de investigaci\u00f3n como punto de partida del proceso. Posteriormente, se aborda la revisi\u00f3n bibliogr\u00e1fica, fase en la cual se analizan los trabajos previos y se identifica la problem\u00e1tica existente. Finalmente, se llega al tercer bloque que corresponde a la formulaci\u00f3n del problema, consolidando as\u00ed las bases iniciales de la investigaci\u00f3n. Sobre todo nos centramos en el bloque de Revisi\u00f3n bibliogr\u00e1fica que nos va a permitir entender el estado del arte de la investigaci\u00f3n para evitar la repetici\u00f3n de trabajos y tener una base s\u00f3lida para la investigaci\u00f3n. Referencias UNACAR: Metodolog\u00eda de investigaci\u00f3n .","title":"Concepto general"},{"location":"investigacion/02_metod-invest/#que-es-una-metodologia-de-investigacion","text":"Una metodolog\u00eda de investigaci\u00f3n es el conjunto de principios, procedimientos y t\u00e9cnicas que gu\u00edan el proceso de investigaci\u00f3n cient\u00edfica. Es un marco estructurado que permite a los investigadores planificar, organizar y ejecutar un estudio de forma sistem\u00e1tica y rigurosa , con el objetivo de: Responder preguntas Resolver problemas Generar nuevo conocimiento El objetivo principal es asegurar que el proceso sea replicable, v\u00e1lido y confiable , adapt\u00e1ndose tanto al tipo de investigaci\u00f3n como al \u00e1rea de estudio. A continuaci\u00f3n, se presenta un mapa metodol\u00f3gico de investigaci\u00f3n: Figura 1: \"Mapa metodol\u00f3gico de investigaci\u00f3n\" El mapa presenta una secuencia estructurada de etapas para el desarrollo de un proceso de investigaci\u00f3n completo. En el presente trabajo, se profundizar\u00e1 principalmente en los tres primeros bloques metodol\u00f3gicos. El primer bloque corresponde a la identificaci\u00f3n del problema, etapa fundamental donde se establece el problema de investigaci\u00f3n como punto de partida del proceso. Posteriormente, se aborda la revisi\u00f3n bibliogr\u00e1fica, fase en la cual se analizan los trabajos previos y se identifica la problem\u00e1tica existente. Finalmente, se llega al tercer bloque que corresponde a la formulaci\u00f3n del problema, consolidando as\u00ed las bases iniciales de la investigaci\u00f3n. Sobre todo nos centramos en el bloque de Revisi\u00f3n bibliogr\u00e1fica que nos va a permitir entender el estado del arte de la investigaci\u00f3n para evitar la repetici\u00f3n de trabajos y tener una base s\u00f3lida para la investigaci\u00f3n.","title":"\u00bfQu\u00e9 es una Metodolog\u00eda de Investigaci\u00f3n?"},{"location":"investigacion/02_metod-invest/#referencias","text":"UNACAR: Metodolog\u00eda de investigaci\u00f3n .","title":"Referencias"},{"location":"investigacion/03_estado-arte/","text":"Estado del Arte El estado del arte es un pilar fundamental en cualquier investigaci\u00f3n cient\u00edfica, ya que permite situar el trabajo propio dentro del contexto del conocimiento existente. Su funci\u00f3n principal es demostrar que el investigador comprende el campo, identifica los avances, reconoce los vac\u00edos y justifica la relevancia de su propuesta. En \u00e1reas como la ingenier\u00eda en telecomunicaciones, donde la tecnolog\u00eda y los enfoques evolucionan r\u00e1pidamente, un estado del arte riguroso asegura que la investigaci\u00f3n sea pertinente, novedosa y alineada con los est\u00e1ndares internacionales. \u00bfPor qu\u00e9 es importante? Contextualiza la investigaci\u00f3n : Permite entender c\u00f3mo el problema abordado se relaciona con desarrollos previos y actuales. Evita duplicidad : Ayuda a no repetir trabajos ya realizados y a identificar oportunidades de innovaci\u00f3n. Identifica vac\u00edos y tendencias : Se\u00f1ala \u00e1reas poco exploradas o en debate, orientando la investigaci\u00f3n hacia aportes significativos. Justifica la metodolog\u00eda : Permite seleccionar m\u00e9todos y enfoques validados por la comunidad cient\u00edfica. Aporta credibilidad : Un buen estado del arte respalda la solidez y originalidad del trabajo ante la comunidad acad\u00e9mica. Mapa: Camino para Elaborar el Estado del Arte en un Paper Cient\u00edfico Figura 1: Mapa para elaborar el estado del arte en un paper cient\u00edfico 1. Definici\u00f3n y planificaci\u00f3n Formular preguntas de investigaci\u00f3n claras y espec\u00edficas (por ejemplo, usando el marco PICOC: Poblaci\u00f3n, Intervenci\u00f3n, Comparaci\u00f3n, Resultados, Contexto). Delimitar el alcance tem\u00e1tico y temporal de la revisi\u00f3n. 2. B\u00fasqueda y recopilaci\u00f3n de literatura Seleccionar bases de datos y fuentes relevantes (IEEE Xplore, ACM Digital Library, Scopus, Web of Science, etc.). Definir t\u00e9rminos de b\u00fasqueda y combinaciones l\u00f3gicas (sin\u00f3nimos, palabras clave, operadores booleanos). Consultar expertos y revisar referencias de art\u00edculos clave. 3. Selecci\u00f3n y filtrado de estudios Eliminar duplicados y versiones repetidas. Aplicar criterios de inclusi\u00f3n/exclusi\u00f3n (a\u00f1o, idioma, tipo de publicaci\u00f3n, relevancia). Evaluar la calidad metodol\u00f3gica de los estudios (claridad de objetivos, justificaci\u00f3n de m\u00e9todos, reproducibilidad, an\u00e1lisis de resultados, etc.). Criterios de calidad para la evaluaci\u00f3n de estudios Se recomienda clasificar cada estudio seg\u00fan los siguientes criterios: \u00bfEl objetivo de la investigaci\u00f3n est\u00e1 claramente definido? \u00bfEl estudio se contextualiza respecto a investigaciones previas? \u00bfLas decisiones de dise\u00f1o del sistema o algoritmo est\u00e1n justificadas? \u00bfEl conjunto de datos de prueba es reproducible? \u00bfEl algoritmo es reproducible? \u00bfEl procedimiento experimental est\u00e1 claramente explicado y es reproducible? \u00bfSe especifica con qu\u00e9 otros algoritmos se compara el propuesto? \u00bfSe explican y justifican las m\u00e9tricas de desempe\u00f1o? \u00bfLos resultados de las pruebas est\u00e1n analizados en profundidad? \u00bfLa evidencia experimental respalda las conclusiones? Cada criterio puede puntuarse como s\u00ed (1 punto), parcialmente (0,5 puntos) o no (0 puntos). El protocolo debe definir el umbral m\u00ednimo de calidad para aceptar un estudio y si es aceptable obtener cero puntos en alg\u00fan criterio. 4. Organizaci\u00f3n y an\u00e1lisis Agrupar la literatura por enfoques tem\u00e1ticos, metodol\u00f3gicos, cronol\u00f3gicos o te\u00f3ricos. Identificar patrones, tendencias, controversias y vac\u00edos en la literatura. Comparar y sintetizar hallazgos relevantes. Realizar un mapa de citaciones o de autores m\u00e1s citados: analizar las referencias cruzadas entre los papers revisados para identificar qu\u00e9 trabajos o autores son m\u00e1s influyentes y c\u00f3mo se relacionan entre s\u00ed. Esto ayuda a visualizar la estructura del campo y a detectar los referentes clave en el \u00e1rea de estudio. 5. Redacci\u00f3n del estado del arte Introducci\u00f3n: Presentar el tema, justificar la revisi\u00f3n y explicar el criterio de selecci\u00f3n. Cuerpo: Desarrollar el an\u00e1lisis agrupado, resaltando avances, debates y limitaciones. Conclusi\u00f3n: Resumir los aportes principales, se\u00f1alar vac\u00edos y proponer l\u00edneas futuras de investigaci\u00f3n. 6. Revisi\u00f3n y referencias Revisar la coherencia, claridad y estructura del texto. Citar correctamente todas las fuentes utilizadas, siguiendo normas acad\u00e9micas. Herramientas recomendadas Gestores de referencias Zotero (Open Source): Gestor de referencias bibliogr\u00e1ficas que permite recolectar, organizar, citar y compartir fuentes de investigaci\u00f3n. Incluye plugins para navegadores y procesadores de texto. JabRef (Open Source): Gestor de referencias bibliogr\u00e1ficas especializado en archivos BibTeX, ideal para usuarios de LaTeX. Permite organizar y buscar referencias de manera eficiente. Herramientas de an\u00e1lisis bibliom\u00e9trico VosViewer (de pago): Software para construir y visualizar redes bibliom\u00e9tricas. Permite analizar patrones de colaboraci\u00f3n, co-citaci\u00f3n y co-ocurrencia de t\u00e9rminos. VOSviewer-Online (versi\u00f3n web de VosViewer, Open Source): Versi\u00f3n web de VosViewer que permite realizar an\u00e1lisis bibliom\u00e9tricos directamente desde el navegador. Esta herramienta puede ser utilizada como alternativa para la creaci\u00f3n de mapas de citaciones, considerando que VosViewer es una herramienta de pago. Se puede adaptar para casos de uso espec\u00edficos mediante la implementaci\u00f3n de scripts en Python, aprovechando su naturaleza de c\u00f3digo abierto. Plataformas para gesti\u00f3n de revisiones sistem\u00e1ticas Parsif.al (uso libre): Plataforma web para gestionar revisiones sistem\u00e1ticas de literatura. Facilita la planificaci\u00f3n, ejecuci\u00f3n y documentaci\u00f3n de revisiones sistem\u00e1ticas siguiendo metodolog\u00edas estandarizadas. Herramientas de IA para investigaci\u00f3n Consensus (de pago): Plataforma que utiliza IA para analizar y sintetizar hallazgos cient\u00edficos, ayudando a encontrar respuestas basadas en evidencia en la literatura cient\u00edfica. Elicit (de pago): Herramienta de IA que automatiza partes del proceso de revisi\u00f3n de literatura, ayudando a encontrar, resumir y analizar papers relevantes. Scite (de pago): Plataforma que utiliza IA para analizar c\u00f3mo los art\u00edculos cient\u00edficos son citados, proporcionando contexto sobre si los hallazgos son apoyados o refutados por investigaciones posteriores. SciSpace (de pago): Plataforma que combina IA y an\u00e1lisis de literatura cient\u00edfica para ayudar a los investigadores a encontrar, leer y comprender papers relevantes de manera m\u00e1s eficiente. Research Rabbit (uso libre): Herramienta que utiliza IA para descubrir y visualizar conexiones entre papers cient\u00edficos, ayudando a mapear el panorama de la investigaci\u00f3n en un campo espec\u00edfico. Referencias Literature Review University of Edinburgh The Literature Review: A Few Tips On Conducting It University of Wisconsin-Madison Literature Reviews University of North Carolina at Chapel Hill Literature Review University of Arizona Libraries Language Models are Few-Shot Learners BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Google Scholar IEEE Xplore ACM Digital Library ScienceDirect SpringerLink","title":"Revisi\u00f3n Bibliogr\u00e1fica"},{"location":"investigacion/03_estado-arte/#estado-del-arte","text":"El estado del arte es un pilar fundamental en cualquier investigaci\u00f3n cient\u00edfica, ya que permite situar el trabajo propio dentro del contexto del conocimiento existente. Su funci\u00f3n principal es demostrar que el investigador comprende el campo, identifica los avances, reconoce los vac\u00edos y justifica la relevancia de su propuesta. En \u00e1reas como la ingenier\u00eda en telecomunicaciones, donde la tecnolog\u00eda y los enfoques evolucionan r\u00e1pidamente, un estado del arte riguroso asegura que la investigaci\u00f3n sea pertinente, novedosa y alineada con los est\u00e1ndares internacionales. \u00bfPor qu\u00e9 es importante? Contextualiza la investigaci\u00f3n : Permite entender c\u00f3mo el problema abordado se relaciona con desarrollos previos y actuales. Evita duplicidad : Ayuda a no repetir trabajos ya realizados y a identificar oportunidades de innovaci\u00f3n. Identifica vac\u00edos y tendencias : Se\u00f1ala \u00e1reas poco exploradas o en debate, orientando la investigaci\u00f3n hacia aportes significativos. Justifica la metodolog\u00eda : Permite seleccionar m\u00e9todos y enfoques validados por la comunidad cient\u00edfica. Aporta credibilidad : Un buen estado del arte respalda la solidez y originalidad del trabajo ante la comunidad acad\u00e9mica.","title":"Estado del Arte"},{"location":"investigacion/03_estado-arte/#mapa-camino-para-elaborar-el-estado-del-arte-en-un-paper-cientifico","text":"Figura 1: Mapa para elaborar el estado del arte en un paper cient\u00edfico","title":"Mapa: Camino para Elaborar el Estado del Arte en un Paper Cient\u00edfico"},{"location":"investigacion/03_estado-arte/#1-definicion-y-planificacion","text":"Formular preguntas de investigaci\u00f3n claras y espec\u00edficas (por ejemplo, usando el marco PICOC: Poblaci\u00f3n, Intervenci\u00f3n, Comparaci\u00f3n, Resultados, Contexto). Delimitar el alcance tem\u00e1tico y temporal de la revisi\u00f3n.","title":"1. Definici\u00f3n y planificaci\u00f3n"},{"location":"investigacion/03_estado-arte/#2-busqueda-y-recopilacion-de-literatura","text":"Seleccionar bases de datos y fuentes relevantes (IEEE Xplore, ACM Digital Library, Scopus, Web of Science, etc.). Definir t\u00e9rminos de b\u00fasqueda y combinaciones l\u00f3gicas (sin\u00f3nimos, palabras clave, operadores booleanos). Consultar expertos y revisar referencias de art\u00edculos clave.","title":"2. B\u00fasqueda y recopilaci\u00f3n de literatura"},{"location":"investigacion/03_estado-arte/#3-seleccion-y-filtrado-de-estudios","text":"Eliminar duplicados y versiones repetidas. Aplicar criterios de inclusi\u00f3n/exclusi\u00f3n (a\u00f1o, idioma, tipo de publicaci\u00f3n, relevancia). Evaluar la calidad metodol\u00f3gica de los estudios (claridad de objetivos, justificaci\u00f3n de m\u00e9todos, reproducibilidad, an\u00e1lisis de resultados, etc.). Criterios de calidad para la evaluaci\u00f3n de estudios Se recomienda clasificar cada estudio seg\u00fan los siguientes criterios: \u00bfEl objetivo de la investigaci\u00f3n est\u00e1 claramente definido? \u00bfEl estudio se contextualiza respecto a investigaciones previas? \u00bfLas decisiones de dise\u00f1o del sistema o algoritmo est\u00e1n justificadas? \u00bfEl conjunto de datos de prueba es reproducible? \u00bfEl algoritmo es reproducible? \u00bfEl procedimiento experimental est\u00e1 claramente explicado y es reproducible? \u00bfSe especifica con qu\u00e9 otros algoritmos se compara el propuesto? \u00bfSe explican y justifican las m\u00e9tricas de desempe\u00f1o? \u00bfLos resultados de las pruebas est\u00e1n analizados en profundidad? \u00bfLa evidencia experimental respalda las conclusiones? Cada criterio puede puntuarse como s\u00ed (1 punto), parcialmente (0,5 puntos) o no (0 puntos). El protocolo debe definir el umbral m\u00ednimo de calidad para aceptar un estudio y si es aceptable obtener cero puntos en alg\u00fan criterio.","title":"3. Selecci\u00f3n y filtrado de estudios"},{"location":"investigacion/03_estado-arte/#4-organizacion-y-analisis","text":"Agrupar la literatura por enfoques tem\u00e1ticos, metodol\u00f3gicos, cronol\u00f3gicos o te\u00f3ricos. Identificar patrones, tendencias, controversias y vac\u00edos en la literatura. Comparar y sintetizar hallazgos relevantes. Realizar un mapa de citaciones o de autores m\u00e1s citados: analizar las referencias cruzadas entre los papers revisados para identificar qu\u00e9 trabajos o autores son m\u00e1s influyentes y c\u00f3mo se relacionan entre s\u00ed. Esto ayuda a visualizar la estructura del campo y a detectar los referentes clave en el \u00e1rea de estudio.","title":"4. Organizaci\u00f3n y an\u00e1lisis"},{"location":"investigacion/03_estado-arte/#5-redaccion-del-estado-del-arte","text":"Introducci\u00f3n: Presentar el tema, justificar la revisi\u00f3n y explicar el criterio de selecci\u00f3n. Cuerpo: Desarrollar el an\u00e1lisis agrupado, resaltando avances, debates y limitaciones. Conclusi\u00f3n: Resumir los aportes principales, se\u00f1alar vac\u00edos y proponer l\u00edneas futuras de investigaci\u00f3n.","title":"5. Redacci\u00f3n del estado del arte"},{"location":"investigacion/03_estado-arte/#6-revision-y-referencias","text":"Revisar la coherencia, claridad y estructura del texto. Citar correctamente todas las fuentes utilizadas, siguiendo normas acad\u00e9micas.","title":"6. Revisi\u00f3n y referencias"},{"location":"investigacion/03_estado-arte/#herramientas-recomendadas","text":"","title":"Herramientas recomendadas"},{"location":"investigacion/03_estado-arte/#gestores-de-referencias","text":"Zotero (Open Source): Gestor de referencias bibliogr\u00e1ficas que permite recolectar, organizar, citar y compartir fuentes de investigaci\u00f3n. Incluye plugins para navegadores y procesadores de texto. JabRef (Open Source): Gestor de referencias bibliogr\u00e1ficas especializado en archivos BibTeX, ideal para usuarios de LaTeX. Permite organizar y buscar referencias de manera eficiente.","title":"Gestores de referencias"},{"location":"investigacion/03_estado-arte/#herramientas-de-analisis-bibliometrico","text":"VosViewer (de pago): Software para construir y visualizar redes bibliom\u00e9tricas. Permite analizar patrones de colaboraci\u00f3n, co-citaci\u00f3n y co-ocurrencia de t\u00e9rminos. VOSviewer-Online (versi\u00f3n web de VosViewer, Open Source): Versi\u00f3n web de VosViewer que permite realizar an\u00e1lisis bibliom\u00e9tricos directamente desde el navegador. Esta herramienta puede ser utilizada como alternativa para la creaci\u00f3n de mapas de citaciones, considerando que VosViewer es una herramienta de pago. Se puede adaptar para casos de uso espec\u00edficos mediante la implementaci\u00f3n de scripts en Python, aprovechando su naturaleza de c\u00f3digo abierto.","title":"Herramientas de an\u00e1lisis bibliom\u00e9trico"},{"location":"investigacion/03_estado-arte/#plataformas-para-gestion-de-revisiones-sistematicas","text":"Parsif.al (uso libre): Plataforma web para gestionar revisiones sistem\u00e1ticas de literatura. Facilita la planificaci\u00f3n, ejecuci\u00f3n y documentaci\u00f3n de revisiones sistem\u00e1ticas siguiendo metodolog\u00edas estandarizadas.","title":"Plataformas para gesti\u00f3n de revisiones sistem\u00e1ticas"},{"location":"investigacion/03_estado-arte/#herramientas-de-ia-para-investigacion","text":"Consensus (de pago): Plataforma que utiliza IA para analizar y sintetizar hallazgos cient\u00edficos, ayudando a encontrar respuestas basadas en evidencia en la literatura cient\u00edfica. Elicit (de pago): Herramienta de IA que automatiza partes del proceso de revisi\u00f3n de literatura, ayudando a encontrar, resumir y analizar papers relevantes. Scite (de pago): Plataforma que utiliza IA para analizar c\u00f3mo los art\u00edculos cient\u00edficos son citados, proporcionando contexto sobre si los hallazgos son apoyados o refutados por investigaciones posteriores. SciSpace (de pago): Plataforma que combina IA y an\u00e1lisis de literatura cient\u00edfica para ayudar a los investigadores a encontrar, leer y comprender papers relevantes de manera m\u00e1s eficiente. Research Rabbit (uso libre): Herramienta que utiliza IA para descubrir y visualizar conexiones entre papers cient\u00edficos, ayudando a mapear el panorama de la investigaci\u00f3n en un campo espec\u00edfico.","title":"Herramientas de IA para investigaci\u00f3n"},{"location":"investigacion/03_estado-arte/#referencias","text":"Literature Review University of Edinburgh The Literature Review: A Few Tips On Conducting It University of Wisconsin-Madison Literature Reviews University of North Carolina at Chapel Hill Literature Review University of Arizona Libraries Language Models are Few-Shot Learners BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Google Scholar IEEE Xplore ACM Digital Library ScienceDirect SpringerLink","title":"Referencias"},{"location":"investigacion/04_arquitectura/","text":"Arquitecturas para el agente IA Tipos de Arquitecturas En primer lugar, se describir\u00e1 la arquitectura empleada en el presente proyecto, brindando una explicaci\u00f3n detallada de su funcionamiento. Luego, se mencionar\u00e1n algunas arquitecturas generales relevantes en el contexto. Cabe destacar que este proyecto tiene un enfoque open source, por lo que, en caso de inter\u00e9s, es posible modificar su estructura o incorporar nuevas capacidades seg\u00fan las necesidades espec\u00edficas de cada implementaci\u00f3n. RAG (Retrieval Augmented Generation) Figura 1: Arquitectura Augmented LLM Este tipo de arquitectura tiene un modelo de lenguaje (LLM) que dispone de capacidades como recuperaci\u00f3n de informaci\u00f3n \" retrieval \", uso de herramientas externas \" tools \" y gesti\u00f3n de memoria \" memory \". Estos modelos no solo cuentan con dichas funciones, sino que tambi\u00e9n son capaces de utilizarlas activamente: pueden generar sus propias consultas de b\u00fasqueda, seleccionar las herramientas m\u00e1s adecuadas y decidir qu\u00e9 informaci\u00f3n conservar a lo largo de una conversaci\u00f3n. Este es el modelo que vamos a usar como modelo para nuestro proyecto, a continuaci\u00f3n se explica mejor c\u00f3mo es el proceso del mismo. Figura 2: Arquitectura RAG (Retrieval Augmented Generation) \u00bfC\u00f3mo funciona RAG? Podemos dividir el funcionamiento de RAG en dos grandes etapas: 1. Carga de la informaci\u00f3n a la base de datos vectorial 2. Interacci\u00f3n del usuario con el agente Etapa 1: Carga de informaci\u00f3n Comenzamos con una fuente de informaci\u00f3n, como un archivo PDF. El primer paso es aplicar un proceso llamado chunking , que consiste en dividir el contenido en fragmentos m\u00e1s peque\u00f1os ( etapa 1 ). Esto permite un tratamiento m\u00e1s eficiente y preciso, especialmente durante las b\u00fasquedas sem\u00e1nticas, donde queremos que el sistema encuentre informaci\u00f3n en partes espec\u00edficas del documento, y no en su totalidad. Luego preparamos estos fragmentos ( etapa 2 ). Existen dos estrategias comunes: Fixed Token Splitter: divide el contenido en fragmentos de longitud fija. Recursive Token Splitter: realiza divisiones m\u00e1s inteligentes y superpuestas para evitar que se pierda contexto cuando una pregunta coincide justo en los bordes entre fragmentos. Esta t\u00e9cnica permite mantener solapamiento entre fragmentos, como se ilustra en la siguiente imagen. Figura 3: Fixed vs Recursive splitting Luego, Cada fragmento se convierte en un vector mediante un modelo de embeddings ( etapa 3 ). Esto transforma el contenido textual en una representaci\u00f3n matem\u00e1tica comprensible para el sistema. Finalmente, los vectores generados se almacenan en una base de datos vectorial ( etapa 4 ), que queda lista para ser consultada durante la etapa de recuperaci\u00f3n de informaci\u00f3n. Etapa 2: Consulta del usuario Cuando el usuario realiza una consulta ( etapa 7 ), esta tambi\u00e9n se convierte en un vector utilizando el mismo modelo de embeddings ( etapa 3 ). Luego, se compara este vector con los almacenados en la base de datos para encontrar los m\u00e1s similares (esto es el retrieval ). Luego, El sistema recupera ( etapa 5 ): El contexto relevante desde la base de datos (fragmentos similares). La consulta del usuario original. Ambos elementos se integran en una plantilla de prompt, por ejemplo: \"Sos un buscador especializado en Bibliograf\u00eda\" ( esto ser\u00eda el prompt ) Responde esta \"consulta\" ( lo que pregunta el usuario ) basandote en el siguiente \"contexto\" ( va a ser lo que te devuelva la base vectorial ) Adem\u00e1s, en este paso se puede personalizar el comportamiento del agente, indicando que responda con un estilo determinado o como si fuera especialista en un \u00e1rea. Finalmente, un modelo de lenguaje (LLM) preentrenado toma ese prompt completo (contexto + consulta) y genera la respuesta final para el usuario ( etapa 6 ). Es importante destacar que el modelo ya est\u00e1 entrenado previamente, ya que realizar un entrenamiento desde cero suele ser costoso o t\u00e9cnicamente complejo. Por eso, es importante elegir un modelo que sea eficiente para el uso que le daremos. Contextual RAG El Contextual RAG (Retrieval Augmented Generation) es una evoluci\u00f3n del RAG tradicional que mejora significativamente la precisi\u00f3n y relevancia de las respuestas mediante la incorporaci\u00f3n de contexto adicional durante el proceso de recuperaci\u00f3n. A diferencia del RAG est\u00e1ndar, que busca fragmentos similares bas\u00e1ndose \u00fanicamente en la similitud sem\u00e1ntica, el Contextual RAG considera m\u00faltiples factores para determinar qu\u00e9 informaci\u00f3n es verdaderamente relevante para la consulta del usuario. La arquitectura se puede ver en la siguiente figura: Figura 4: Arquitectura Contextual RAG El Contextual RAG es una arquitectura desarrollada en 2024 que supera una limitaci\u00f3n hist\u00f3rica: la capacidad de procesar documentos completos de una sola vez. Aunque su funcionamiento base es similar al RAG tradicional, introduce una etapa adicional de procesamiento contextual. En esta etapa, cada fragmento (chunk) se analiza en relaci\u00f3n con el documento completo mediante un prompt estructurado: <document> {{ WHOLE_DOCUMENT }} </document> Here is the chunk we want to situate within the whole document <chunk> {{ CHUNK_CONTENT }} </chunk> Please give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk. Answer only with the succinct context and nothing else . Este proceso enriquece la base de datos vectorial al almacenar no solo el fragmento original, sino tambi\u00e9n el contexto generado que lo sit\u00faa dentro del documento completo. Ventajas sobre RAG tradicional: Mayor precisi\u00f3n : Al considerar el contexto completo, reduce las respuestas fuera de tema o irrelevantes. Mejor coherencia : Mantiene una l\u00ednea de pensamiento m\u00e1s consistente en las respuestas. Respuestas m\u00e1s completas : Al entender mejor el contexto, puede proporcionar informaci\u00f3n m\u00e1s relevante y \u00fatil. Desventajas sobre RAG tradicional: Mayor complejidad : Requiere un procesamiento adicional y una base de datos vectorial m\u00e1s grande. Latencia : Puede ser m\u00e1s lento debido al procesamiento adicional. Comparaci\u00f3n entre RAG tradicional y Contextual RAG Los resultados de la comparaci\u00f3n entre RAG tradicional y Contextual RAG muestran una mejora significativa en la precisi\u00f3n de recuperaci\u00f3n: el uso de Contextual Embeddings redujo la tasa de fallos en la recuperaci\u00f3n de los 20 fragmentos m\u00e1s relevantes en un 35%, pasando de un 5.7% a un 3.7% de fallos. Esto demuestra que el procesamiento contextual adicional mejora sustancialmente la calidad de las b\u00fasquedas. (en las graficas aparece tambien BM25, que es un modelo de recuperaci\u00f3n de informaci\u00f3n que se basa en la frecuencia de las palabras en el documento pero no lo vamos a usar) Figura 5: Comparaci\u00f3n entre RAG (embedding) y CRAG (contextual embedding) Prompt chaining Figura 6: Arquitectura Prompt chaining El encadenamiento de prompts descompone una tarea en una secuencia de pasos, donde cada llamada a un modelo de lenguaje procesa la salida de la anterior. Este flujo de trabajo es ideal para situaciones en las que la tarea puede dividirse de forma clara y sencilla en subtareas fijas. El objetivo principal es sacrificar algo de latencia a cambio de una mayor precisi\u00f3n, haciendo que cada llamada al modelo sea una tarea m\u00e1s simple. Routing Figura 7: Arquitectura Routing El enrutamiento clasifica una entrada y la dirige a una tarea de seguimiento especializada. Este enfoque permite una clara separaci\u00f3n de responsabilidades y la creaci\u00f3n de prompts m\u00e1s espec\u00edficos y eficaces. Sin esta estructura, optimizar el rendimiento para un tipo de entrada podr\u00eda perjudicar el desempe\u00f1o en otros casos. El enrutamiento resulta especialmente \u00fatil en tareas complejas con categor\u00edas bien diferenciadas, donde cada una puede ser tratada de forma m\u00e1s eficiente por separado. La clasificaci\u00f3n inicial puede realizarse mediante un modelo de lenguaje o utilizando m\u00e9todos tradicionales de clasificaci\u00f3n. Parallelization Figura 8: Arquitectura Parallelization Los LLMs pueden abordar m\u00faltiples tareas de forma simult\u00e1nea, y sus resultados pueden combinarse posteriormente mediante programaci\u00f3n. Este enfoque, conocido como paralelizaci\u00f3n, se presenta en dos formas principales: Seccionamiento : consiste en dividir una tarea en subtareas independientes que se ejecutan en paralelo. Votaci\u00f3n : implica ejecutar la misma tarea varias veces para obtener resultados variados y seleccionar el m\u00e1s adecuado. La paralelizaci\u00f3n es especialmente \u00fatil cuando las subtareas pueden procesarse simult\u00e1neamente para mejorar la velocidad, o cuando se requieren m\u00faltiples perspectivas para obtener respuestas m\u00e1s confiables. En tareas complejas con m\u00faltiples dimensiones, los LLMs suelen ofrecer mejores resultados cuando cada aspecto se gestiona por separado, permitiendo que cada parte reciba una atenci\u00f3n m\u00e1s enfocada. Orchestrator-workers Figura 9: Arquitectura Orchestrator-workers En el flujo de trabajo de orquestador y trabajadores, un LLM central act\u00faa como orquestador: descompone din\u00e1micamente las tareas, las asigna a LLMs especializados (trabajadores) y luego sintetiza sus respuestas. Aunque este enfoque puede parecer similar a la paralelizaci\u00f3n, su diferencia clave radica en la flexibilidad: las subtareas no est\u00e1n predefinidas, sino que son generadas en tiempo real por el orquestador seg\u00fan la naturaleza de la tarea. Evaluator-optimizer Figura 10: Arquitectura Evaluator-optimizer En el flujo de trabajo de evaluador-optimizador, una instancia de LLM genera una respuesta, mientras que otra eval\u00faa su calidad y proporciona retroalimentaci\u00f3n, formando un ciclo iterativo. Este enfoque resulta efectivo cuando se disponen de criterios de evaluaci\u00f3n bien definidos y cuando la mejora progresiva aporta un valor tangible. Referencia Anthropic: Building Effective Agents Anthropic: Contextual Retrieval","title":"Tipos de Arquitecturas"},{"location":"investigacion/04_arquitectura/#arquitecturas-para-el-agente-ia","text":"","title":"Arquitecturas para el agente IA"},{"location":"investigacion/04_arquitectura/#tipos-de-arquitecturas","text":"En primer lugar, se describir\u00e1 la arquitectura empleada en el presente proyecto, brindando una explicaci\u00f3n detallada de su funcionamiento. Luego, se mencionar\u00e1n algunas arquitecturas generales relevantes en el contexto. Cabe destacar que este proyecto tiene un enfoque open source, por lo que, en caso de inter\u00e9s, es posible modificar su estructura o incorporar nuevas capacidades seg\u00fan las necesidades espec\u00edficas de cada implementaci\u00f3n.","title":"Tipos de Arquitecturas"},{"location":"investigacion/04_arquitectura/#rag-retrieval-augmented-generation","text":"Figura 1: Arquitectura Augmented LLM Este tipo de arquitectura tiene un modelo de lenguaje (LLM) que dispone de capacidades como recuperaci\u00f3n de informaci\u00f3n \" retrieval \", uso de herramientas externas \" tools \" y gesti\u00f3n de memoria \" memory \". Estos modelos no solo cuentan con dichas funciones, sino que tambi\u00e9n son capaces de utilizarlas activamente: pueden generar sus propias consultas de b\u00fasqueda, seleccionar las herramientas m\u00e1s adecuadas y decidir qu\u00e9 informaci\u00f3n conservar a lo largo de una conversaci\u00f3n. Este es el modelo que vamos a usar como modelo para nuestro proyecto, a continuaci\u00f3n se explica mejor c\u00f3mo es el proceso del mismo. Figura 2: Arquitectura RAG (Retrieval Augmented Generation) \u00bfC\u00f3mo funciona RAG? Podemos dividir el funcionamiento de RAG en dos grandes etapas: 1. Carga de la informaci\u00f3n a la base de datos vectorial 2. Interacci\u00f3n del usuario con el agente","title":"RAG (Retrieval Augmented Generation)"},{"location":"investigacion/04_arquitectura/#etapa-1-carga-de-informacion","text":"Comenzamos con una fuente de informaci\u00f3n, como un archivo PDF. El primer paso es aplicar un proceso llamado chunking , que consiste en dividir el contenido en fragmentos m\u00e1s peque\u00f1os ( etapa 1 ). Esto permite un tratamiento m\u00e1s eficiente y preciso, especialmente durante las b\u00fasquedas sem\u00e1nticas, donde queremos que el sistema encuentre informaci\u00f3n en partes espec\u00edficas del documento, y no en su totalidad. Luego preparamos estos fragmentos ( etapa 2 ). Existen dos estrategias comunes: Fixed Token Splitter: divide el contenido en fragmentos de longitud fija. Recursive Token Splitter: realiza divisiones m\u00e1s inteligentes y superpuestas para evitar que se pierda contexto cuando una pregunta coincide justo en los bordes entre fragmentos. Esta t\u00e9cnica permite mantener solapamiento entre fragmentos, como se ilustra en la siguiente imagen. Figura 3: Fixed vs Recursive splitting Luego, Cada fragmento se convierte en un vector mediante un modelo de embeddings ( etapa 3 ). Esto transforma el contenido textual en una representaci\u00f3n matem\u00e1tica comprensible para el sistema. Finalmente, los vectores generados se almacenan en una base de datos vectorial ( etapa 4 ), que queda lista para ser consultada durante la etapa de recuperaci\u00f3n de informaci\u00f3n.","title":"Etapa 1: Carga de informaci\u00f3n"},{"location":"investigacion/04_arquitectura/#etapa-2-consulta-del-usuario","text":"Cuando el usuario realiza una consulta ( etapa 7 ), esta tambi\u00e9n se convierte en un vector utilizando el mismo modelo de embeddings ( etapa 3 ). Luego, se compara este vector con los almacenados en la base de datos para encontrar los m\u00e1s similares (esto es el retrieval ). Luego, El sistema recupera ( etapa 5 ): El contexto relevante desde la base de datos (fragmentos similares). La consulta del usuario original. Ambos elementos se integran en una plantilla de prompt, por ejemplo: \"Sos un buscador especializado en Bibliograf\u00eda\" ( esto ser\u00eda el prompt ) Responde esta \"consulta\" ( lo que pregunta el usuario ) basandote en el siguiente \"contexto\" ( va a ser lo que te devuelva la base vectorial ) Adem\u00e1s, en este paso se puede personalizar el comportamiento del agente, indicando que responda con un estilo determinado o como si fuera especialista en un \u00e1rea. Finalmente, un modelo de lenguaje (LLM) preentrenado toma ese prompt completo (contexto + consulta) y genera la respuesta final para el usuario ( etapa 6 ). Es importante destacar que el modelo ya est\u00e1 entrenado previamente, ya que realizar un entrenamiento desde cero suele ser costoso o t\u00e9cnicamente complejo. Por eso, es importante elegir un modelo que sea eficiente para el uso que le daremos.","title":"Etapa 2: Consulta del usuario"},{"location":"investigacion/04_arquitectura/#contextual-rag","text":"El Contextual RAG (Retrieval Augmented Generation) es una evoluci\u00f3n del RAG tradicional que mejora significativamente la precisi\u00f3n y relevancia de las respuestas mediante la incorporaci\u00f3n de contexto adicional durante el proceso de recuperaci\u00f3n. A diferencia del RAG est\u00e1ndar, que busca fragmentos similares bas\u00e1ndose \u00fanicamente en la similitud sem\u00e1ntica, el Contextual RAG considera m\u00faltiples factores para determinar qu\u00e9 informaci\u00f3n es verdaderamente relevante para la consulta del usuario. La arquitectura se puede ver en la siguiente figura: Figura 4: Arquitectura Contextual RAG El Contextual RAG es una arquitectura desarrollada en 2024 que supera una limitaci\u00f3n hist\u00f3rica: la capacidad de procesar documentos completos de una sola vez. Aunque su funcionamiento base es similar al RAG tradicional, introduce una etapa adicional de procesamiento contextual. En esta etapa, cada fragmento (chunk) se analiza en relaci\u00f3n con el documento completo mediante un prompt estructurado: <document> {{ WHOLE_DOCUMENT }} </document> Here is the chunk we want to situate within the whole document <chunk> {{ CHUNK_CONTENT }} </chunk> Please give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk. Answer only with the succinct context and nothing else . Este proceso enriquece la base de datos vectorial al almacenar no solo el fragmento original, sino tambi\u00e9n el contexto generado que lo sit\u00faa dentro del documento completo.","title":"Contextual RAG"},{"location":"investigacion/04_arquitectura/#ventajas-sobre-rag-tradicional","text":"Mayor precisi\u00f3n : Al considerar el contexto completo, reduce las respuestas fuera de tema o irrelevantes. Mejor coherencia : Mantiene una l\u00ednea de pensamiento m\u00e1s consistente en las respuestas. Respuestas m\u00e1s completas : Al entender mejor el contexto, puede proporcionar informaci\u00f3n m\u00e1s relevante y \u00fatil.","title":"Ventajas sobre RAG tradicional:"},{"location":"investigacion/04_arquitectura/#desventajas-sobre-rag-tradicional","text":"Mayor complejidad : Requiere un procesamiento adicional y una base de datos vectorial m\u00e1s grande. Latencia : Puede ser m\u00e1s lento debido al procesamiento adicional.","title":"Desventajas sobre RAG tradicional:"},{"location":"investigacion/04_arquitectura/#comparacion-entre-rag-tradicional-y-contextual-rag","text":"Los resultados de la comparaci\u00f3n entre RAG tradicional y Contextual RAG muestran una mejora significativa en la precisi\u00f3n de recuperaci\u00f3n: el uso de Contextual Embeddings redujo la tasa de fallos en la recuperaci\u00f3n de los 20 fragmentos m\u00e1s relevantes en un 35%, pasando de un 5.7% a un 3.7% de fallos. Esto demuestra que el procesamiento contextual adicional mejora sustancialmente la calidad de las b\u00fasquedas. (en las graficas aparece tambien BM25, que es un modelo de recuperaci\u00f3n de informaci\u00f3n que se basa en la frecuencia de las palabras en el documento pero no lo vamos a usar) Figura 5: Comparaci\u00f3n entre RAG (embedding) y CRAG (contextual embedding)","title":"Comparaci\u00f3n entre RAG tradicional y Contextual RAG"},{"location":"investigacion/04_arquitectura/#prompt-chaining","text":"Figura 6: Arquitectura Prompt chaining El encadenamiento de prompts descompone una tarea en una secuencia de pasos, donde cada llamada a un modelo de lenguaje procesa la salida de la anterior. Este flujo de trabajo es ideal para situaciones en las que la tarea puede dividirse de forma clara y sencilla en subtareas fijas. El objetivo principal es sacrificar algo de latencia a cambio de una mayor precisi\u00f3n, haciendo que cada llamada al modelo sea una tarea m\u00e1s simple.","title":"Prompt chaining"},{"location":"investigacion/04_arquitectura/#routing","text":"Figura 7: Arquitectura Routing El enrutamiento clasifica una entrada y la dirige a una tarea de seguimiento especializada. Este enfoque permite una clara separaci\u00f3n de responsabilidades y la creaci\u00f3n de prompts m\u00e1s espec\u00edficos y eficaces. Sin esta estructura, optimizar el rendimiento para un tipo de entrada podr\u00eda perjudicar el desempe\u00f1o en otros casos. El enrutamiento resulta especialmente \u00fatil en tareas complejas con categor\u00edas bien diferenciadas, donde cada una puede ser tratada de forma m\u00e1s eficiente por separado. La clasificaci\u00f3n inicial puede realizarse mediante un modelo de lenguaje o utilizando m\u00e9todos tradicionales de clasificaci\u00f3n.","title":"Routing"},{"location":"investigacion/04_arquitectura/#parallelization","text":"Figura 8: Arquitectura Parallelization Los LLMs pueden abordar m\u00faltiples tareas de forma simult\u00e1nea, y sus resultados pueden combinarse posteriormente mediante programaci\u00f3n. Este enfoque, conocido como paralelizaci\u00f3n, se presenta en dos formas principales: Seccionamiento : consiste en dividir una tarea en subtareas independientes que se ejecutan en paralelo. Votaci\u00f3n : implica ejecutar la misma tarea varias veces para obtener resultados variados y seleccionar el m\u00e1s adecuado. La paralelizaci\u00f3n es especialmente \u00fatil cuando las subtareas pueden procesarse simult\u00e1neamente para mejorar la velocidad, o cuando se requieren m\u00faltiples perspectivas para obtener respuestas m\u00e1s confiables. En tareas complejas con m\u00faltiples dimensiones, los LLMs suelen ofrecer mejores resultados cuando cada aspecto se gestiona por separado, permitiendo que cada parte reciba una atenci\u00f3n m\u00e1s enfocada.","title":"Parallelization"},{"location":"investigacion/04_arquitectura/#orchestrator-workers","text":"Figura 9: Arquitectura Orchestrator-workers En el flujo de trabajo de orquestador y trabajadores, un LLM central act\u00faa como orquestador: descompone din\u00e1micamente las tareas, las asigna a LLMs especializados (trabajadores) y luego sintetiza sus respuestas. Aunque este enfoque puede parecer similar a la paralelizaci\u00f3n, su diferencia clave radica en la flexibilidad: las subtareas no est\u00e1n predefinidas, sino que son generadas en tiempo real por el orquestador seg\u00fan la naturaleza de la tarea.","title":"Orchestrator-workers"},{"location":"investigacion/04_arquitectura/#evaluator-optimizer","text":"Figura 10: Arquitectura Evaluator-optimizer En el flujo de trabajo de evaluador-optimizador, una instancia de LLM genera una respuesta, mientras que otra eval\u00faa su calidad y proporciona retroalimentaci\u00f3n, formando un ciclo iterativo. Este enfoque resulta efectivo cuando se disponen de criterios de evaluaci\u00f3n bien definidos y cuando la mejora progresiva aporta un valor tangible.","title":"Evaluator-optimizer"},{"location":"investigacion/04_arquitectura/#referencia","text":"Anthropic: Building Effective Agents Anthropic: Contextual Retrieval","title":"Referencia"},{"location":"investigacion/05_opentools/","text":"\ud83d\udee0\ufe0f Herramientas Open Source para el Proyecto Este documento presenta un resumen de herramientas de c\u00f3digo abierto seleccionadas para automatizar flujos de trabajo y procesar documentos cient\u00edficos. Para cada herramienta, se evaluar\u00e1 su aplicabilidad en el sistema propuesto, considerando sus ventajas, desventajas y casos de uso espec\u00edficos. Adem\u00e1s, se analizar\u00e1 su compatibilidad con Docker, disponibilidad de APIs y capacidad de integraci\u00f3n en flujos de automatizaci\u00f3n. El objetivo es proporcionar una base s\u00f3lida para la implementaci\u00f3n de un sistema eficiente y escalable. Tabla de herramientas Herramienta Rol principal Aplicabilidad en el sistema n8n Automatizaci\u00f3n Orquestaci\u00f3n de flujos de trabajo GROBID Extracci\u00f3n de metadatos Procesamiento de PDFs cient\u00edficos Qdrant Almacenamiento vectorial Sistema RAG y b\u00fasqueda sem\u00e1ntica VosViewer An\u00e1lisis bibliom\u00e9trico Visualizaci\u00f3n de redes de colaboraci\u00f3n Ollama Modelos de lenguaje Generaci\u00f3n y procesamiento de texto \ud83e\udde0 1. n8n Funci\u00f3n: Automatizaci\u00f3n de flujos de trabajo (workflow automation). Aplicabilidad: Ideal para orquestar procesos como descarga de documentos, extracci\u00f3n de metadatos, IA, carga en base vectorial, notificaciones, etc. Ventajas: Interfaz visual, altamente extensible, soporte para HTTP, Webhooks, Google Drive, Python y m\u00e1s. Desventajas: Curva de aprendizaje inicial, puede requerir recursos significativos para workflows complejos. Docker: \u2705 Compatible con Docker, imagen oficial disponible. APIs: \u2705 API REST disponible para integraci\u00f3n program\u00e1tica. \ud83d\udcc4 2. GROBID Funci\u00f3n: Extracci\u00f3n estructurada de informaci\u00f3n de documentos cient\u00edficos en PDF. Aplicabilidad: Extrae t\u00edtulo, autores, referencias, secciones, keywords y m\u00e1s en formato XML. Ventajas: Alta precisi\u00f3n en papers acad\u00e9micos, f\u00e1cil de dockerizar e integrar con Python o n8n. Desventajas: Rendimiento variable seg\u00fan la calidad del PDF, requiere configuraci\u00f3n espec\u00edfica para diferentes formatos de documentos. Docker: \u2705 Compatible con Docker, imagen oficial disponible. APIs: \u2705 API REST disponible para procesamiento de documentos. \ud83d\udd0d 3. VosViewer Funci\u00f3n: An\u00e1lisis bibliom\u00e9trico y visualizaci\u00f3n de mapas de ciencia. Aplicabilidad: concurrencia de autores. Ventajas: interfaz gr\u00e1fica simple. Desventajas: Limitado en personalizaci\u00f3n, requiere datos preprocesados en formatos espec\u00edficos. Docker: \u274c No compatible con Docker, aplicaci\u00f3n de escritorio. (trataremos de hacer una adaptaci\u00f3n para que sea compatible) APIs: \u274c No dispone de API oficial. \ud83d\uddc3\ufe0f 4. Qdrant Funci\u00f3n: Base de datos vectorial. Aplicabilidad: Almacena los embeddings generados desde los textos cient\u00edficos para el sistema RAG. Ventajas: R\u00e1pido, soporte para metadata. Desventajas: Consumo de memoria significativo con grandes vol\u00famenes de datos, requiere optimizaci\u00f3n para producci\u00f3n. Docker: \u2705 Compatible con Docker, imagen oficial disponible. APIs: \u2705 API REST y gRPC disponibles para operaciones vectoriales. \ud83d\udcc4 5. Ollama Funci\u00f3n: Modelos de lenguaje. Aplicabilidad: Generaci\u00f3n y procesamiento de texto. Ventajas: F\u00e1cil de usar, soporte para m\u00faltiples modelos de lenguaje, ejecuci\u00f3n local sin necesidad de conexi\u00f3n a internet. Desventajas: Limitado por recursos locales, modelos pueden ser menos potentes que versiones en la nube, requiere gesti\u00f3n de memoria cuidadosa. Docker: \u2705 Compatible con Docker, imagen oficial disponible. APIs: \u2705 API REST disponible para interacci\u00f3n con modelos. \ud83d\udd17 Referencias Oficiales n8n : Sitio oficial | GitHub GROBID : Sitio oficial | GitHub VosViewer : Sitio oficial | GitHub (versi\u00f3n open source utilizada) Qdrant : Sitio oficial | GitHub Ollama : Sitio oficial | GitHub","title":"Resumen Opentools"},{"location":"investigacion/05_opentools/#herramientas-open-source-para-el-proyecto","text":"Este documento presenta un resumen de herramientas de c\u00f3digo abierto seleccionadas para automatizar flujos de trabajo y procesar documentos cient\u00edficos. Para cada herramienta, se evaluar\u00e1 su aplicabilidad en el sistema propuesto, considerando sus ventajas, desventajas y casos de uso espec\u00edficos. Adem\u00e1s, se analizar\u00e1 su compatibilidad con Docker, disponibilidad de APIs y capacidad de integraci\u00f3n en flujos de automatizaci\u00f3n. El objetivo es proporcionar una base s\u00f3lida para la implementaci\u00f3n de un sistema eficiente y escalable.","title":"\ud83d\udee0\ufe0f Herramientas Open Source para el Proyecto"},{"location":"investigacion/05_opentools/#tabla-de-herramientas","text":"Herramienta Rol principal Aplicabilidad en el sistema n8n Automatizaci\u00f3n Orquestaci\u00f3n de flujos de trabajo GROBID Extracci\u00f3n de metadatos Procesamiento de PDFs cient\u00edficos Qdrant Almacenamiento vectorial Sistema RAG y b\u00fasqueda sem\u00e1ntica VosViewer An\u00e1lisis bibliom\u00e9trico Visualizaci\u00f3n de redes de colaboraci\u00f3n Ollama Modelos de lenguaje Generaci\u00f3n y procesamiento de texto","title":"Tabla de herramientas"},{"location":"investigacion/05_opentools/#1-n8n","text":"Funci\u00f3n: Automatizaci\u00f3n de flujos de trabajo (workflow automation). Aplicabilidad: Ideal para orquestar procesos como descarga de documentos, extracci\u00f3n de metadatos, IA, carga en base vectorial, notificaciones, etc. Ventajas: Interfaz visual, altamente extensible, soporte para HTTP, Webhooks, Google Drive, Python y m\u00e1s. Desventajas: Curva de aprendizaje inicial, puede requerir recursos significativos para workflows complejos. Docker: \u2705 Compatible con Docker, imagen oficial disponible. APIs: \u2705 API REST disponible para integraci\u00f3n program\u00e1tica.","title":"\ud83e\udde0 1. n8n"},{"location":"investigacion/05_opentools/#2-grobid","text":"Funci\u00f3n: Extracci\u00f3n estructurada de informaci\u00f3n de documentos cient\u00edficos en PDF. Aplicabilidad: Extrae t\u00edtulo, autores, referencias, secciones, keywords y m\u00e1s en formato XML. Ventajas: Alta precisi\u00f3n en papers acad\u00e9micos, f\u00e1cil de dockerizar e integrar con Python o n8n. Desventajas: Rendimiento variable seg\u00fan la calidad del PDF, requiere configuraci\u00f3n espec\u00edfica para diferentes formatos de documentos. Docker: \u2705 Compatible con Docker, imagen oficial disponible. APIs: \u2705 API REST disponible para procesamiento de documentos.","title":"\ud83d\udcc4 2. GROBID"},{"location":"investigacion/05_opentools/#3-vosviewer","text":"Funci\u00f3n: An\u00e1lisis bibliom\u00e9trico y visualizaci\u00f3n de mapas de ciencia. Aplicabilidad: concurrencia de autores. Ventajas: interfaz gr\u00e1fica simple. Desventajas: Limitado en personalizaci\u00f3n, requiere datos preprocesados en formatos espec\u00edficos. Docker: \u274c No compatible con Docker, aplicaci\u00f3n de escritorio. (trataremos de hacer una adaptaci\u00f3n para que sea compatible) APIs: \u274c No dispone de API oficial.","title":"\ud83d\udd0d 3. VosViewer"},{"location":"investigacion/05_opentools/#4-qdrant","text":"Funci\u00f3n: Base de datos vectorial. Aplicabilidad: Almacena los embeddings generados desde los textos cient\u00edficos para el sistema RAG. Ventajas: R\u00e1pido, soporte para metadata. Desventajas: Consumo de memoria significativo con grandes vol\u00famenes de datos, requiere optimizaci\u00f3n para producci\u00f3n. Docker: \u2705 Compatible con Docker, imagen oficial disponible. APIs: \u2705 API REST y gRPC disponibles para operaciones vectoriales.","title":"\ud83d\uddc3\ufe0f 4. Qdrant"},{"location":"investigacion/05_opentools/#5-ollama","text":"Funci\u00f3n: Modelos de lenguaje. Aplicabilidad: Generaci\u00f3n y procesamiento de texto. Ventajas: F\u00e1cil de usar, soporte para m\u00faltiples modelos de lenguaje, ejecuci\u00f3n local sin necesidad de conexi\u00f3n a internet. Desventajas: Limitado por recursos locales, modelos pueden ser menos potentes que versiones en la nube, requiere gesti\u00f3n de memoria cuidadosa. Docker: \u2705 Compatible con Docker, imagen oficial disponible. APIs: \u2705 API REST disponible para interacci\u00f3n con modelos.","title":"\ud83d\udcc4 5. Ollama"},{"location":"investigacion/05_opentools/#referencias-oficiales","text":"n8n : Sitio oficial | GitHub GROBID : Sitio oficial | GitHub VosViewer : Sitio oficial | GitHub (versi\u00f3n open source utilizada) Qdrant : Sitio oficial | GitHub Ollama : Sitio oficial | GitHub","title":"\ud83d\udd17 Referencias Oficiales"},{"location":"investigacion/06_n8n/","text":"n8n n8n es una herramienta de automatizaci\u00f3n de flujos de trabajo ( workflow automation ) basada en nodos, de c\u00f3digo abierto y altamente personalizable. Permite conectar diferentes servicios, APIs y procesos de manera visual e intuitiva, ideal para integrar tareas complejas sin necesidad de escribir mucho c\u00f3digo. Figura 1: Ejemplo Interfaz gr\u00e1fica de n8n Caracter\u00edsticas principales \u2699\ufe0f Basado en nodos : cada paso del flujo es un nodo que realiza una tarea (leer un archivo, hacer una petici\u00f3n HTTP, analizar texto, etc.). \ud83d\udd01 Automatizaci\u00f3n condicional : permite bifurcar flujos seg\u00fan condiciones l\u00f3gicas, errores, o datos. \ud83c\udf10 Integraci\u00f3n con cientos de servicios : Google Drive, APIs REST, bases de datos, herramientas de IA, entre otros. \ud83e\udd16 Extensible con c\u00f3digo : se pueden usar c\u00f3digos JavaScript o Python personalizados. \ud83d\udcbe Self-hosted o en la nube : puede desplegarse localmente, en servidores propios, o usar su versi\u00f3n cloud. \u00bfC\u00f3mo se usa? 1. Crear un nuevo flujo Los flujos en n8n comienzan desde un nodo gatillo (trigger), como por ejemplo: Un archivo subido a Google Drive Una petici\u00f3n entrante por HTTP Un webhook desde una aplicaci\u00f3n externa mediante otro workflow 2. Encadenar nodos Luego del nodo de inicio, se encadenan nodos de procesamiento: Transformaciones con nodos como Set , Function , Merge Interacciones externas como HTTP Request , Google Sheets , Email An\u00e1lisis de datos, consultas a bases, o comunicaci\u00f3n con APIs de IA Aplicaci\u00f3n en este proyecto En el contexto del presente proyecto, n8n se utiliza como motor de orquestaci\u00f3n para automatizar tareas como: \ud83d\udd04 Esperar la notificaci\u00f3n de petici\u00f3n de procesamiento de papers \ud83d\udcca Ejecuci\u00f3n GROBID para an\u00e1lisis del paper \ud83d\udcc8 Generaci\u00f3n de res\u00famenes o procesamiento necesario \ud83d\udce4 Indexaci\u00f3n de informaci\u00f3n en la base de datos vectorial (Qdrant) \ud83e\udde0 Interacci\u00f3n con modelos de lenguaje (LLMs) \ud83d\udcc1 Interacci\u00f3n con el usuario final Ventajas de usar n8n en el proyecto Ventaja Descripci\u00f3n Ejecuci\u00f3n Local Permite el control total sin necesidad de pagos mensuales a aplicaciones externas. Automatizaci\u00f3n sin c\u00f3digo Reduce la necesidad de scripting manual. Reutilizaci\u00f3n de flujos Se pueden guardar y clonar flujos. Integraci\u00f3n r\u00e1pida Conexiones simples con m\u00faltiples servicios. Observabilidad Logs visuales y control sobre cada nodo. Control de errores Permite manejar errores de forma precisa. Instalaci\u00f3n Para utilizar n8n existen distintas alternativas. La m\u00e1s sencilla ser\u00eda usarlo directamente en la nube ( n8n cloud ); sin embargo, no optaremos por esta opci\u00f3n debido a las limitaciones que presenta, especialmente en cuanto al manejo de tokens con modelos de IA y otras integraciones. Nuestro objetivo es instalar n8n de manera local , lo que ofrece mayor control y escalabilidad. Actualmente, existen dos m\u00e9todos principales para hacerlo (solo la instalaci\u00f3n de n8n, pero luego se explica una tercer forma que implica un Self-hosted AI starter kit que incluye unas caracteristicas adicionales): 1. Instalaci\u00f3n global mediante npm No utilizaremos esta opci\u00f3n, ya que, pensando en una soluci\u00f3n escalable y mantenible, preferimos una alternativa m\u00e1s robusta. 2. Instalaci\u00f3n mediante Docker Esta es la opci\u00f3n que elegiremos por sus m\u00faltiples ventajas: Proporciona un entorno limpio para la instalaci\u00f3n. Facilita la configuraci\u00f3n de la base de datos preferida. Evita problemas de compatibilidad entre diferentes sistemas operativos, ya que Docker ofrece un entorno consistente. Simplifica la migraci\u00f3n a nuevos servidores o entornos. 3. Instalaci\u00f3n Self-hosted AI starter kit \u00bfQu\u00e9 es el Self-hosted AI Starter Kit? El Self-hosted AI Starter Kit es una soluci\u00f3n de c\u00f3digo abierto (proporcionada por n8n) dise\u00f1ada para facilitar la implementaci\u00f3n de infraestructuras de inteligencia artificial de manera local, sin depender de servicios en la nube. Proporciona un entorno preconfigurado que integra m\u00faltiples componentes clave, como: Bases de datos (PostgreSQL, Redis, etc.) Almacenamiento vectorial Servidores de modelos de lenguaje (LLMs) Interfaces web de administraci\u00f3n Todo el sistema se despliega mediante contenedores Docker , lo que garantiza portabilidad, consistencia y facilidad de mantenimiento. El kit est\u00e1 pensado para ser modular y escalable, adapt\u00e1ndose tanto a proyectos personales como a entornos productivos de mayor escala. se eligi\u00f3 utilizar el Self-hosted AI Starter Kit debido a varias razones fundamentales: Control total sobre los datos y la infraestructura. Evitar limitaciones de plataformas en la nube, como restricciones de tokens, memoria o tiempo de ejecuci\u00f3n. Mayor privacidad y seguridad de la informaci\u00f3n procesada. Facilidad de integraci\u00f3n con otras herramientas y sistemas propios. Escalabilidad , permitiendo crecer en recursos y capacidades a medida que el proyecto lo requiera. Adem\u00e1s, el enfoque basado en Docker permite que toda la infraestructura pueda ser levantada, replicada o migrada f\u00e1cilmente entre distintos entornos. Conclusi\u00f3n n8n es una herramienta poderosa y vers\u00e1til que facilita la integraci\u00f3n de servicios y la creaci\u00f3n de automatismos sin requerir gran experiencia en programaci\u00f3n. En este proyecto, permite centralizar todas las tareas repetitivas relacionadas con el an\u00e1lisis, resumen e indexaci\u00f3n de papers cient\u00edficos, funcionando como columna vertebral de la arquitectura de RAG implementada. Referencia Si est\u00e1s m\u00e1s interesado conocer m\u00e1s sobre n8n. P\u00e1gina principal Documentaci\u00f3n N8N GITHUB self-hosted-ai-starter-kit","title":"n8n"},{"location":"investigacion/06_n8n/#n8n","text":"n8n es una herramienta de automatizaci\u00f3n de flujos de trabajo ( workflow automation ) basada en nodos, de c\u00f3digo abierto y altamente personalizable. Permite conectar diferentes servicios, APIs y procesos de manera visual e intuitiva, ideal para integrar tareas complejas sin necesidad de escribir mucho c\u00f3digo. Figura 1: Ejemplo Interfaz gr\u00e1fica de n8n","title":"n8n"},{"location":"investigacion/06_n8n/#caracteristicas-principales","text":"\u2699\ufe0f Basado en nodos : cada paso del flujo es un nodo que realiza una tarea (leer un archivo, hacer una petici\u00f3n HTTP, analizar texto, etc.). \ud83d\udd01 Automatizaci\u00f3n condicional : permite bifurcar flujos seg\u00fan condiciones l\u00f3gicas, errores, o datos. \ud83c\udf10 Integraci\u00f3n con cientos de servicios : Google Drive, APIs REST, bases de datos, herramientas de IA, entre otros. \ud83e\udd16 Extensible con c\u00f3digo : se pueden usar c\u00f3digos JavaScript o Python personalizados. \ud83d\udcbe Self-hosted o en la nube : puede desplegarse localmente, en servidores propios, o usar su versi\u00f3n cloud.","title":"Caracter\u00edsticas principales"},{"location":"investigacion/06_n8n/#como-se-usa","text":"","title":"\u00bfC\u00f3mo se usa?"},{"location":"investigacion/06_n8n/#1-crear-un-nuevo-flujo","text":"Los flujos en n8n comienzan desde un nodo gatillo (trigger), como por ejemplo: Un archivo subido a Google Drive Una petici\u00f3n entrante por HTTP Un webhook desde una aplicaci\u00f3n externa mediante otro workflow","title":"1. Crear un nuevo flujo"},{"location":"investigacion/06_n8n/#2-encadenar-nodos","text":"Luego del nodo de inicio, se encadenan nodos de procesamiento: Transformaciones con nodos como Set , Function , Merge Interacciones externas como HTTP Request , Google Sheets , Email An\u00e1lisis de datos, consultas a bases, o comunicaci\u00f3n con APIs de IA","title":"2. Encadenar nodos"},{"location":"investigacion/06_n8n/#aplicacion-en-este-proyecto","text":"En el contexto del presente proyecto, n8n se utiliza como motor de orquestaci\u00f3n para automatizar tareas como: \ud83d\udd04 Esperar la notificaci\u00f3n de petici\u00f3n de procesamiento de papers \ud83d\udcca Ejecuci\u00f3n GROBID para an\u00e1lisis del paper \ud83d\udcc8 Generaci\u00f3n de res\u00famenes o procesamiento necesario \ud83d\udce4 Indexaci\u00f3n de informaci\u00f3n en la base de datos vectorial (Qdrant) \ud83e\udde0 Interacci\u00f3n con modelos de lenguaje (LLMs) \ud83d\udcc1 Interacci\u00f3n con el usuario final","title":"Aplicaci\u00f3n en este proyecto"},{"location":"investigacion/06_n8n/#ventajas-de-usar-n8n-en-el-proyecto","text":"Ventaja Descripci\u00f3n Ejecuci\u00f3n Local Permite el control total sin necesidad de pagos mensuales a aplicaciones externas. Automatizaci\u00f3n sin c\u00f3digo Reduce la necesidad de scripting manual. Reutilizaci\u00f3n de flujos Se pueden guardar y clonar flujos. Integraci\u00f3n r\u00e1pida Conexiones simples con m\u00faltiples servicios. Observabilidad Logs visuales y control sobre cada nodo. Control de errores Permite manejar errores de forma precisa.","title":"Ventajas de usar n8n en el proyecto"},{"location":"investigacion/06_n8n/#instalacion","text":"Para utilizar n8n existen distintas alternativas. La m\u00e1s sencilla ser\u00eda usarlo directamente en la nube ( n8n cloud ); sin embargo, no optaremos por esta opci\u00f3n debido a las limitaciones que presenta, especialmente en cuanto al manejo de tokens con modelos de IA y otras integraciones. Nuestro objetivo es instalar n8n de manera local , lo que ofrece mayor control y escalabilidad. Actualmente, existen dos m\u00e9todos principales para hacerlo (solo la instalaci\u00f3n de n8n, pero luego se explica una tercer forma que implica un Self-hosted AI starter kit que incluye unas caracteristicas adicionales):","title":"Instalaci\u00f3n"},{"location":"investigacion/06_n8n/#1-instalacion-global-mediante-npm","text":"No utilizaremos esta opci\u00f3n, ya que, pensando en una soluci\u00f3n escalable y mantenible, preferimos una alternativa m\u00e1s robusta.","title":"1. Instalaci\u00f3n global mediante npm"},{"location":"investigacion/06_n8n/#2-instalacion-mediante-docker","text":"Esta es la opci\u00f3n que elegiremos por sus m\u00faltiples ventajas: Proporciona un entorno limpio para la instalaci\u00f3n. Facilita la configuraci\u00f3n de la base de datos preferida. Evita problemas de compatibilidad entre diferentes sistemas operativos, ya que Docker ofrece un entorno consistente. Simplifica la migraci\u00f3n a nuevos servidores o entornos.","title":"2. Instalaci\u00f3n mediante Docker"},{"location":"investigacion/06_n8n/#3-instalacion-self-hosted-ai-starter-kit","text":"","title":"3. Instalaci\u00f3n Self-hosted AI starter kit"},{"location":"investigacion/06_n8n/#que-es-el-self-hosted-ai-starter-kit","text":"El Self-hosted AI Starter Kit es una soluci\u00f3n de c\u00f3digo abierto (proporcionada por n8n) dise\u00f1ada para facilitar la implementaci\u00f3n de infraestructuras de inteligencia artificial de manera local, sin depender de servicios en la nube. Proporciona un entorno preconfigurado que integra m\u00faltiples componentes clave, como: Bases de datos (PostgreSQL, Redis, etc.) Almacenamiento vectorial Servidores de modelos de lenguaje (LLMs) Interfaces web de administraci\u00f3n Todo el sistema se despliega mediante contenedores Docker , lo que garantiza portabilidad, consistencia y facilidad de mantenimiento. El kit est\u00e1 pensado para ser modular y escalable, adapt\u00e1ndose tanto a proyectos personales como a entornos productivos de mayor escala. se eligi\u00f3 utilizar el Self-hosted AI Starter Kit debido a varias razones fundamentales: Control total sobre los datos y la infraestructura. Evitar limitaciones de plataformas en la nube, como restricciones de tokens, memoria o tiempo de ejecuci\u00f3n. Mayor privacidad y seguridad de la informaci\u00f3n procesada. Facilidad de integraci\u00f3n con otras herramientas y sistemas propios. Escalabilidad , permitiendo crecer en recursos y capacidades a medida que el proyecto lo requiera. Adem\u00e1s, el enfoque basado en Docker permite que toda la infraestructura pueda ser levantada, replicada o migrada f\u00e1cilmente entre distintos entornos.","title":"\u00bfQu\u00e9 es el Self-hosted AI Starter Kit?"},{"location":"investigacion/06_n8n/#conclusion","text":"n8n es una herramienta poderosa y vers\u00e1til que facilita la integraci\u00f3n de servicios y la creaci\u00f3n de automatismos sin requerir gran experiencia en programaci\u00f3n. En este proyecto, permite centralizar todas las tareas repetitivas relacionadas con el an\u00e1lisis, resumen e indexaci\u00f3n de papers cient\u00edficos, funcionando como columna vertebral de la arquitectura de RAG implementada.","title":"Conclusi\u00f3n"},{"location":"investigacion/06_n8n/#referencia","text":"Si est\u00e1s m\u00e1s interesado conocer m\u00e1s sobre n8n. P\u00e1gina principal Documentaci\u00f3n N8N GITHUB self-hosted-ai-starter-kit","title":"Referencia"},{"location":"investigacion/07_grobid/","text":"GROBID Introducci\u00f3n / Problem\u00e1tica n8n dispone de un nodo llamado Extract from File que permite m\u00faltiples opciones, entre ellas, \"extract from PDF\" . Inicialmente, el flujo del sistema consist\u00eda en descargar cada paper y luego procesarlo con este nodo. Si bien este m\u00e9todo es relativamente r\u00e1pido para extraer texto plano desde un PDF, el output no es preciso ni estructurado . Esto genera una gran desventaja cuando se busca trabajar los datos de forma organizada por cada paper, especialmente al intentar identificar secciones espec\u00edficas como introducci\u00f3n, metodolog\u00eda, resultados, etc. Por lo tanto, se vuelve confuso y poco confiable para alimentar una base de datos vectorial, que luego ser\u00e1 utilizada en el sistema RAG para responder preguntas basadas en el contenido de los papers. Ante esta necesidad, se exploraron soluciones m\u00e1s robustas y espec\u00edficas para la lectura y estructuraci\u00f3n de art\u00edculos cient\u00edficos en PDF. De esta b\u00fasqueda surge GROBID , una herramienta open-source, dockerizada y ampliamente adoptada en contextos acad\u00e9micos y cient\u00edficos, que permite una extracci\u00f3n estructurada y enriquecida de los documentos. Figura 1: \"Ilustraci\u00f3n de d\u00f3nde entrar\u00eda GROBID en nuestro esquema\" \u00bfQu\u00e9 es GROBID? GROBID (GeneRation Of BIbliographic Data) es una herramienta de c\u00f3digo abierto basada en machine learning que permite extraer y estructurar informaci\u00f3n desde documentos cient\u00edficos a PDF. Est\u00e1 entrenado espec\u00edficamente en el dominio acad\u00e9mico , por lo que reconoce con alta precisi\u00f3n elementos como: T\u00edtulos Autores Afiliaciones Secciones del paper Citas bibliogr\u00e1ficas Referencias cruzadas Figuras y ecuaciones Su modelo se basa en t\u00e9cnicas de procesamiento de lenguaje natural (NLP) y CRF (Conditional Random Fields) para el etiquetado de textos, lo cual lo convierte en una alternativa ideal para estructurar art\u00edculos cient\u00edficos de forma autom\u00e1tica. Nuestro uso En nuestro proyecto, utilizamos GROBID para transformar documentos PDF en archivos XML . Esto permite luego procesar los papers por secciones espec\u00edficas, enriquecer los res\u00famenes autom\u00e1ticos y alimentar la base de datos vectorial para responder preguntas con mayor precisi\u00f3n. Figura 2: \"Conversi\u00f3n de PDF a XML estructurado mediante GROBID\" Que es XML? XML (eXtensible Markup Language) es un lenguaje de marcado dise\u00f1ado para almacenar y transportar datos de forma estructurada, legible tanto para humanos como para m\u00e1quinas. Caracter\u00edsticas clave: Etiquetas personalizables : <dato>contenido</dato> . Autodescriptivo : La estructura define el significado de los datos. Ejemplo : <libro> <titulo> El principito </titulo> <autor> Antoine de Saint-Exup\u00e9ry </autor> </libro> Referencia GROBID GitHub GROBID Documentaci\u00f3n GROBID Docker","title":"GROBID"},{"location":"investigacion/07_grobid/#grobid","text":"","title":"GROBID"},{"location":"investigacion/07_grobid/#introduccion-problematica","text":"n8n dispone de un nodo llamado Extract from File que permite m\u00faltiples opciones, entre ellas, \"extract from PDF\" . Inicialmente, el flujo del sistema consist\u00eda en descargar cada paper y luego procesarlo con este nodo. Si bien este m\u00e9todo es relativamente r\u00e1pido para extraer texto plano desde un PDF, el output no es preciso ni estructurado . Esto genera una gran desventaja cuando se busca trabajar los datos de forma organizada por cada paper, especialmente al intentar identificar secciones espec\u00edficas como introducci\u00f3n, metodolog\u00eda, resultados, etc. Por lo tanto, se vuelve confuso y poco confiable para alimentar una base de datos vectorial, que luego ser\u00e1 utilizada en el sistema RAG para responder preguntas basadas en el contenido de los papers. Ante esta necesidad, se exploraron soluciones m\u00e1s robustas y espec\u00edficas para la lectura y estructuraci\u00f3n de art\u00edculos cient\u00edficos en PDF. De esta b\u00fasqueda surge GROBID , una herramienta open-source, dockerizada y ampliamente adoptada en contextos acad\u00e9micos y cient\u00edficos, que permite una extracci\u00f3n estructurada y enriquecida de los documentos. Figura 1: \"Ilustraci\u00f3n de d\u00f3nde entrar\u00eda GROBID en nuestro esquema\"","title":"Introducci\u00f3n / Problem\u00e1tica"},{"location":"investigacion/07_grobid/#que-es-grobid","text":"GROBID (GeneRation Of BIbliographic Data) es una herramienta de c\u00f3digo abierto basada en machine learning que permite extraer y estructurar informaci\u00f3n desde documentos cient\u00edficos a PDF. Est\u00e1 entrenado espec\u00edficamente en el dominio acad\u00e9mico , por lo que reconoce con alta precisi\u00f3n elementos como: T\u00edtulos Autores Afiliaciones Secciones del paper Citas bibliogr\u00e1ficas Referencias cruzadas Figuras y ecuaciones Su modelo se basa en t\u00e9cnicas de procesamiento de lenguaje natural (NLP) y CRF (Conditional Random Fields) para el etiquetado de textos, lo cual lo convierte en una alternativa ideal para estructurar art\u00edculos cient\u00edficos de forma autom\u00e1tica.","title":"\u00bfQu\u00e9 es GROBID?"},{"location":"investigacion/07_grobid/#nuestro-uso","text":"En nuestro proyecto, utilizamos GROBID para transformar documentos PDF en archivos XML . Esto permite luego procesar los papers por secciones espec\u00edficas, enriquecer los res\u00famenes autom\u00e1ticos y alimentar la base de datos vectorial para responder preguntas con mayor precisi\u00f3n. Figura 2: \"Conversi\u00f3n de PDF a XML estructurado mediante GROBID\"","title":"Nuestro uso"},{"location":"investigacion/07_grobid/#que-es-xml","text":"XML (eXtensible Markup Language) es un lenguaje de marcado dise\u00f1ado para almacenar y transportar datos de forma estructurada, legible tanto para humanos como para m\u00e1quinas.","title":"Que es XML?"},{"location":"investigacion/07_grobid/#caracteristicas-clave","text":"Etiquetas personalizables : <dato>contenido</dato> . Autodescriptivo : La estructura define el significado de los datos. Ejemplo : <libro> <titulo> El principito </titulo> <autor> Antoine de Saint-Exup\u00e9ry </autor> </libro>","title":"Caracter\u00edsticas clave:"},{"location":"investigacion/07_grobid/#referencia","text":"GROBID GitHub GROBID Documentaci\u00f3n GROBID Docker","title":"Referencia"},{"location":"investigacion/08_nlp-models/","text":"Modelos de NLP para Revisi\u00f3n Automatizada de Literatura La integraci\u00f3n de NLP (Procesamiento de Lenguaje Natural / Natural Language Processing) representa un cambio significativo, permitiendo automatizar tareas manuales y mejorar la precisi\u00f3n de los procesos. Estas tecnolog\u00edas cierran la brecha entre lenguaje humano y m\u00e1quina, permitiendo a los sistemas procesar y comprender lenguaje natural. Esta capacidad es fundamental para desarrollar herramientas inteligentes que asistan a los ingenieros en la gesti\u00f3n de requerimientos, facilitando el desarrollo de software m\u00e1s robusto y alineado con las necesidades de los usuarios. T\u00e9cnicas de NLP para Revisi\u00f3n Automatizada de Literatura Existen tres enfoques principales para el procesamiento de lenguaje natural: El m\u00e9todo basado en frecuencia (Spacy) es el m\u00e1s simple y ligero, ideal para tareas b\u00e1sicas con bajo consumo de recursos, aunque tiene limitaciones en la comprensi\u00f3n del contexto. Los modelos Transformer representan un punto medio, ofreciendo mejor comprensi\u00f3n contextual y precisi\u00f3n sem\u00e1ntica a costa de mayores recursos y necesidad de m\u00e1s datos de entrenamiento. Los modelos RAG con LLM son la opci\u00f3n m\u00e1s avanzada, proporcionando el mejor rendimiento en an\u00e1lisis y generaci\u00f3n de texto, pero requieren significativos recursos computacionales y una implementaci\u00f3n m\u00e1s compleja. Comparativa de Rendimiento T\u00e9cnica Precisi\u00f3n Velocidad Recursos Coherencia Frecuencia Baja Alta Bajos Media Transformer Media Media Medios Alta LLM Alta Baja Altos Muy Alta Nota : Los LLMs han demostrado ser la mejor opci\u00f3n seg\u00fan m\u00e9tricas ROUGE-N, aunque requieren m\u00e1s recursos computacionales. Primero, \u00bfPor qu\u00e9 usar una arquitectura local? Implementar una arquitectura local puede ser una estrategia conveniente seg\u00fan el contexto y los objetivos del proyecto. Estas son algunas de sus principales ventajas: Privacidad : Permite mantener los datos dentro de un entorno seguro, sin necesidad de compartir informaci\u00f3n sensible en la nube. Esto es especialmente importante en contextos empresariales o acad\u00e9micos donde se maneja informaci\u00f3n cr\u00edtica. Control : Ofrece total autonom\u00eda sobre la infraestructura y el flujo de trabajo. Las decisiones t\u00e9cnicas no dependen de terceros ni de pol\u00edticas externas. Costo : Aunque la inversi\u00f3n inicial en hardware o configuraci\u00f3n puede ser m\u00e1s alta, se eliminan los pagos recurrentes de servicios en la nube. Adem\u00e1s, si el sistema de IA depende de una API externa (como ChatGPT), el costo ser\u00e1 por uso lo que puede escalar r\u00e1pidamente con el tiempo. Tambi\u00e9n implica un riesgo operativo si esa API deja de funcionar o entra en mantenimiento. Escalabilidad personalizada : Es posible adaptar la infraestructura a medida que crecen las necesidades del proyecto, sin depender de planes predeterminados o restricciones externas. Latencia : La respuesta del sistema depende \u00fanicamente de tu red y hardware, sin pasar por servidores externos. Esto mejora la velocidad, estabilidad y confiabilidad. Segundo, \u00bfQu\u00e9 modelo local escojo? Esta es una parte esencial del proyecto. esto va a depender de: - \u00bfCuantos recursos tengo? - \u00bfCuanta potencia necesito? - \u00bfQu\u00e9 tipo de tarea voy a realizar? Algunos factores a tener en cuenta: Tama\u00f1o de modelo: modelos grandes ofrecen m\u00e1s precisi\u00f3n, pero consumen m\u00e1s recursos; modelos m\u00e1s peque\u00f1os son r\u00e1pidos y ligeros. Dominio: existen modelos especializados para tareas espec\u00edficas. por ejemplo: medicina. (en su fase de entrenamiento fueron preparados con una base m\u00e1s orientada a su uso.) Esto es importante porque no siempre un modelo m\u00e1s complejo va a ser mejor para resolver una tarea, con menos recursos un modelo especializado puede responder much\u00edsimo mejor. Idioma: algunos modelos est\u00e1n entrenados en idiomas espec\u00edficos. Velocidad vs precisi\u00f3n: modelos ligeros son r\u00e1pidos para tiempo real; modelos grandes destacan en tareas complejas. Par\u00e1metros: Los par\u00e1metros en los modelos de inteligencia artificial son valores que el modelo ajusta autom\u00e1ticamente durante el proceso de entrenamiento. Estos determinan c\u00f3mo el modelo procesa la informaci\u00f3n y toma decisiones. En redes neuronales, por ejemplo, los par\u00e1metros son los pesos y sesgos que se ajustan para minimizar el error entre las predicciones del modelo y los resultados reales. Cuantos m\u00e1s par\u00e1metros tiene un modelo, mayor es su capacidad para aprender patrones complejos, aunque tambi\u00e9n aumenta el riesgo de sobreajuste y la necesidad de m\u00e1s datos y potencia de c\u00f3mputo. Lo que pasa con los modelos es que salen nuevos cada d\u00eda, y es muy dif\u00edcil mantenerse al d\u00eda. Una manera de elegir es basandose en el leaderboard de modelos. En este caso, lo que haremos es usar el leaderboard de Hugging Face como referencia de los modelos que se podrian utilizar. RANKING DE MODELOS EN LOCAL Aclaraci\u00f3n: Actualmente, no existe un leaderboard oficial y centralizado para modelos espec\u00edficamente preparados para ejecutarse en Ollama, como s\u00ed lo hay en Hugging Face. El ranking enlazado corresponde a modelos evaluados y comparados por la comunidad de Hugging Face, pero no necesariamente refleja el rendimiento de esos modelos ejecutados en Ollama. Si bien Ollama soporta varios de estos modelos y permite correrlos localmente, la plataforma no mantiene un ranking propio ni m\u00e9tricas de comparaci\u00f3n estandarizadas. Para comparar modelos en Ollama, se recomienda consultar el cat\u00e1logo oficial de modelos o buscar benchmarks independientes realizados por la comunidad. Comparativa de modelos aplicables al caso (creo que esto es mejor responderlo luego) Referencias A Systematic Literature Review on Natural Language Processing (NLP) The State of the Art of Natural Language Automated Literature Review Using NLP Techniques and LLM-Based Retrieval-Augmented Generation The Use of Natural Language Processing in Literature Reviews","title":"Modelos de lenguaje"},{"location":"investigacion/08_nlp-models/#modelos-de-nlp-para-revision-automatizada-de-literatura","text":"La integraci\u00f3n de NLP (Procesamiento de Lenguaje Natural / Natural Language Processing) representa un cambio significativo, permitiendo automatizar tareas manuales y mejorar la precisi\u00f3n de los procesos. Estas tecnolog\u00edas cierran la brecha entre lenguaje humano y m\u00e1quina, permitiendo a los sistemas procesar y comprender lenguaje natural. Esta capacidad es fundamental para desarrollar herramientas inteligentes que asistan a los ingenieros en la gesti\u00f3n de requerimientos, facilitando el desarrollo de software m\u00e1s robusto y alineado con las necesidades de los usuarios.","title":"Modelos de NLP para Revisi\u00f3n Automatizada de Literatura"},{"location":"investigacion/08_nlp-models/#tecnicas-de-nlp-para-revision-automatizada-de-literatura","text":"Existen tres enfoques principales para el procesamiento de lenguaje natural: El m\u00e9todo basado en frecuencia (Spacy) es el m\u00e1s simple y ligero, ideal para tareas b\u00e1sicas con bajo consumo de recursos, aunque tiene limitaciones en la comprensi\u00f3n del contexto. Los modelos Transformer representan un punto medio, ofreciendo mejor comprensi\u00f3n contextual y precisi\u00f3n sem\u00e1ntica a costa de mayores recursos y necesidad de m\u00e1s datos de entrenamiento. Los modelos RAG con LLM son la opci\u00f3n m\u00e1s avanzada, proporcionando el mejor rendimiento en an\u00e1lisis y generaci\u00f3n de texto, pero requieren significativos recursos computacionales y una implementaci\u00f3n m\u00e1s compleja.","title":"T\u00e9cnicas de NLP para Revisi\u00f3n Automatizada de Literatura"},{"location":"investigacion/08_nlp-models/#comparativa-de-rendimiento","text":"T\u00e9cnica Precisi\u00f3n Velocidad Recursos Coherencia Frecuencia Baja Alta Bajos Media Transformer Media Media Medios Alta LLM Alta Baja Altos Muy Alta Nota : Los LLMs han demostrado ser la mejor opci\u00f3n seg\u00fan m\u00e9tricas ROUGE-N, aunque requieren m\u00e1s recursos computacionales.","title":"Comparativa de Rendimiento"},{"location":"investigacion/08_nlp-models/#primero-por-que-usar-una-arquitectura-local","text":"Implementar una arquitectura local puede ser una estrategia conveniente seg\u00fan el contexto y los objetivos del proyecto. Estas son algunas de sus principales ventajas: Privacidad : Permite mantener los datos dentro de un entorno seguro, sin necesidad de compartir informaci\u00f3n sensible en la nube. Esto es especialmente importante en contextos empresariales o acad\u00e9micos donde se maneja informaci\u00f3n cr\u00edtica. Control : Ofrece total autonom\u00eda sobre la infraestructura y el flujo de trabajo. Las decisiones t\u00e9cnicas no dependen de terceros ni de pol\u00edticas externas. Costo : Aunque la inversi\u00f3n inicial en hardware o configuraci\u00f3n puede ser m\u00e1s alta, se eliminan los pagos recurrentes de servicios en la nube. Adem\u00e1s, si el sistema de IA depende de una API externa (como ChatGPT), el costo ser\u00e1 por uso lo que puede escalar r\u00e1pidamente con el tiempo. Tambi\u00e9n implica un riesgo operativo si esa API deja de funcionar o entra en mantenimiento. Escalabilidad personalizada : Es posible adaptar la infraestructura a medida que crecen las necesidades del proyecto, sin depender de planes predeterminados o restricciones externas. Latencia : La respuesta del sistema depende \u00fanicamente de tu red y hardware, sin pasar por servidores externos. Esto mejora la velocidad, estabilidad y confiabilidad.","title":"Primero, \u00bfPor qu\u00e9 usar una arquitectura local?"},{"location":"investigacion/08_nlp-models/#segundo-que-modelo-local-escojo","text":"Esta es una parte esencial del proyecto. esto va a depender de: - \u00bfCuantos recursos tengo? - \u00bfCuanta potencia necesito? - \u00bfQu\u00e9 tipo de tarea voy a realizar? Algunos factores a tener en cuenta: Tama\u00f1o de modelo: modelos grandes ofrecen m\u00e1s precisi\u00f3n, pero consumen m\u00e1s recursos; modelos m\u00e1s peque\u00f1os son r\u00e1pidos y ligeros. Dominio: existen modelos especializados para tareas espec\u00edficas. por ejemplo: medicina. (en su fase de entrenamiento fueron preparados con una base m\u00e1s orientada a su uso.) Esto es importante porque no siempre un modelo m\u00e1s complejo va a ser mejor para resolver una tarea, con menos recursos un modelo especializado puede responder much\u00edsimo mejor. Idioma: algunos modelos est\u00e1n entrenados en idiomas espec\u00edficos. Velocidad vs precisi\u00f3n: modelos ligeros son r\u00e1pidos para tiempo real; modelos grandes destacan en tareas complejas. Par\u00e1metros: Los par\u00e1metros en los modelos de inteligencia artificial son valores que el modelo ajusta autom\u00e1ticamente durante el proceso de entrenamiento. Estos determinan c\u00f3mo el modelo procesa la informaci\u00f3n y toma decisiones. En redes neuronales, por ejemplo, los par\u00e1metros son los pesos y sesgos que se ajustan para minimizar el error entre las predicciones del modelo y los resultados reales. Cuantos m\u00e1s par\u00e1metros tiene un modelo, mayor es su capacidad para aprender patrones complejos, aunque tambi\u00e9n aumenta el riesgo de sobreajuste y la necesidad de m\u00e1s datos y potencia de c\u00f3mputo. Lo que pasa con los modelos es que salen nuevos cada d\u00eda, y es muy dif\u00edcil mantenerse al d\u00eda. Una manera de elegir es basandose en el leaderboard de modelos. En este caso, lo que haremos es usar el leaderboard de Hugging Face como referencia de los modelos que se podrian utilizar. RANKING DE MODELOS EN LOCAL Aclaraci\u00f3n: Actualmente, no existe un leaderboard oficial y centralizado para modelos espec\u00edficamente preparados para ejecutarse en Ollama, como s\u00ed lo hay en Hugging Face. El ranking enlazado corresponde a modelos evaluados y comparados por la comunidad de Hugging Face, pero no necesariamente refleja el rendimiento de esos modelos ejecutados en Ollama. Si bien Ollama soporta varios de estos modelos y permite correrlos localmente, la plataforma no mantiene un ranking propio ni m\u00e9tricas de comparaci\u00f3n estandarizadas. Para comparar modelos en Ollama, se recomienda consultar el cat\u00e1logo oficial de modelos o buscar benchmarks independientes realizados por la comunidad.","title":"Segundo, \u00bfQu\u00e9 modelo local escojo?"},{"location":"investigacion/08_nlp-models/#comparativa-de-modelos-aplicables-al-caso-creo-que-esto-es-mejor-responderlo-luego","text":"","title":"Comparativa de modelos aplicables al caso (creo que esto es mejor responderlo luego)"},{"location":"investigacion/08_nlp-models/#referencias","text":"A Systematic Literature Review on Natural Language Processing (NLP) The State of the Art of Natural Language Automated Literature Review Using NLP Techniques and LLM-Based Retrieval-Augmented Generation The Use of Natural Language Processing in Literature Reviews","title":"Referencias"},{"location":"investigacion/09_mcp/","text":"\u00bfQu\u00e9 es el Model Context Protocol (MCP)? Es un est\u00e1ndar abierto dise\u00f1ado para mejorar la interoperabilidad y la transparencia en el uso de modelos de inteligencia artificial, especialmente los modelos de lenguaje. MCP fue presentado por Anthropic y otros colaboradores de la industria con el objetivo de facilitar la integraci\u00f3n, el control y la comprensi\u00f3n de c\u00f3mo los modelos procesan y utilizan el contexto proporcionado por los usuarios y las aplicaciones. MCP es un protocolo abierto que estandariza c\u00f3mo las aplicaciones proporcionan contexto a los LLMs. Se puede pensar en MCP como el puerto USB-C para las aplicaciones de IA: as\u00ed como el USB-C ofrece una forma estandarizada de conectar dispositivos a diferentes perif\u00e9ricos y accesorios, MCP proporciona una forma estandarizada de conectar modelos de IA a distintas fuentes de datos y herramientas. Figura 1: \"Antes y despu\u00e9s de MCP\" Objetivos principales del MCP Interoperabilidad y flexibilidad: Permite la conexi\u00f3n directa con integraciones preconstruidas y facilita el intercambio entre diferentes proveedores de LLMs, estandarizando la forma en que los modelos y aplicaciones comparten informaci\u00f3n contextual. Transparencia y control: Estandariza la descripci\u00f3n y transmisi\u00f3n del contexto a los modelos, permitiendo a usuarios y desarrolladores entender mejor qu\u00e9 informaci\u00f3n se utiliza en cada interacci\u00f3n, mientras mantiene los datos dentro de la infraestructura propia. Seguridad y cumplimiento: Permite especificar y auditar qu\u00e9 datos se incluyen en el contexto, mejorando la privacidad y el cumplimiento de normativas mediante mejores pr\u00e1cticas de seguridad. Arquitectura general de MCP En esencia, MCP sigue una arquitectura cliente-servidor donde una aplicaci\u00f3n anfitriona puede conectarse a m\u00faltiples servidores: Figura 2: \"Arquitectura general de MCP\" MCP Hosts: Programas como Claude Desktop, IDEs o herramientas de IA que desean acceder a datos a trav\u00e9s de MCP. MCP Clients: Clientes de protocolo que mantienen conexiones 1:1 con los servidores. MCP Servers: Programas ligeros que exponen capacidades espec\u00edficas mediante el protocolo MCP estandarizado. Fuentes de datos locales: Archivos, bases de datos y servicios de tu computadora a los que los servidores MCP pueden acceder de forma segura. Servicios remotos: Sistemas externos disponibles en internet (por ejemplo, a trav\u00e9s de APIs) a los que los servidores MCP pueden conectarse. Referencias Anthropic. (2024). Model Context Protocol Model Context Protocol. (2024). Introduction Model Context Protocol. (2024). GitHub Repository","title":"MCP"},{"location":"investigacion/09_mcp/#que-es-el-model-context-protocol-mcp","text":"Es un est\u00e1ndar abierto dise\u00f1ado para mejorar la interoperabilidad y la transparencia en el uso de modelos de inteligencia artificial, especialmente los modelos de lenguaje. MCP fue presentado por Anthropic y otros colaboradores de la industria con el objetivo de facilitar la integraci\u00f3n, el control y la comprensi\u00f3n de c\u00f3mo los modelos procesan y utilizan el contexto proporcionado por los usuarios y las aplicaciones. MCP es un protocolo abierto que estandariza c\u00f3mo las aplicaciones proporcionan contexto a los LLMs. Se puede pensar en MCP como el puerto USB-C para las aplicaciones de IA: as\u00ed como el USB-C ofrece una forma estandarizada de conectar dispositivos a diferentes perif\u00e9ricos y accesorios, MCP proporciona una forma estandarizada de conectar modelos de IA a distintas fuentes de datos y herramientas. Figura 1: \"Antes y despu\u00e9s de MCP\"","title":"\u00bfQu\u00e9 es el Model Context Protocol (MCP)?"},{"location":"investigacion/09_mcp/#objetivos-principales-del-mcp","text":"Interoperabilidad y flexibilidad: Permite la conexi\u00f3n directa con integraciones preconstruidas y facilita el intercambio entre diferentes proveedores de LLMs, estandarizando la forma en que los modelos y aplicaciones comparten informaci\u00f3n contextual. Transparencia y control: Estandariza la descripci\u00f3n y transmisi\u00f3n del contexto a los modelos, permitiendo a usuarios y desarrolladores entender mejor qu\u00e9 informaci\u00f3n se utiliza en cada interacci\u00f3n, mientras mantiene los datos dentro de la infraestructura propia. Seguridad y cumplimiento: Permite especificar y auditar qu\u00e9 datos se incluyen en el contexto, mejorando la privacidad y el cumplimiento de normativas mediante mejores pr\u00e1cticas de seguridad.","title":"Objetivos principales del MCP"},{"location":"investigacion/09_mcp/#arquitectura-general-de-mcp","text":"En esencia, MCP sigue una arquitectura cliente-servidor donde una aplicaci\u00f3n anfitriona puede conectarse a m\u00faltiples servidores: Figura 2: \"Arquitectura general de MCP\" MCP Hosts: Programas como Claude Desktop, IDEs o herramientas de IA que desean acceder a datos a trav\u00e9s de MCP. MCP Clients: Clientes de protocolo que mantienen conexiones 1:1 con los servidores. MCP Servers: Programas ligeros que exponen capacidades espec\u00edficas mediante el protocolo MCP estandarizado. Fuentes de datos locales: Archivos, bases de datos y servicios de tu computadora a los que los servidores MCP pueden acceder de forma segura. Servicios remotos: Sistemas externos disponibles en internet (por ejemplo, a trav\u00e9s de APIs) a los que los servidores MCP pueden conectarse.","title":"Arquitectura general de MCP"},{"location":"investigacion/09_mcp/#referencias","text":"Anthropic. (2024). Model Context Protocol Model Context Protocol. (2024). Introduction Model Context Protocol. (2024). GitHub Repository","title":"Referencias"},{"location":"investigacion/10_a2a/","text":"\u00bfQu\u00e9 es el protocolo A2A (Agent2Agent)? El protocolo A2A (Agent2Agent) es un est\u00e1ndar abierto impulsado por Google Cloud y una amplia coalici\u00f3n de socios tecnol\u00f3gicos, dise\u00f1ado para permitir la interoperabilidad y colaboraci\u00f3n entre agentes de inteligencia artificial (AI agents), incluso si han sido desarrollados por diferentes proveedores o frameworks. A2A busca crear un ecosistema donde los agentes puedan comunicarse, compartir informaci\u00f3n y coordinar acciones de manera segura y eficiente, facilitando la automatizaci\u00f3n de flujos de trabajo complejos en entornos empresariales y m\u00e1s all\u00e1. \u00bfPor qu\u00e9 A2A? A medida que las empresas y organizaciones adoptan agentes aut\u00f3nomos para automatizar tareas y procesos, surge la necesidad de que estos agentes puedan colaborar entre s\u00ed, independientemente de su origen o tecnolog\u00eda subyacente. A2A responde a esta necesidad proporcionando: Interoperabilidad universal: Permite que agentes de distintos proveedores y plataformas trabajen juntos sin fricciones. Colaboraci\u00f3n segura: Incorpora mecanismos de autenticaci\u00f3n y autorizaci\u00f3n de nivel empresarial. Flexibilidad y escalabilidad: Soporta desde tareas r\u00e1pidas hasta procesos de larga duraci\u00f3n, con actualizaciones de estado y retroalimentaci\u00f3n en tiempo real. Soporte multimodal: No se limita solo a texto, sino que tambi\u00e9n puede manejar audio, video y otros tipos de datos. Basado en est\u00e1ndares existentes: Utiliza tecnolog\u00edas ampliamente adoptadas como HTTP, SSE y JSON-RPC, facilitando la integraci\u00f3n con sistemas actuales. MCP vs A2A: \u00bfCu\u00e1l es la diferencia? La diferencia principal entre MCP y A2A radica en su enfoque: MCP funciona como un \"puerto USB-C\" que conecta modelos de IA con sus fuentes de datos, mientras que A2A opera como un \"protocolo de red\" que facilita la colaboraci\u00f3n y coordinaci\u00f3n entre diferentes agentes de IA. Figura 1: \"Comparativa esquem\u00e1tica entre MCP y A2A\" \u00bfC\u00f3mo funciona A2A? A2A define la comunicaci\u00f3n entre un \"agente cliente\" (que formula y comunica tareas) y un \"agente remoto\" (que ejecuta las tareas y responde). El protocolo incluye: Descubrimiento de capacidades: Los agentes pueden anunciar sus capacidades mediante un \"Agent Card\" en formato JSON, permitiendo que otros agentes identifiquen cu\u00e1l es el m\u00e1s adecuado para una tarea. Figura 2: \"Ejemplo de Agent Card\" Gesti\u00f3n de tareas: Las tareas tienen un ciclo de vida definido y pueden ser seguidas y actualizadas en tiempo real. Colaboraci\u00f3n y mensajer\u00eda: Los agentes pueden intercambiar mensajes, contexto, artefactos y directivas del usuario. Negociaci\u00f3n de experiencia de usuario: Los mensajes pueden incluir diferentes tipos de contenido (por ejemplo, im\u00e1genes, formularios web, video), permitiendo adaptar la interacci\u00f3n a las capacidades del usuario o la interfaz. Ejemplo de uso En la planificaci\u00f3n de unas vacaciones, un agente coordinador puede buscar destinos, reservar vuelos y alojamientos, mientras colabora con otros agentes especializados en encontrar actividades tur\u00edsticas, gestionar itinerarios y optimizar presupuestos, todo de manera integrada y automatizada a trav\u00e9s de una \u00fanica interfaz. Impacto y futuro A2A tiene el potencial de transformar la forma en que los agentes de IA colaboran, permitiendo la creaci\u00f3n de soluciones m\u00e1s potentes, flexibles y seguras en sectores como la empresa, la manufactura, el comercio y m\u00e1s. Al estandarizar la comunicaci\u00f3n entre agentes, se fomenta la innovaci\u00f3n y se facilita la adopci\u00f3n de la inteligencia artificial a gran escala. Referencias A2A: A new era of agent interoperability (Google Developers Blog) A2A Protocol Specification (GitHub)","title":"A2A"},{"location":"investigacion/10_a2a/#que-es-el-protocolo-a2a-agent2agent","text":"El protocolo A2A (Agent2Agent) es un est\u00e1ndar abierto impulsado por Google Cloud y una amplia coalici\u00f3n de socios tecnol\u00f3gicos, dise\u00f1ado para permitir la interoperabilidad y colaboraci\u00f3n entre agentes de inteligencia artificial (AI agents), incluso si han sido desarrollados por diferentes proveedores o frameworks. A2A busca crear un ecosistema donde los agentes puedan comunicarse, compartir informaci\u00f3n y coordinar acciones de manera segura y eficiente, facilitando la automatizaci\u00f3n de flujos de trabajo complejos en entornos empresariales y m\u00e1s all\u00e1.","title":"\u00bfQu\u00e9 es el protocolo A2A (Agent2Agent)?"},{"location":"investigacion/10_a2a/#por-que-a2a","text":"A medida que las empresas y organizaciones adoptan agentes aut\u00f3nomos para automatizar tareas y procesos, surge la necesidad de que estos agentes puedan colaborar entre s\u00ed, independientemente de su origen o tecnolog\u00eda subyacente. A2A responde a esta necesidad proporcionando: Interoperabilidad universal: Permite que agentes de distintos proveedores y plataformas trabajen juntos sin fricciones. Colaboraci\u00f3n segura: Incorpora mecanismos de autenticaci\u00f3n y autorizaci\u00f3n de nivel empresarial. Flexibilidad y escalabilidad: Soporta desde tareas r\u00e1pidas hasta procesos de larga duraci\u00f3n, con actualizaciones de estado y retroalimentaci\u00f3n en tiempo real. Soporte multimodal: No se limita solo a texto, sino que tambi\u00e9n puede manejar audio, video y otros tipos de datos. Basado en est\u00e1ndares existentes: Utiliza tecnolog\u00edas ampliamente adoptadas como HTTP, SSE y JSON-RPC, facilitando la integraci\u00f3n con sistemas actuales.","title":"\u00bfPor qu\u00e9 A2A?"},{"location":"investigacion/10_a2a/#mcp-vs-a2a-cual-es-la-diferencia","text":"La diferencia principal entre MCP y A2A radica en su enfoque: MCP funciona como un \"puerto USB-C\" que conecta modelos de IA con sus fuentes de datos, mientras que A2A opera como un \"protocolo de red\" que facilita la colaboraci\u00f3n y coordinaci\u00f3n entre diferentes agentes de IA. Figura 1: \"Comparativa esquem\u00e1tica entre MCP y A2A\"","title":"MCP vs A2A: \u00bfCu\u00e1l es la diferencia?"},{"location":"investigacion/10_a2a/#como-funciona-a2a","text":"A2A define la comunicaci\u00f3n entre un \"agente cliente\" (que formula y comunica tareas) y un \"agente remoto\" (que ejecuta las tareas y responde). El protocolo incluye: Descubrimiento de capacidades: Los agentes pueden anunciar sus capacidades mediante un \"Agent Card\" en formato JSON, permitiendo que otros agentes identifiquen cu\u00e1l es el m\u00e1s adecuado para una tarea. Figura 2: \"Ejemplo de Agent Card\" Gesti\u00f3n de tareas: Las tareas tienen un ciclo de vida definido y pueden ser seguidas y actualizadas en tiempo real. Colaboraci\u00f3n y mensajer\u00eda: Los agentes pueden intercambiar mensajes, contexto, artefactos y directivas del usuario. Negociaci\u00f3n de experiencia de usuario: Los mensajes pueden incluir diferentes tipos de contenido (por ejemplo, im\u00e1genes, formularios web, video), permitiendo adaptar la interacci\u00f3n a las capacidades del usuario o la interfaz.","title":"\u00bfC\u00f3mo funciona A2A?"},{"location":"investigacion/10_a2a/#ejemplo-de-uso","text":"En la planificaci\u00f3n de unas vacaciones, un agente coordinador puede buscar destinos, reservar vuelos y alojamientos, mientras colabora con otros agentes especializados en encontrar actividades tur\u00edsticas, gestionar itinerarios y optimizar presupuestos, todo de manera integrada y automatizada a trav\u00e9s de una \u00fanica interfaz.","title":"Ejemplo de uso"},{"location":"investigacion/10_a2a/#impacto-y-futuro","text":"A2A tiene el potencial de transformar la forma en que los agentes de IA colaboran, permitiendo la creaci\u00f3n de soluciones m\u00e1s potentes, flexibles y seguras en sectores como la empresa, la manufactura, el comercio y m\u00e1s. Al estandarizar la comunicaci\u00f3n entre agentes, se fomenta la innovaci\u00f3n y se facilita la adopci\u00f3n de la inteligencia artificial a gran escala.","title":"Impacto y futuro"},{"location":"investigacion/10_a2a/#referencias","text":"A2A: A new era of agent interoperability (Google Developers Blog) A2A Protocol Specification (GitHub)","title":"Referencias"},{"location":"investigacion/11_interface-options/","text":"Evaluaci\u00f3n de Interfaces de Usuario para el Agente de IA Una parte esencial del sistema consiste en definir c\u00f3mo el usuario final interact\u00faa con el agente de inteligencia artificial integrado mediante n8n. A continuaci\u00f3n, se presentan distintas interfaces consideradas, evaluando su facilidad de implementaci\u00f3n, flexibilidad, experiencia de usuario y compatibilidad con el enfoque actual del proyecto. Opciones evaluadas: Interfaz Tipo Requiere Backend Propio Personalizable Experiencia de Usuario Observaciones Webstudio.is Web builder visual No Limitada Alta Ideal para sitios est\u00e1ticos. Poco flexible para flujos IA din\u00e1micos. UXPilot.ai Plataforma No-code IA No Baja Alta Orientada a formularios r\u00e1pidos. No admite l\u00f3gica compleja o flujos con herramientas como n8n. Wix Web builder No (con Velo opcional) Media Media-Alta Compatible con Webhooks de n8n, pero limitado en backend y escalabilidad. Streamlit Python App S\u00ed Alta Alta Ideal para visualizaciones r\u00e1pidas, pero requiere levantar servidor externo. CLI (l\u00ednea de comandos) Terminal No Alta Baja \u00datil para pruebas t\u00e9cnicas, no apta para usuarios finales. HTML + fetch + Webhook Web simple No Media Media Implementaci\u00f3n r\u00e1pida, pero no escala bien sin backend intermedio. OpenWebUI Web UI local + API No (self-hosted) Alta Alta Interfaz amigable, pensada para servir como frontend de agentes IA. Integrable con Ollama/n8n f\u00e1cilmente. Elecci\u00f3n final: \u2705 OpenWebUI Figura 1: \"Ejemplo de uso de OpenWebUI\" Se opta por utilizar OpenWebUI como interfaz de interacci\u00f3n con el agente por las siguientes razones: Autohospedada y local : Compatible con la filosof\u00eda del proyecto basada en infraestructura local y control de datos. Dise\u00f1ada para IA : Ofrece una experiencia tipo \"chat\" optimizada para trabajar con modelos como LLaMA, Mistral u Ollama sin necesidad de desarrollos frontend desde cero. Integraci\u00f3n simple con n8n : Se puede conectar a flujos de n8n mediante API, Webhooks o enrutar consultas directamente desde la interfaz de chat. Buena experiencia de usuario : Presenta una interfaz limpia, intuitiva y personalizable. Escalable : Admite m\u00faltiples usuarios, historial de conversaciones y ajustes por backend, permitiendo su evoluci\u00f3n a futuro. Nota : En fases futuras podr\u00eda integrarse OpenWebUI con funcionalidades adicionales de seguimiento, contexto conversacional avanzado o dashboards espec\u00edficos para an\u00e1lisis de papers, seg\u00fan la evoluci\u00f3n del agente. Referencias Webstudio.is - Plataforma de desarrollo web visual UXPilot.ai - Constructor de interfaces de IA sin c\u00f3digo Wix - Constructor de sitios web con opciones de desarrollo Streamlit - Framework para crear aplicaciones web con Python OpenWebUI - Interfaz web de c\u00f3digo abierto para Ollama n8n - Plataforma de automatizaci\u00f3n de flujos de trabajo Ollama - Framework para ejecutar modelos de lenguaje localmente OpenWebUI Tutorial - Video tutorial sobre la configuraci\u00f3n y uso de OpenWebUI OpenWebUI n8n Pipe - integraci\u00f3n entre OpenWebUI y n8n OpenWebUI Documentation - Documentaci\u00f3n oficial de OpenWebUI","title":"Interfaz de usuario"},{"location":"investigacion/11_interface-options/#evaluacion-de-interfaces-de-usuario-para-el-agente-de-ia","text":"Una parte esencial del sistema consiste en definir c\u00f3mo el usuario final interact\u00faa con el agente de inteligencia artificial integrado mediante n8n. A continuaci\u00f3n, se presentan distintas interfaces consideradas, evaluando su facilidad de implementaci\u00f3n, flexibilidad, experiencia de usuario y compatibilidad con el enfoque actual del proyecto.","title":"Evaluaci\u00f3n de Interfaces de Usuario para el Agente de IA"},{"location":"investigacion/11_interface-options/#opciones-evaluadas","text":"Interfaz Tipo Requiere Backend Propio Personalizable Experiencia de Usuario Observaciones Webstudio.is Web builder visual No Limitada Alta Ideal para sitios est\u00e1ticos. Poco flexible para flujos IA din\u00e1micos. UXPilot.ai Plataforma No-code IA No Baja Alta Orientada a formularios r\u00e1pidos. No admite l\u00f3gica compleja o flujos con herramientas como n8n. Wix Web builder No (con Velo opcional) Media Media-Alta Compatible con Webhooks de n8n, pero limitado en backend y escalabilidad. Streamlit Python App S\u00ed Alta Alta Ideal para visualizaciones r\u00e1pidas, pero requiere levantar servidor externo. CLI (l\u00ednea de comandos) Terminal No Alta Baja \u00datil para pruebas t\u00e9cnicas, no apta para usuarios finales. HTML + fetch + Webhook Web simple No Media Media Implementaci\u00f3n r\u00e1pida, pero no escala bien sin backend intermedio. OpenWebUI Web UI local + API No (self-hosted) Alta Alta Interfaz amigable, pensada para servir como frontend de agentes IA. Integrable con Ollama/n8n f\u00e1cilmente.","title":"Opciones evaluadas:"},{"location":"investigacion/11_interface-options/#eleccion-final-openwebui","text":"Figura 1: \"Ejemplo de uso de OpenWebUI\" Se opta por utilizar OpenWebUI como interfaz de interacci\u00f3n con el agente por las siguientes razones: Autohospedada y local : Compatible con la filosof\u00eda del proyecto basada en infraestructura local y control de datos. Dise\u00f1ada para IA : Ofrece una experiencia tipo \"chat\" optimizada para trabajar con modelos como LLaMA, Mistral u Ollama sin necesidad de desarrollos frontend desde cero. Integraci\u00f3n simple con n8n : Se puede conectar a flujos de n8n mediante API, Webhooks o enrutar consultas directamente desde la interfaz de chat. Buena experiencia de usuario : Presenta una interfaz limpia, intuitiva y personalizable. Escalable : Admite m\u00faltiples usuarios, historial de conversaciones y ajustes por backend, permitiendo su evoluci\u00f3n a futuro. Nota : En fases futuras podr\u00eda integrarse OpenWebUI con funcionalidades adicionales de seguimiento, contexto conversacional avanzado o dashboards espec\u00edficos para an\u00e1lisis de papers, seg\u00fan la evoluci\u00f3n del agente.","title":"Elecci\u00f3n final: \u2705 OpenWebUI"},{"location":"investigacion/11_interface-options/#referencias","text":"Webstudio.is - Plataforma de desarrollo web visual UXPilot.ai - Constructor de interfaces de IA sin c\u00f3digo Wix - Constructor de sitios web con opciones de desarrollo Streamlit - Framework para crear aplicaciones web con Python OpenWebUI - Interfaz web de c\u00f3digo abierto para Ollama n8n - Plataforma de automatizaci\u00f3n de flujos de trabajo Ollama - Framework para ejecutar modelos de lenguaje localmente OpenWebUI Tutorial - Video tutorial sobre la configuraci\u00f3n y uso de OpenWebUI OpenWebUI n8n Pipe - integraci\u00f3n entre OpenWebUI y n8n OpenWebUI Documentation - Documentaci\u00f3n oficial de OpenWebUI","title":"Referencias"},{"location":"proyecto/01_metodologia/","text":"Desarrollo \u00c1gil de Software El desarrollo \u00e1gil es un enfoque flexible e iterativo para la creaci\u00f3n de software que prioriza la colaboraci\u00f3n, la adaptaci\u00f3n a los cambios y la entrega de valor al cliente. Se basa en el Manifiesto \u00c1gil , que establece cuatro valores fundamentales: Individuos e interacciones sobre procesos y herramientas. Software funcionando sobre documentaci\u00f3n extensiva. Colaboraci\u00f3n con el cliente sobre negociaci\u00f3n de contratos. Respuesta ante el cambio sobre seguir un plan r\u00edgido. Adem\u00e1s, se apoya en 12 principios , entre los que destacan: - Entrega frecuente de software funcional. - Cooperaci\u00f3n entre equipos de desarrollo y negocio. - Capacidad de adaptaci\u00f3n a cambios en los requisitos del proyecto. Scrum: Un Marco de Trabajo \u00c1gil Dentro de las metodolog\u00edas \u00e1giles, Scrum Es un marco ampliamente utilizado para la gesti\u00f3n de proyectos. Se basa en ciclos iterativos llamados sprints , que suelen durar entre una y cuatro semanas. Antes de cada sprint, el equipo trabaja con el Product Backlog (Backlog del Producto) , una lista priorizada de todas las funcionalidades, mejoras y correcciones necesarias para el proyecto. De este backlog se seleccionan los elementos m\u00e1s importantes que ser\u00e1n abordados en el sprint. Cada sprint comienza con una planificaci\u00f3n, donde se define el Sprint Backlog (Backlog del Sprint) , que es el conjunto espec\u00edfico de tareas y objetivos seleccionados del backlog del producto para ser desarrollados durante ese sprint. Luego, se sigue con la fase de desarrollo, revisi\u00f3n y retrospectiva para mejorar en el siguiente ciclo. Roles en Scrum Product Owner (Due\u00f1o del Producto) : Define los objetivos del producto, prioriza el backlog y se asegura de que el equipo entregue valor al negocio. Scrum Master : Facilita el proceso, elimina obst\u00e1culos y ayuda al equipo a seguir las pr\u00e1cticas \u00e1giles. Development Team (Equipo de Desarrollo) : Grupo autoorganizado y multidisciplinario que implementa las tareas definidas en el sprint. Eventos Claves en Scrum Sprint Planning : Se planifican los objetivos y tareas del sprint. Daily Scrum : Reuni\u00f3n diaria corta donde el equipo revisa avances y obst\u00e1culos. Sprint Review : Al final del sprint, se presenta el trabajo realizado. Sprint Retrospective : Se analizan mejoras para aplicar en el siguiente sprint. Scrum permite una entrega incremental de software funcional, promoviendo la transparencia, inspecci\u00f3n y adaptaci\u00f3n continua .","title":"Metodolog\u00eda de Trabajo"},{"location":"proyecto/01_metodologia/#desarrollo-agil-de-software","text":"El desarrollo \u00e1gil es un enfoque flexible e iterativo para la creaci\u00f3n de software que prioriza la colaboraci\u00f3n, la adaptaci\u00f3n a los cambios y la entrega de valor al cliente. Se basa en el Manifiesto \u00c1gil , que establece cuatro valores fundamentales: Individuos e interacciones sobre procesos y herramientas. Software funcionando sobre documentaci\u00f3n extensiva. Colaboraci\u00f3n con el cliente sobre negociaci\u00f3n de contratos. Respuesta ante el cambio sobre seguir un plan r\u00edgido. Adem\u00e1s, se apoya en 12 principios , entre los que destacan: - Entrega frecuente de software funcional. - Cooperaci\u00f3n entre equipos de desarrollo y negocio. - Capacidad de adaptaci\u00f3n a cambios en los requisitos del proyecto.","title":"Desarrollo \u00c1gil de Software"},{"location":"proyecto/01_metodologia/#scrum-un-marco-de-trabajo-agil","text":"Dentro de las metodolog\u00edas \u00e1giles, Scrum Es un marco ampliamente utilizado para la gesti\u00f3n de proyectos. Se basa en ciclos iterativos llamados sprints , que suelen durar entre una y cuatro semanas. Antes de cada sprint, el equipo trabaja con el Product Backlog (Backlog del Producto) , una lista priorizada de todas las funcionalidades, mejoras y correcciones necesarias para el proyecto. De este backlog se seleccionan los elementos m\u00e1s importantes que ser\u00e1n abordados en el sprint. Cada sprint comienza con una planificaci\u00f3n, donde se define el Sprint Backlog (Backlog del Sprint) , que es el conjunto espec\u00edfico de tareas y objetivos seleccionados del backlog del producto para ser desarrollados durante ese sprint. Luego, se sigue con la fase de desarrollo, revisi\u00f3n y retrospectiva para mejorar en el siguiente ciclo.","title":"Scrum: Un Marco de Trabajo \u00c1gil"},{"location":"proyecto/01_metodologia/#roles-en-scrum","text":"Product Owner (Due\u00f1o del Producto) : Define los objetivos del producto, prioriza el backlog y se asegura de que el equipo entregue valor al negocio. Scrum Master : Facilita el proceso, elimina obst\u00e1culos y ayuda al equipo a seguir las pr\u00e1cticas \u00e1giles. Development Team (Equipo de Desarrollo) : Grupo autoorganizado y multidisciplinario que implementa las tareas definidas en el sprint.","title":"Roles en Scrum"},{"location":"proyecto/01_metodologia/#eventos-claves-en-scrum","text":"Sprint Planning : Se planifican los objetivos y tareas del sprint. Daily Scrum : Reuni\u00f3n diaria corta donde el equipo revisa avances y obst\u00e1culos. Sprint Review : Al final del sprint, se presenta el trabajo realizado. Sprint Retrospective : Se analizan mejoras para aplicar en el siguiente sprint. Scrum permite una entrega incremental de software funcional, promoviendo la transparencia, inspecci\u00f3n y adaptaci\u00f3n continua .","title":"Eventos Claves en Scrum"},{"location":"proyecto/02_contribuir/","text":"Contribuir Workflow seleccionado \"Git Flow\" El workflow Git Flow , creado por Vincent Driessen en 2010, organiza el desarrollo del c\u00f3digo en dos ramas principales: master : Contiene el c\u00f3digo en producci\u00f3n. develop : Contiene el c\u00f3digo en pre-producci\u00f3n. Adem\u00e1s, utiliza ramas auxiliares para distintas etapas del desarrollo: feature *: Para nuevas funcionalidades (se crean desde develop y se fusionan en develop ). hotfix *: Para corregir errores cr\u00edticos en producci\u00f3n (se crean desde master y se fusionan en master y develop ). release *: Para preparar nuevas versiones de producci\u00f3n (se crean desde develop y se fusionan en master y develop ). \u00bfC\u00f3mo puedo contribuir? Nos emociona que est\u00e9s interesado en contribuir a nuestro proyecto. Esta gu\u00eda est\u00e1 dise\u00f1ada para ayudarte a entender c\u00f3mo puedes colaborar con nosotros, ya sea corrigiendo errores, mejorando la documentaci\u00f3n, agregando nuevas caracter\u00edsticas o cualquier otra forma de contribuci\u00f3n que desees realizar. Para incorporar una funci\u00f3n en la rama main , simplemente se crea un \"PR\" (Pull Request), que deber\u00e1 ser aprobado por alg\u00fan colaborador. Cualquier colaborador puede hacerlo, o bien, si no requiere revisi\u00f3n, puede ser aceptado por quien est\u00e9 incluyendo la funcionalidad. Es crucial que el nombre de las ramas creadas sea lo m\u00e1s descriptivo posible. Por ejemplo, si trabajamos en una nueva funcionalidad relacionada con la API, la rama se debe llamar como referencia a la funcionalidad en cuesti\u00f3n. En el caso de tratarse de la correcci\u00f3n de un error en el c\u00f3digo de la API, la llamaremos fix-api . Este proceso asegura un flujo de trabajo ordenado y facilita la colaboraci\u00f3n entre los miembros del equipo. Los pasos para contribuir en este proyecto como individuo son: Forkear el repositorio ( git fork ) Crear una nueva rama para la funci\u00f3n ( git checkout -b feature/AmazingFeature ) Publicar la rama en el repositorio remoto( git push --set-upstream origin <nombre-de-la-nueva-rama> ) Commit los cambios ( git commit -m 'Add some AmazingFeature' ) Push a la rama ( git push origin feature/AmazingFeature ) Abrir un Pull Request dirigido a la rama develop Commits Los commits convencionales nos permiten mantener la organizaci\u00f3n al realizar los commits y facilitan la creaci\u00f3n de releases de forma automatizada. Se basan en el uso de palabras clave al inicio del mensaje de cada commit, de la siguiente manera: feat(tema de la modificaci\u00f3n): Breve explicaci\u00f3n : Para cambios significativos o nuevas caracter\u00edsticas. fix(tema de la modificaci\u00f3n): Breve explicaci\u00f3n : Para correcciones peque\u00f1as. chore(tema de la modificaci\u00f3n): Breve explicaci\u00f3n : Para cambios menores insignificantes para el usuario. docs: Breve explicaci\u00f3n : Para cambios que se realizan a la documentaci\u00f3n.","title":"Como contribuir"},{"location":"proyecto/02_contribuir/#contribuir","text":"","title":"Contribuir"},{"location":"proyecto/02_contribuir/#workflow-seleccionado-git-flow","text":"El workflow Git Flow , creado por Vincent Driessen en 2010, organiza el desarrollo del c\u00f3digo en dos ramas principales: master : Contiene el c\u00f3digo en producci\u00f3n. develop : Contiene el c\u00f3digo en pre-producci\u00f3n. Adem\u00e1s, utiliza ramas auxiliares para distintas etapas del desarrollo: feature *: Para nuevas funcionalidades (se crean desde develop y se fusionan en develop ). hotfix *: Para corregir errores cr\u00edticos en producci\u00f3n (se crean desde master y se fusionan en master y develop ). release *: Para preparar nuevas versiones de producci\u00f3n (se crean desde develop y se fusionan en master y develop ).","title":"Workflow seleccionado \"Git Flow\""},{"location":"proyecto/02_contribuir/#como-puedo-contribuir","text":"Nos emociona que est\u00e9s interesado en contribuir a nuestro proyecto. Esta gu\u00eda est\u00e1 dise\u00f1ada para ayudarte a entender c\u00f3mo puedes colaborar con nosotros, ya sea corrigiendo errores, mejorando la documentaci\u00f3n, agregando nuevas caracter\u00edsticas o cualquier otra forma de contribuci\u00f3n que desees realizar. Para incorporar una funci\u00f3n en la rama main , simplemente se crea un \"PR\" (Pull Request), que deber\u00e1 ser aprobado por alg\u00fan colaborador. Cualquier colaborador puede hacerlo, o bien, si no requiere revisi\u00f3n, puede ser aceptado por quien est\u00e9 incluyendo la funcionalidad. Es crucial que el nombre de las ramas creadas sea lo m\u00e1s descriptivo posible. Por ejemplo, si trabajamos en una nueva funcionalidad relacionada con la API, la rama se debe llamar como referencia a la funcionalidad en cuesti\u00f3n. En el caso de tratarse de la correcci\u00f3n de un error en el c\u00f3digo de la API, la llamaremos fix-api . Este proceso asegura un flujo de trabajo ordenado y facilita la colaboraci\u00f3n entre los miembros del equipo. Los pasos para contribuir en este proyecto como individuo son: Forkear el repositorio ( git fork ) Crear una nueva rama para la funci\u00f3n ( git checkout -b feature/AmazingFeature ) Publicar la rama en el repositorio remoto( git push --set-upstream origin <nombre-de-la-nueva-rama> ) Commit los cambios ( git commit -m 'Add some AmazingFeature' ) Push a la rama ( git push origin feature/AmazingFeature ) Abrir un Pull Request dirigido a la rama develop","title":"\u00bfC\u00f3mo puedo contribuir?"},{"location":"proyecto/02_contribuir/#commits","text":"Los commits convencionales nos permiten mantener la organizaci\u00f3n al realizar los commits y facilitan la creaci\u00f3n de releases de forma automatizada. Se basan en el uso de palabras clave al inicio del mensaje de cada commit, de la siguiente manera: feat(tema de la modificaci\u00f3n): Breve explicaci\u00f3n : Para cambios significativos o nuevas caracter\u00edsticas. fix(tema de la modificaci\u00f3n): Breve explicaci\u00f3n : Para correcciones peque\u00f1as. chore(tema de la modificaci\u00f3n): Breve explicaci\u00f3n : Para cambios menores insignificantes para el usuario. docs: Breve explicaci\u00f3n : Para cambios que se realizan a la documentaci\u00f3n.","title":"Commits"},{"location":"proyecto/03_about/","text":"Acerca del Proyecto Esta PPS (Pr\u00e1ctica Profesional Supervisada) ha sido desarrollado en el marco de una asignatura de la Carrera de Ingenier\u00eda en Telecomunicaciones de la Universidad Nacional de R\u00edo Cuarto , con el objetivo de aplicar y profundizar conocimientos adquiridos a lo largo de la formaci\u00f3n. \ud83d\udc68\u200d\ud83c\udfeb Profesor Responsable Nombre: Ing. Daniel Anunziata Correo institucional: danunziata@ing.unrc.edu.ar \ud83d\udc68\u200d\ud83c\udf93 Alumno Responsable Nombre: Kevin Haponiuk Carrera: Ingenier\u00eda en Telecomunicaciones Correo: kevinhapo@gmail.com \ud83e\uddea Grupo / Laboratorio Grupo de Investigaci\u00f3n en Ciencia de Datos (GCID) El GCID se sit\u00faa en la vanguardia de la investigaci\u00f3n y la innovaci\u00f3n en el campo de la Ciencia de Datos, abordando m\u00e9todos cient\u00edficos, procesos y sistemas dise\u00f1ados para extraer conocimiento de datos estructurados y no estructurados. Su enfoque interdisciplinario incluye matem\u00e1ticas, estad\u00edstica, aprendizaje autom\u00e1tico y ciencia de la informaci\u00f3n. Adem\u00e1s, se encuentra alineado con los avances de la Industria 4.0, aprovechando tecnolog\u00edas avanzadas para el an\u00e1lisis y la gesti\u00f3n de datos en tiempo real. Sitio web: https://www.ing.unrc.edu.ar/grupos/gidat/ \ud83c\udf10 Colaboran en este proyecto","title":"Acerca de"},{"location":"proyecto/03_about/#acerca-del-proyecto","text":"Esta PPS (Pr\u00e1ctica Profesional Supervisada) ha sido desarrollado en el marco de una asignatura de la Carrera de Ingenier\u00eda en Telecomunicaciones de la Universidad Nacional de R\u00edo Cuarto , con el objetivo de aplicar y profundizar conocimientos adquiridos a lo largo de la formaci\u00f3n.","title":"Acerca del Proyecto"},{"location":"proyecto/03_about/#profesor-responsable","text":"Nombre: Ing. Daniel Anunziata Correo institucional: danunziata@ing.unrc.edu.ar","title":"\ud83d\udc68\u200d\ud83c\udfeb Profesor Responsable"},{"location":"proyecto/03_about/#alumno-responsable","text":"Nombre: Kevin Haponiuk Carrera: Ingenier\u00eda en Telecomunicaciones Correo: kevinhapo@gmail.com","title":"\ud83d\udc68\u200d\ud83c\udf93 Alumno Responsable"},{"location":"proyecto/03_about/#grupo-laboratorio","text":"Grupo de Investigaci\u00f3n en Ciencia de Datos (GCID) El GCID se sit\u00faa en la vanguardia de la investigaci\u00f3n y la innovaci\u00f3n en el campo de la Ciencia de Datos, abordando m\u00e9todos cient\u00edficos, procesos y sistemas dise\u00f1ados para extraer conocimiento de datos estructurados y no estructurados. Su enfoque interdisciplinario incluye matem\u00e1ticas, estad\u00edstica, aprendizaje autom\u00e1tico y ciencia de la informaci\u00f3n. Adem\u00e1s, se encuentra alineado con los avances de la Industria 4.0, aprovechando tecnolog\u00edas avanzadas para el an\u00e1lisis y la gesti\u00f3n de datos en tiempo real. Sitio web: https://www.ing.unrc.edu.ar/grupos/gidat/","title":"\ud83e\uddea Grupo / Laboratorio"},{"location":"proyecto/03_about/#colaboran-en-este-proyecto","text":"","title":"\ud83c\udf10 Colaboran en este proyecto"},{"location":"pruebas/01_install-n8n/","text":"Instalaci\u00f3n de n8n con Self-hosted AI Starter Kit Figura 1: Self-hosted AI Starter Kit Para instalar n8n en un entorno local, utilizaremos el Self-hosted AI Starter Kit , una plantilla open-source basada en Docker Compose que permite desplegar r\u00e1pidamente un entorno completo de desarrollo low-code e inteligencia artificial. \u00bfQu\u00e9 es el Self-hosted AI Starter Kit? El Self-hosted AI Starter Kit es un conjunto de herramientas preconfiguradas que facilita la instalaci\u00f3n y puesta en marcha de n8n junto a otros servicios \u00fatiles para proyectos de automatizaci\u00f3n e IA. Incluye: n8n : Plataforma low-code con m\u00e1s de 400 integraciones y componentes avanzados de IA. Ollama : Plataforma para ejecutar modelos de lenguaje (LLM) localmente. Qdrant : Vector store de alto rendimiento, ideal para b\u00fasquedas sem\u00e1nticas y almacenamiento de embeddings. PostgreSQL : Base de datos robusta y ampliamente utilizada en ingenier\u00eda de datos. Pasos de instalaci\u00f3n 1. Clonar el repositorio git clone https://github.com/n8n-io/self-hosted-ai-starter-kit.git cd self-hosted-ai-starter-kit 2. Ejecutar con Docker Compose Dependiendo de tu hardware, elige la opci\u00f3n adecuada: a) Usuarios con GPU Nvidia docker compose --profile gpu-nvidia up b) Usuarios con GPU AMD (Linux) docker compose --profile gpu-amd up c) Usuarios de Mac / Apple Silicon Nota: En Mac con procesadores M1 o superiores, no es posible exponer la GPU a Docker. Hay dos alternativas: - Ejecutar todo el entorno en CPU (ver siguiente secci\u00f3n). - Instalar Ollama directamente en tu Mac para inferencia m\u00e1s r\u00e1pida y conectar n8n a ese servicio. Para instalar Ollama en Mac, consulta la p\u00e1gina oficial de Ollama . Para levantar el entorno en CPU: docker compose up d) Para todos los dem\u00e1s (CPU) docker compose --profile cpu up 3. Visualizaci\u00f3n mediante docker desktop Se utilizar\u00e1 docker desktop para la visualizaci\u00f3n y ejecuci\u00f3n del entorno. Figura 2: Visualizaci\u00f3n de docker desktop donde podremos observar los contenedores que se est\u00e1n ejecutando. Figura 3: Visualizaci\u00f3n de contenedores Una vez que los contenedores est\u00e9n en ejecuci\u00f3n, podr\u00e1s acceder a la interfaz web de n8n a trav\u00e9s de tu navegador utilizando la siguiente URL: http://localhost:5678 Nota : Aseg\u00farate de que el puerto 5678 no est\u00e9 siendo utilizado por otra aplicaci\u00f3n en tu sistema. Una vez que inicies sesi\u00f3n, podr\u00e1s acceder a la interfaz principal de n8n. En la imagen se muestra un ejemplo de un flujo de trabajo configurado. Si has llegado a este punto y puedes ver la interfaz, significa que n8n est\u00e1 correctamente instalado y listo para comenzar a crear tus propios flujos de trabajo. Figura 4: Interfaz de n8n Instalaciones / Dependencias necesarias Instalaci\u00f3n de modelos en Ollama Primero, es necesario instalar las dependencias necesarias para poder ejecutar los ejemplos. vamos a usar en ollama un modelo de embeddings que se llama nomic-embed-text y un modelo de lenguaje que se llama llama3.1:8b . para instalarlos, debemos buscarlos en la p\u00e1gina de ollama. Repositorio de nomic-embed-text de esta p\u00e1gina, podemos seleccionar la versi\u00f3n que queramos instalar. ollama pull nomic-embed-text con este comando vamos a ir al docker desktop y vamos a pegarlo en esta secci\u00f3n \"Exec\". Figura 5: Instalaci\u00f3n de nomic-embed-text de manera similar, podemos instalar el modelo de lenguaje llama3.1:8b Repositorio de llama3.1:8b ollama pull llama3.1:8b Credenciales de Google Para vincular servicios de Google con n8n, necesitamos seguir estos pasos: Primero, necesitamos crear un proyecto en Google Cloud Console y habilitar las APIs que queremos usar: Ve a Google Cloud Console Crea un nuevo proyecto o selecciona uno existente En el men\u00fa lateral, ve a \"APIs y servicios\" > \"Biblioteca\" Busca y habilita las APIs que necesites (por ejemplo, \"Google Drive API\", \"Google Docs API\") Crear credenciales OAuth 2.0: En el men\u00fa lateral, ve a \"APIs y servicios\" > \"Credenciales\" Haz clic en \"Crear credenciales\" > \"ID de cliente de OAuth\" Selecciona \"Aplicaci\u00f3n web\" como tipo de aplicaci\u00f3n Agrega los URIs de redirecci\u00f3n autorizados: http://localhost:5678/rest/oauth2-credential/callback Guarda el ID de cliente y el secreto del cliente En n8n: Para configurar las credenciales de Google, simplemente abre el nodo que necesitas conectar (por ejemplo, si es Google Drive, arrastra el nodo \"Google Drive\" a tu flujo de trabajo). Luego, haz clic en el nodo y ve a la pesta\u00f1a \"Credenciales\". Aqu\u00ed ver\u00e1s los campos que necesitas completar: OAuth Redirect URL: http://localhost:5678/rest/oauth2-credential/callback Client ID: El ID de cliente que obtuviste de Google Cloud Console Client Secret: El secreto del cliente que obtuviste de Google Cloud Console Una vez que ingreses estos datos, n8n manejar\u00e1 autom\u00e1ticamente la configuraci\u00f3n OAuth2 necesaria para la autenticaci\u00f3n. Una vez configuradas las credenciales, podr\u00e1s usarlas en tus flujos de trabajo de n8n para interactuar con los servicios de Google. Figura 6: Credenciales de Google Nota : Aseg\u00farate de mantener seguras tus credenciales y no compartirlas p\u00fablicamente. Tambi\u00e9n es importante configurar correctamente los URIs de redirecci\u00f3n para que la autenticaci\u00f3n funcione correctamente. Referencias Repositorio oficial de Self-hosted AI Starter Kit Repositorio oficial de Ollama Repositorio de nomic-embed-text Repositorio de llama3.1:8b","title":"Instalaci\u00f3n de n8n / Dependencias necesarias"},{"location":"pruebas/01_install-n8n/#instalacion-de-n8n-con-self-hosted-ai-starter-kit","text":"Figura 1: Self-hosted AI Starter Kit Para instalar n8n en un entorno local, utilizaremos el Self-hosted AI Starter Kit , una plantilla open-source basada en Docker Compose que permite desplegar r\u00e1pidamente un entorno completo de desarrollo low-code e inteligencia artificial.","title":"Instalaci\u00f3n de n8n con Self-hosted AI Starter Kit"},{"location":"pruebas/01_install-n8n/#que-es-el-self-hosted-ai-starter-kit","text":"El Self-hosted AI Starter Kit es un conjunto de herramientas preconfiguradas que facilita la instalaci\u00f3n y puesta en marcha de n8n junto a otros servicios \u00fatiles para proyectos de automatizaci\u00f3n e IA. Incluye: n8n : Plataforma low-code con m\u00e1s de 400 integraciones y componentes avanzados de IA. Ollama : Plataforma para ejecutar modelos de lenguaje (LLM) localmente. Qdrant : Vector store de alto rendimiento, ideal para b\u00fasquedas sem\u00e1nticas y almacenamiento de embeddings. PostgreSQL : Base de datos robusta y ampliamente utilizada en ingenier\u00eda de datos.","title":"\u00bfQu\u00e9 es el Self-hosted AI Starter Kit?"},{"location":"pruebas/01_install-n8n/#pasos-de-instalacion","text":"","title":"Pasos de instalaci\u00f3n"},{"location":"pruebas/01_install-n8n/#1-clonar-el-repositorio","text":"git clone https://github.com/n8n-io/self-hosted-ai-starter-kit.git cd self-hosted-ai-starter-kit","title":"1. Clonar el repositorio"},{"location":"pruebas/01_install-n8n/#2-ejecutar-con-docker-compose","text":"Dependiendo de tu hardware, elige la opci\u00f3n adecuada:","title":"2. Ejecutar con Docker Compose"},{"location":"pruebas/01_install-n8n/#a-usuarios-con-gpu-nvidia","text":"docker compose --profile gpu-nvidia up","title":"a) Usuarios con GPU Nvidia"},{"location":"pruebas/01_install-n8n/#b-usuarios-con-gpu-amd-linux","text":"docker compose --profile gpu-amd up","title":"b) Usuarios con GPU AMD (Linux)"},{"location":"pruebas/01_install-n8n/#c-usuarios-de-mac-apple-silicon","text":"Nota: En Mac con procesadores M1 o superiores, no es posible exponer la GPU a Docker. Hay dos alternativas: - Ejecutar todo el entorno en CPU (ver siguiente secci\u00f3n). - Instalar Ollama directamente en tu Mac para inferencia m\u00e1s r\u00e1pida y conectar n8n a ese servicio. Para instalar Ollama en Mac, consulta la p\u00e1gina oficial de Ollama . Para levantar el entorno en CPU: docker compose up","title":"c) Usuarios de Mac / Apple Silicon"},{"location":"pruebas/01_install-n8n/#d-para-todos-los-demas-cpu","text":"docker compose --profile cpu up","title":"d) Para todos los dem\u00e1s (CPU)"},{"location":"pruebas/01_install-n8n/#3-visualizacion-mediante-docker-desktop","text":"Se utilizar\u00e1 docker desktop para la visualizaci\u00f3n y ejecuci\u00f3n del entorno. Figura 2: Visualizaci\u00f3n de docker desktop donde podremos observar los contenedores que se est\u00e1n ejecutando. Figura 3: Visualizaci\u00f3n de contenedores Una vez que los contenedores est\u00e9n en ejecuci\u00f3n, podr\u00e1s acceder a la interfaz web de n8n a trav\u00e9s de tu navegador utilizando la siguiente URL: http://localhost:5678 Nota : Aseg\u00farate de que el puerto 5678 no est\u00e9 siendo utilizado por otra aplicaci\u00f3n en tu sistema. Una vez que inicies sesi\u00f3n, podr\u00e1s acceder a la interfaz principal de n8n. En la imagen se muestra un ejemplo de un flujo de trabajo configurado. Si has llegado a este punto y puedes ver la interfaz, significa que n8n est\u00e1 correctamente instalado y listo para comenzar a crear tus propios flujos de trabajo. Figura 4: Interfaz de n8n","title":"3. Visualizaci\u00f3n mediante docker desktop"},{"location":"pruebas/01_install-n8n/#instalaciones-dependencias-necesarias","text":"","title":"Instalaciones / Dependencias necesarias"},{"location":"pruebas/01_install-n8n/#instalacion-de-modelos-en-ollama","text":"Primero, es necesario instalar las dependencias necesarias para poder ejecutar los ejemplos. vamos a usar en ollama un modelo de embeddings que se llama nomic-embed-text y un modelo de lenguaje que se llama llama3.1:8b . para instalarlos, debemos buscarlos en la p\u00e1gina de ollama. Repositorio de nomic-embed-text de esta p\u00e1gina, podemos seleccionar la versi\u00f3n que queramos instalar. ollama pull nomic-embed-text con este comando vamos a ir al docker desktop y vamos a pegarlo en esta secci\u00f3n \"Exec\". Figura 5: Instalaci\u00f3n de nomic-embed-text de manera similar, podemos instalar el modelo de lenguaje llama3.1:8b Repositorio de llama3.1:8b ollama pull llama3.1:8b","title":"Instalaci\u00f3n de modelos en Ollama"},{"location":"pruebas/01_install-n8n/#credenciales-de-google","text":"Para vincular servicios de Google con n8n, necesitamos seguir estos pasos: Primero, necesitamos crear un proyecto en Google Cloud Console y habilitar las APIs que queremos usar: Ve a Google Cloud Console Crea un nuevo proyecto o selecciona uno existente En el men\u00fa lateral, ve a \"APIs y servicios\" > \"Biblioteca\" Busca y habilita las APIs que necesites (por ejemplo, \"Google Drive API\", \"Google Docs API\") Crear credenciales OAuth 2.0: En el men\u00fa lateral, ve a \"APIs y servicios\" > \"Credenciales\" Haz clic en \"Crear credenciales\" > \"ID de cliente de OAuth\" Selecciona \"Aplicaci\u00f3n web\" como tipo de aplicaci\u00f3n Agrega los URIs de redirecci\u00f3n autorizados: http://localhost:5678/rest/oauth2-credential/callback Guarda el ID de cliente y el secreto del cliente En n8n: Para configurar las credenciales de Google, simplemente abre el nodo que necesitas conectar (por ejemplo, si es Google Drive, arrastra el nodo \"Google Drive\" a tu flujo de trabajo). Luego, haz clic en el nodo y ve a la pesta\u00f1a \"Credenciales\". Aqu\u00ed ver\u00e1s los campos que necesitas completar: OAuth Redirect URL: http://localhost:5678/rest/oauth2-credential/callback Client ID: El ID de cliente que obtuviste de Google Cloud Console Client Secret: El secreto del cliente que obtuviste de Google Cloud Console Una vez que ingreses estos datos, n8n manejar\u00e1 autom\u00e1ticamente la configuraci\u00f3n OAuth2 necesaria para la autenticaci\u00f3n. Una vez configuradas las credenciales, podr\u00e1s usarlas en tus flujos de trabajo de n8n para interactuar con los servicios de Google. Figura 6: Credenciales de Google Nota : Aseg\u00farate de mantener seguras tus credenciales y no compartirlas p\u00fablicamente. Tambi\u00e9n es importante configurar correctamente los URIs de redirecci\u00f3n para que la autenticaci\u00f3n funcione correctamente.","title":"Credenciales de Google"},{"location":"pruebas/01_install-n8n/#referencias","text":"Repositorio oficial de Self-hosted AI Starter Kit Repositorio oficial de Ollama Repositorio de nomic-embed-text Repositorio de llama3.1:8b","title":"Referencias"},{"location":"pruebas/02_ejemplo-rag-crag/","text":"En esta secci\u00f3n se presentar\u00e1n dos ejemplos pr\u00e1cticos de implementaci\u00f3n de las arquitecturas RAG y CRAG que fueron explicadas en detalle en la secci\u00f3n de Tipos de Arquitecturas . Estos ejemplos demostrar\u00e1n c\u00f3mo aplicar los conceptos te\u00f3ricos en casos reales. Para comenzar con los ejemplos, necesitamos importar el flujo de trabajo de ejemplo en n8n. El archivo ejemplo_RAG_vs_CRAG.json contiene la configuraci\u00f3n completa de ambos ejemplos. Para importarlo: En la interfaz de n8n, haz clic en el bot\u00f3n \"Workflows\" en el men\u00fa lateral Haz clic en el bot\u00f3n \"Import from File\" en la esquina superior derecha Navega hasta la carpeta docs/implementacion/files y selecciona el archivo ejemplo_RAG_vs_CRAG.json Haz clic en \"Import\" Una vez importado, deber\u00e1s ver algo como lo siguiente: Figura 1: Ejemplo de RAG y CRAG en n8n La implementaci\u00f3n se divide en tres bloques principales: Bloque RAG (Amarillo) : Implementa el flujo de trabajo del RAG tradicional Bloque CRAG (Rojo) : Implementa el flujo de trabajo del CRAG Bloque de Consultas (Azul) : Permite realizar consultas a la base de datos vectorial Configuraci\u00f3n de las Colecciones Cada bloque de base de datos vectorial debe configurarse con su colecci\u00f3n correspondiente: El bloque RAG se conecta a la colecci\u00f3n prueba_RAG El bloque CRAG se conecta a la colecci\u00f3n prueba_CRAG El bloque de consultas permite seleccionar la colecci\u00f3n a consultar Flujo de Trabajo RAG El flujo de trabajo del RAG sigue estos pasos: Descarga un archivo XML desde Google Drive (extra\u00eddo previamente de un paper mediante GROBID) Procesa el archivo para extraer el texto contenido Carga el contenido en la base de datos vectorial con una configuraci\u00f3n recursiva que permite que cada chunk contenga informaci\u00f3n de contexto de los chunks adyacentes Flujo de Trabajo CRAG El flujo de trabajo del CRAG implementa una estrategia m\u00e1s sofisticada: Realiza la misma descarga y extracci\u00f3n inicial que el RAG Divide manualmente el contenido en chunks Procesa los chunks en grupos de 5 mediante un loop Para cada grupo, utiliza un modelo de lenguaje para generar contexto individual basado en el documento completo Carga los chunks enriquecidos en la base de datos vectorial Nota: No se utiliza la configuraci\u00f3n recursiva ya que la separaci\u00f3n y enriquecimiento de chunks se realiza previamente Ejecuci\u00f3n del flujo de trabajo para la ejecuci\u00f3n debemos agregar el bloque when clicking 'test workflow' y conectarlo con el bloque de google drive para testear el flujo de trabajo que queremos ejecutar, ya sea RAG o CRAG. Figura 2: Ejecuci\u00f3n del flujo de trabajo RAG Una vez ejecutado el flujo de trabajo, se puede observar el proceso completo en acci\u00f3n: Para verificar que los datos se han cargado correctamente, se puede acceder al panel de control de Qdrant a trav\u00e9s de: http://localhost:6333/dashboard#/collections Aqu\u00ed se puede visualizar las colecciones creadas y verificar que los vectores se hayan almacenado correctamente. Figura 3: Colecciones en Qdrant una vez llegado a este punto, est\u00e1 preparada la base de datos para que el agente pueda preguntar algo relacionado con el contenido que se carg\u00f3. Figura 4: Pregunta al agente","title":"Ejemplo RAG / CRAG"},{"location":"pruebas/02_ejemplo-rag-crag/#configuracion-de-las-colecciones","text":"Cada bloque de base de datos vectorial debe configurarse con su colecci\u00f3n correspondiente: El bloque RAG se conecta a la colecci\u00f3n prueba_RAG El bloque CRAG se conecta a la colecci\u00f3n prueba_CRAG El bloque de consultas permite seleccionar la colecci\u00f3n a consultar","title":"Configuraci\u00f3n de las Colecciones"},{"location":"pruebas/02_ejemplo-rag-crag/#flujo-de-trabajo-rag","text":"El flujo de trabajo del RAG sigue estos pasos: Descarga un archivo XML desde Google Drive (extra\u00eddo previamente de un paper mediante GROBID) Procesa el archivo para extraer el texto contenido Carga el contenido en la base de datos vectorial con una configuraci\u00f3n recursiva que permite que cada chunk contenga informaci\u00f3n de contexto de los chunks adyacentes","title":"Flujo de Trabajo RAG"},{"location":"pruebas/02_ejemplo-rag-crag/#flujo-de-trabajo-crag","text":"El flujo de trabajo del CRAG implementa una estrategia m\u00e1s sofisticada: Realiza la misma descarga y extracci\u00f3n inicial que el RAG Divide manualmente el contenido en chunks Procesa los chunks en grupos de 5 mediante un loop Para cada grupo, utiliza un modelo de lenguaje para generar contexto individual basado en el documento completo Carga los chunks enriquecidos en la base de datos vectorial Nota: No se utiliza la configuraci\u00f3n recursiva ya que la separaci\u00f3n y enriquecimiento de chunks se realiza previamente","title":"Flujo de Trabajo CRAG"},{"location":"pruebas/02_ejemplo-rag-crag/#ejecucion-del-flujo-de-trabajo","text":"para la ejecuci\u00f3n debemos agregar el bloque when clicking 'test workflow' y conectarlo con el bloque de google drive para testear el flujo de trabajo que queremos ejecutar, ya sea RAG o CRAG. Figura 2: Ejecuci\u00f3n del flujo de trabajo RAG Una vez ejecutado el flujo de trabajo, se puede observar el proceso completo en acci\u00f3n: Para verificar que los datos se han cargado correctamente, se puede acceder al panel de control de Qdrant a trav\u00e9s de: http://localhost:6333/dashboard#/collections Aqu\u00ed se puede visualizar las colecciones creadas y verificar que los vectores se hayan almacenado correctamente. Figura 3: Colecciones en Qdrant una vez llegado a este punto, est\u00e1 preparada la base de datos para que el agente pueda preguntar algo relacionado con el contenido que se carg\u00f3. Figura 4: Pregunta al agente","title":"Ejecuci\u00f3n del flujo de trabajo"},{"location":"pruebas/03_ejemplo-mcp/","text":"Ejemplo de Integraci\u00f3n MCP en n8n Esta secci\u00f3n describe el proceso para instalar y utilizar los nodos MCP (Model Context Protocol) en n8n, utilizando como ejemplo la conexi\u00f3n con el servicio Brave Search. 1. Instalaci\u00f3n de los nodos MCP Para instalar los nodos MCP desarrollados por la comunidad, se deben seguir los siguientes pasos: Acceder a Ajustes en n8n. Seleccionar Community Nodes . Buscar \"n8n-nodes-mcp\" y hacer clic en Install . Figura 1: Instalaci\u00f3n del nodo MCP en n8n Una vez instalado, el nodo MCP estar\u00e1 disponible para ser agregado a los flujos de trabajo. Figura 2: Nodo MCP disponible en n8n Nota: Si el nodo no aparece, se recomienda refrescar la p\u00e1gina. 2. Configuraci\u00f3n de credenciales MCP Para conectar el nodo MCP con un servicio, es necesario agregar las credenciales correspondientes. La lista de servicios compatibles se encuentra disponible en: https://github.com/modelcontextprotocol/servers En este ejemplo, se utilizar\u00e1 Brave Search . Para ello, se debe localizar el servicio en la lista: https://github.com/modelcontextprotocol/servers/tree/main/src/brave-search La informaci\u00f3n obtenida debe ser ingresada en el bloque de configuraci\u00f3n del cliente MCP: Figura 3: Configuraci\u00f3n del cliente MCP con credenciales de Brave Posteriormente, estos datos deben ser cargados en las credenciales del nodo MCP Client en n8n, como se puede observar, necesitamos agregar la API KEY tambi\u00e9n: Figura 4: Ingreso de credenciales en n8n para MCP Client la cual puede obtenerse registr\u00e1ndose en: https://api-dashboard.search.brave.com/app/keys Figura 5: Panel de Brave Search para obtener la API Key 3. Pruebas de conexi\u00f3n y uso de herramientas Para verificar las herramientas disponibles, se puede realizar una consulta desde el nodo MCP: Figura 6: Consulta de herramientas disponibles en MCP De igual manera, es posible probar la herramienta de b\u00fasqueda de Brave de forma individual: Figura 7: Ejecuci\u00f3n de la herramienta de b\u00fasqueda de Brave Referencias Video tutorial: Integraci\u00f3n MCP en n8n (YouTube)","title":"Ejemplo MCP"},{"location":"pruebas/03_ejemplo-mcp/#ejemplo-de-integracion-mcp-en-n8n","text":"Esta secci\u00f3n describe el proceso para instalar y utilizar los nodos MCP (Model Context Protocol) en n8n, utilizando como ejemplo la conexi\u00f3n con el servicio Brave Search.","title":"Ejemplo de Integraci\u00f3n MCP en n8n"},{"location":"pruebas/03_ejemplo-mcp/#1-instalacion-de-los-nodos-mcp","text":"Para instalar los nodos MCP desarrollados por la comunidad, se deben seguir los siguientes pasos: Acceder a Ajustes en n8n. Seleccionar Community Nodes . Buscar \"n8n-nodes-mcp\" y hacer clic en Install . Figura 1: Instalaci\u00f3n del nodo MCP en n8n Una vez instalado, el nodo MCP estar\u00e1 disponible para ser agregado a los flujos de trabajo. Figura 2: Nodo MCP disponible en n8n Nota: Si el nodo no aparece, se recomienda refrescar la p\u00e1gina.","title":"1. Instalaci\u00f3n de los nodos MCP"},{"location":"pruebas/03_ejemplo-mcp/#2-configuracion-de-credenciales-mcp","text":"Para conectar el nodo MCP con un servicio, es necesario agregar las credenciales correspondientes. La lista de servicios compatibles se encuentra disponible en: https://github.com/modelcontextprotocol/servers En este ejemplo, se utilizar\u00e1 Brave Search . Para ello, se debe localizar el servicio en la lista: https://github.com/modelcontextprotocol/servers/tree/main/src/brave-search La informaci\u00f3n obtenida debe ser ingresada en el bloque de configuraci\u00f3n del cliente MCP: Figura 3: Configuraci\u00f3n del cliente MCP con credenciales de Brave Posteriormente, estos datos deben ser cargados en las credenciales del nodo MCP Client en n8n, como se puede observar, necesitamos agregar la API KEY tambi\u00e9n: Figura 4: Ingreso de credenciales en n8n para MCP Client la cual puede obtenerse registr\u00e1ndose en: https://api-dashboard.search.brave.com/app/keys Figura 5: Panel de Brave Search para obtener la API Key","title":"2. Configuraci\u00f3n de credenciales MCP"},{"location":"pruebas/03_ejemplo-mcp/#3-pruebas-de-conexion-y-uso-de-herramientas","text":"Para verificar las herramientas disponibles, se puede realizar una consulta desde el nodo MCP: Figura 6: Consulta de herramientas disponibles en MCP De igual manera, es posible probar la herramienta de b\u00fasqueda de Brave de forma individual: Figura 7: Ejecuci\u00f3n de la herramienta de b\u00fasqueda de Brave","title":"3. Pruebas de conexi\u00f3n y uso de herramientas"},{"location":"pruebas/03_ejemplo-mcp/#referencias","text":"Video tutorial: Integraci\u00f3n MCP en n8n (YouTube)","title":"Referencias"},{"location":"pruebas/05_install-grobid/","text":"Instalaci\u00f3n y Ejecuci\u00f3n de GROBID La forma m\u00e1s recomendada de utilizar GROBID es a trav\u00e9s de Docker , ejecut\u00e1ndolo como un servidor local. Existen dos opciones de im\u00e1genes disponibles: Imagen completa Esta imagen ofrece la mejor precisi\u00f3n, ya que incluye todas las bibliotecas necesarias de Python, TensorFlow, soporte para GPU y todos los modelos de Deep Learning. Es ideal para quienes disponen de una computadora con buena capacidad (preferentemente con GPU) y trabajan con una cantidad limitada de archivos PDF. Requiere m\u00e1s recursos de hardware. Para ejecutarla, se debe utilizar el siguiente comando: docker run --name grobid --gpus all --init --ulimit core = 0 -p 8070 :8070 -d grobid/grobid:0.8.2 Nota importante: Durante las pruebas, se intent\u00f3 utilizar la imagen liviana ( lfoppiano/grobid:0.8.1 ), pero no estaba disponible en el momento de la ejecuci\u00f3n. Por este motivo, se opt\u00f3 por utilizar la imagen completa ( grobid/grobid:0.8.1 ), que ofrece una mejor precisi\u00f3n y rendimiento en la extracci\u00f3n de datos de los documentos PDF. Una vez que GROBID est\u00e1 en ejecuci\u00f3n, se puede verificar su funcionamiento accediendo, desde un navegador web, a: http://localhost:8070 All\u00ed se encuentra la p\u00e1gina de bienvenida del servidor, desde donde es posible utilizar los servicios de procesamiento de documentos de manera manual. Sin embargo, para este proyecto, el procesamiento se realiza de forma automatizada mediante clientes Python, como se explica a continuaci\u00f3n. Instalaci\u00f3n del cliente Python de GROBID Para automatizar el procesamiento de documentos con GROBID desde Python, es necesario instalar el cliente oficial. Este cliente permite interactuar f\u00e1cilmente con el servidor GROBID desde scripts Python. python3 -m pip install grobid-client-python Esto instalar\u00e1 la biblioteca grobid-client y sus dependencias. Si necesitas instalarlo en un entorno espec\u00edfico (por ejemplo, dentro de un contenedor Docker), aseg\u00farate de ejecutar el comando en ese entorno. Estructura del Entorno de Trabajo En el flujo de trabajo propuesto, se define una estructura simple con los siguientes elementos clave: \ud83d\udcc1 input_papers: Carpeta que contiene los archivos PDF a procesar. \ud83d\udcc1 output_papers: Carpeta donde se guardar\u00e1n los archivos XML generados por GROBID. \ud83d\udcc4 config.json: Archivo de configuraci\u00f3n del cliente GROBID. \ud83d\udcc4 cliente_grobid.py: Script Python para ejecutar la conversi\u00f3n masiva. \ud83d\udcc4 config.json Este archivo es necesario para establecer la configuraci\u00f3n del cliente. En este caso, no requiere modificaci\u00f3n: { \"grobid_server\": \"http://localhost:8070\", \"batch_size\": 100, \"sleep_time\": 5, \"timeout\": 60, \"coordinates\": [ \"persName\", \"figure\", \"ref\", \"biblStruct\", \"formula\", \"s\", \"note\", \"title\" ] } \ud83d\udcc4 cliente_grobid.py Este script ejecuta el cliente GROBID para transformar todos los archivos .pdf en la carpeta input_papers y guardar los .xml resultantes en output_papers . from grobid_client.grobid_client import GrobidClient if __name__ == \"__main__\" : client = GrobidClient ( config_path = \"./config.json\" ) client . process ( \"processFulltextDocument\" , \"input_papers\" , output = \"output_papers/\" , consolidate_citations = False , tei_coordinates = False , force = True ) El script utiliza la biblioteca grobid-client para interactuar con el servidor GROBID. La funci\u00f3n process se encarga de procesar los documentos PDF en la carpeta input_papers , generando archivos XML que se guardan en la carpeta output_papers . Los par\u00e1metros adicionales permiten ajustar el comportamiento del procesamiento, como la consolidaci\u00f3n de citas y las coordenadas TEI. Para ejecutar el script, se debe usar espec\u00edficamente Python 3 con el comando: python3 cliente_grobid . py Cada pdf se pasar\u00e1 a XML: Referencia GROBID GitHub GROBID Documentaci\u00f3n GROBID Docker","title":"Instalaci\u00f3n de GROBID / Ejemplo de uso"},{"location":"pruebas/05_install-grobid/#instalacion-y-ejecucion-de-grobid","text":"La forma m\u00e1s recomendada de utilizar GROBID es a trav\u00e9s de Docker , ejecut\u00e1ndolo como un servidor local. Existen dos opciones de im\u00e1genes disponibles: Imagen completa Esta imagen ofrece la mejor precisi\u00f3n, ya que incluye todas las bibliotecas necesarias de Python, TensorFlow, soporte para GPU y todos los modelos de Deep Learning. Es ideal para quienes disponen de una computadora con buena capacidad (preferentemente con GPU) y trabajan con una cantidad limitada de archivos PDF. Requiere m\u00e1s recursos de hardware. Para ejecutarla, se debe utilizar el siguiente comando: docker run --name grobid --gpus all --init --ulimit core = 0 -p 8070 :8070 -d grobid/grobid:0.8.2 Nota importante: Durante las pruebas, se intent\u00f3 utilizar la imagen liviana ( lfoppiano/grobid:0.8.1 ), pero no estaba disponible en el momento de la ejecuci\u00f3n. Por este motivo, se opt\u00f3 por utilizar la imagen completa ( grobid/grobid:0.8.1 ), que ofrece una mejor precisi\u00f3n y rendimiento en la extracci\u00f3n de datos de los documentos PDF. Una vez que GROBID est\u00e1 en ejecuci\u00f3n, se puede verificar su funcionamiento accediendo, desde un navegador web, a: http://localhost:8070 All\u00ed se encuentra la p\u00e1gina de bienvenida del servidor, desde donde es posible utilizar los servicios de procesamiento de documentos de manera manual. Sin embargo, para este proyecto, el procesamiento se realiza de forma automatizada mediante clientes Python, como se explica a continuaci\u00f3n.","title":"Instalaci\u00f3n y Ejecuci\u00f3n de GROBID"},{"location":"pruebas/05_install-grobid/#instalacion-del-cliente-python-de-grobid","text":"Para automatizar el procesamiento de documentos con GROBID desde Python, es necesario instalar el cliente oficial. Este cliente permite interactuar f\u00e1cilmente con el servidor GROBID desde scripts Python. python3 -m pip install grobid-client-python Esto instalar\u00e1 la biblioteca grobid-client y sus dependencias. Si necesitas instalarlo en un entorno espec\u00edfico (por ejemplo, dentro de un contenedor Docker), aseg\u00farate de ejecutar el comando en ese entorno.","title":"Instalaci\u00f3n del cliente Python de GROBID"},{"location":"pruebas/05_install-grobid/#estructura-del-entorno-de-trabajo","text":"En el flujo de trabajo propuesto, se define una estructura simple con los siguientes elementos clave: \ud83d\udcc1 input_papers: Carpeta que contiene los archivos PDF a procesar. \ud83d\udcc1 output_papers: Carpeta donde se guardar\u00e1n los archivos XML generados por GROBID. \ud83d\udcc4 config.json: Archivo de configuraci\u00f3n del cliente GROBID. \ud83d\udcc4 cliente_grobid.py: Script Python para ejecutar la conversi\u00f3n masiva.","title":"Estructura del Entorno de Trabajo"},{"location":"pruebas/05_install-grobid/#configjson","text":"Este archivo es necesario para establecer la configuraci\u00f3n del cliente. En este caso, no requiere modificaci\u00f3n: { \"grobid_server\": \"http://localhost:8070\", \"batch_size\": 100, \"sleep_time\": 5, \"timeout\": 60, \"coordinates\": [ \"persName\", \"figure\", \"ref\", \"biblStruct\", \"formula\", \"s\", \"note\", \"title\" ] }","title":"\ud83d\udcc4 config.json"},{"location":"pruebas/05_install-grobid/#cliente_grobidpy","text":"Este script ejecuta el cliente GROBID para transformar todos los archivos .pdf en la carpeta input_papers y guardar los .xml resultantes en output_papers . from grobid_client.grobid_client import GrobidClient if __name__ == \"__main__\" : client = GrobidClient ( config_path = \"./config.json\" ) client . process ( \"processFulltextDocument\" , \"input_papers\" , output = \"output_papers/\" , consolidate_citations = False , tei_coordinates = False , force = True ) El script utiliza la biblioteca grobid-client para interactuar con el servidor GROBID. La funci\u00f3n process se encarga de procesar los documentos PDF en la carpeta input_papers , generando archivos XML que se guardan en la carpeta output_papers . Los par\u00e1metros adicionales permiten ajustar el comportamiento del procesamiento, como la consolidaci\u00f3n de citas y las coordenadas TEI. Para ejecutar el script, se debe usar espec\u00edficamente Python 3 con el comando: python3 cliente_grobid . py Cada pdf se pasar\u00e1 a XML:","title":"\ud83d\udcc4 cliente_grobid.py"},{"location":"pruebas/05_install-grobid/#referencia","text":"GROBID GitHub GROBID Documentaci\u00f3n GROBID Docker","title":"Referencia"}]}