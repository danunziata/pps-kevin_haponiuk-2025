<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="author" content="Haponiuk Kevin Joel" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>Conceptos Previos - Agente IA orientado a investigaci√≥n cient√≠fica</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../../css/custom.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Conceptos Previos";
        var mkdocs_page_input_path = "investigacion/01_llm.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../.." class="icon icon-home"> Agente IA orientado a investigaci√≥n cient√≠fica
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../..">Introducci√≥n</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Workflow Final</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../implementacion/00_workflow-completo/">Explicaci√≥n Workflow</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../implementacion/01_implementacion-final/">Implementaci√≥n</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../implementacion/02_implementacion-dorotea/">Implementaci√≥n en Dorotea</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Investigaci√≥n</span></p>
              <ul class="current">
                  <li class="toctree-l1 current"><a class="reference internal current" href="#">Conceptos Previos</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#que-es-machine-learning">¬øQu√© es Machine Learning?</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#un-ejemplo-cotidiano">Un ejemplo cotidiano</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#tipos-de-aprendizaje-en-machine-learning">Tipos de aprendizaje en Machine Learning</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#que-es-una-red-neuronal">¬øQu√© es una red neuronal?</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#inspiracion-biologica">Inspiraci√≥n biol√≥gica</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#un-poco-de-historia">Un poco de historia</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#que-es-un-llm">¬øQu√© es un LLM?</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#que-es-el-lenguaje-natural-y-como-puede-interpretarlo-la-ia">¬øQu√© es el lenguaje natural y c√≥mo puede interpretarlo la IA?</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#1-codificacion-numerica">1. Codificaci√≥n num√©rica</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#2-tokenizacion">2. Tokenizaci√≥n</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#3-compresion">3. Compresi√≥n</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#4-marcadores-embedding">4. Marcadores / Embedding</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#5-normalizacion-y-lematizacion">5. Normalizaci√≥n y Lematizaci√≥n</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#6-sampling">6. Sampling</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#evolucion-de-las-redes-neuronales">Evoluci√≥n de las Redes Neuronales</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#redes-neuronales-recurrentes-rnn">Redes Neuronales Recurrentes (RNN)</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#lstm-long-short-term-memory">LSTM (Long Short-Term Memory)</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#el-gran-cambio-transformers">El Gran Cambio: Transformers</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#que-aportan-los-transformers">¬øQu√© aportan los Transformers?</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#ejemplo-con-self-attention">Ejemplo con Self-Attention</a>
    </li>
        </ul>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Metodolog√≠a de investigaci√≥n</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../02_metod-invest/">Concepto general</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../03_estado-arte/">Revisi√≥n Bibliogr√°fica</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../04_arquitectura/">Tipos de Arquitecturas</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Software utilizado</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../05_opentools/">Resumen Opentools</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../06_n8n/">n8n</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../07_grobid/">GROBID</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../08_nlp-models/">Modelos de lenguaje</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../09_mcp/">MCP</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../10_a2a/">A2A</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../11_interface-options/">Interfaz de usuario</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Pruebas durante el desarrollo</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../pruebas/01_install-n8n/">Instalaci√≥n de n8n / Dependencias necesarias</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../pruebas/02_ejemplo-rag-crag/">Ejemplo RAG / CRAG</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../pruebas/03_ejemplo-mcp/">Ejemplo MCP</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../pruebas/05_install-grobid/">Instalaci√≥n de GROBID / Ejemplo de uso</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Info Proyecto</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../proyecto/01_metodologia/">Metodolog√≠a de Trabajo</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../proyecto/02_contribuir/">Como contribuir</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../proyecto/03_about/">Acerca de</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">Agente IA orientado a investigaci√≥n cient√≠fica</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">Investigaci√≥n</li>
      <li class="breadcrumb-item active">Conceptos Previos</li>
    <li class="wy-breadcrumbs-aside">
          <a href="https://github.com/danunziata/pps-kevin_haponiuk-2025/edit/master/docs/investigacion/01_llm.md" class="icon icon-github"> Edit on GitHub</a>
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="como-funciona-un-modelo-de-lenguaje-generativo">¬øC√≥mo funciona un Modelo de Lenguaje Generativo? üß†üí¨</h1>
<p>Para poder desarrollar una soluci√≥n efectiva, es fundamental comprender qu√© ocurre detr√°s de escena cuando le hacemos una solicitud a una inteligencia artificial de texto generativo como puede ser ChatGPT.</p>
<p>En esta secci√≥n vamos a desglosar y explicar, de forma simple y conceptual, los elementos esenciales que conforman un modelo de lenguaje generativo.</p>
<hr />
<h2 id="que-es-machine-learning">¬øQu√© es Machine Learning?</h2>
<p>El <em>Machine Learning</em> es una rama de la inteligencia artificial que permite a las computadoras <strong>aprender de los datos</strong> sin que sea necesario programarlas expl√≠citamente para cada tarea.</p>
<p>Una definici√≥n cl√°sica lo describe as√≠:</p>
<blockquote>
<p><em>Machine Learning es el campo de estudio que da a las computadoras la capacidad de aprender sin ser expl√≠citamente programadas.</em><br />
‚Äî Arthur Samuel, 1959</p>
</blockquote>
<p>Otra, m√°s enfocada en su aplicaci√≥n, dice:</p>
<blockquote>
<p><em>Un programa aprende de la experiencia (E), con respecto a una tarea (T) y una medida de rendimiento (P), si mejora su rendimiento en T, medido por P, gracias a E.</em><br />
‚Äî Tom Mitchell, 1997</p>
</blockquote>
<h3 id="un-ejemplo-cotidiano">Un ejemplo cotidiano</h3>
<p>Tomemos como ejemplo el <strong>filtro de spam</strong>. Este sistema analiza miles de correos que los usuarios marcaron como spam o no spam, y con eso <strong>aprende a detectar patrones</strong> para clasificar correctamente nuevos correos.<br />
Aqu√≠:</p>
<ul>
<li><strong>Tarea (T):</strong> Clasificar correos como spam o no spam.  </li>
<li><strong>Experiencia (E):</strong> Correos etiquetados por usuarios.  </li>
<li><strong>Rendimiento (P):</strong> Porcentaje de correos clasificados correctamente (precisi√≥n o <em>accuracy</em>).</li>
</ul>
<p>Con suficiente experiencia, el sistema mejora su desempe√±o en la tarea, sin necesidad de reglas escritas por humanos.</p>
<h3 id="tipos-de-aprendizaje-en-machine-learning">Tipos de aprendizaje en Machine Learning</h3>
<ul>
<li>
<p><strong>Aprendizaje Supervisado</strong>: Es como aprender a tocar el piano con un profesor. El algoritmo se entrena con datos etiquetados, es decir, con las respuestas correctas ya conocidas. Aprende comparando sus resultados con las respuestas reales y ajustando su comportamiento. Se utiliza, por ejemplo, en sistemas que filtran correos spam.</p>
</li>
<li>
<p><strong>Aprendizaje No Supervisado</strong>: Similar a armar un rompecabezas sin saber c√≥mo deber√≠a verse al final. No se proporcionan etiquetas ni respuestas, y el algoritmo debe descubrir patrones o agrupaciones por s√≠ mismo. Un uso com√∫n es la segmentaci√≥n de clientes en marketing.</p>
</li>
<li>
<p><strong>Aprendizaje por Refuerzo</strong>: Como entrenar a una mascota con premios y castigos. El algoritmo aprende a trav√©s de prueba y error, recibiendo recompensas por buenas acciones y penalizaciones por errores. Es muy usado en videojuegos y rob√≥tica, donde el sistema mejora con la experiencia.</p>
</li>
</ul>
<p align="center">
  <img src="../../images/type_ml.webp" width="100%">
  <br>
  <em>Figura 1: Tipos de aprendizajes en machine learning</em>
</p>

<h2 id="que-es-una-red-neuronal">¬øQu√© es una red neuronal?</h2>
<p>Las redes neuronales artificiales (<em>Artificial Neural Networks</em>, o ANN) son <strong>modelos matem√°ticos inspirados en el funcionamiento del cerebro humano</strong>.
Por medio de una colecci√≥n de unidades llamadas <em>neuronas artificiales</em>, organizadas en capas y conectadas entre s√≠, que <strong>aprende a realizar tareas complejas mediante la experiencia</strong>, es decir, ajustando sus par√°metros a partir de los datos de entrenamiento. Gracias a esta capacidad, pueden realizar tareas como clasificar im√°genes, reconocer voz o jugar videojuegos, todo esto <strong>aprendiendo a partir de datos</strong>.</p>
<h3 id="inspiracion-biologica">Inspiraci√≥n biol√≥gica</h3>
<p>Una <strong>neurona biol√≥gica</strong> es una c√©lula del cerebro que recibe se√±ales de otras neuronas a trav√©s de estructuras llamadas <em>dendritas</em>, procesa esa informaci√≥n en el cuerpo celular, y transmite se√±ales hacia otras neuronas a trav√©s del <em>ax√≥n</em>. Cuando recibe suficientes est√≠mulos, la neurona "dispara" una se√±al el√©ctrica.</p>
<p align="center">
  <img src="../../images/neurona.png"  width="80%">
  <br>
  <em>Figura 2: Una neurona biol√≥gica</em>
</p>

<p align="center">
  <img src="../../images/red_neuronal_humana.png" width="80%">
  <br>
  <em>Figura 3: Representaci√≥n de una red neuronal humana</em>
</p>

<p>De forma an√°loga, una <strong>neurona artificial</strong> recibe m√∫ltiples entradas num√©ricas (como los datos de una imagen o un texto), las combina mediante una funci√≥n matem√°tica y produce una salida. Las conexiones entre neuronas se representan con pesos, que determinan la importancia de cada entrada. Al entrenar la red, estos pesos se ajustan para mejorar el desempe√±o del modelo.</p>
<p align="center">
  <img src="../../images/red_neuronal.png" width="60%">
  <br>
  <em>Figura 4: Representaci√≥n conceptual de una red neuronal artificial </em>
</p>

<h3 id="un-poco-de-historia">Un poco de historia</h3>
<p>Las primeras redes neuronales fueron propuestas en 1943 por <strong>McCulloch y Pitts</strong>, quienes crearon un modelo muy simplificado de neurona artificial capaz de realizar c√°lculos l√≥gicos (como AND, OR y NOT). Este fue el punto de partida para una larga evoluci√≥n.</p>
<p>Durante d√©cadas, las redes neuronales pasaron por varios ciclos de entusiasmo y olvido, hasta que en la √∫ltima d√©cada resurgieron con fuerza, gracias a:</p>
<ul>
<li>la disponibilidad de <strong>grandes cantidades de datos</strong> (Big Data),</li>
<li>el aumento exponencial de la <strong>capacidad de c√≥mputo</strong> (especialmente con GPUs y TPUs),</li>
<li>y las mejoras en los <strong>algoritmos de entrenamiento</strong> (como el descenso por gradiente y sus variantes).</li>
</ul>
<p>Todo esto permiti√≥ entrenar redes neuronales m√°s profundas y complejas, dando lugar al auge de ia que hoy presenciamos.</p>
<h2 id="que-es-un-llm">¬øQu√© es un LLM?</h2>
<p>Un <strong>LLM</strong> (<em>Large Language Model</em>, o <em>Modelo de Lenguaje Grande</em>) es un tipo de modelo de inteligencia artificial entrenado para comprender, generar y trabajar con lenguaje natural de manera fluida. Estos modelos est√°n basados en redes neuronales profundas, especialmente en arquitecturas conocidas como <strong>transformers</strong> (explicado m√°s adelante), que le permite procesar y generar texto a gran escala.</p>
<p>Lo que hace especial a un LLM es la enorme cantidad de datos con los que ha sido entrenado (normalmente textos provenientes de libros, art√≠culos, p√°ginas web y otros contenidos p√∫blicos) y la gran cantidad de par√°metros que posee, muchas veces llegando a <strong>miles de millones de conexiones</strong>. Gracias a esto, los LLM pueden:</p>
<ul>
<li>Redactar textos coherentes y contextuales.</li>
<li>Traducir entre distintos idiomas.</li>
<li>Resumir documentos extensos.</li>
<li>Responder preguntas de forma informada.</li>
<li>Asistir en programaci√≥n, an√°lisis de datos, y mucho m√°s.</li>
</ul>
<h2 id="que-es-el-lenguaje-natural-y-como-puede-interpretarlo-la-ia">¬øQu√© es el lenguaje natural y c√≥mo puede interpretarlo la IA?</h2>
<p>El <strong>lenguaje natural</strong> es la forma de comunicaci√≥n que usamos los seres humanos en nuestra vida cotidiana, como el espa√±ol, ingl√©s, franc√©s, etc. Se caracteriza por ser <strong>rico, flexible y ambiguo</strong>, y est√° lleno de matices culturales, contextuales y emocionales.</p>
<p>En el √°mbito de la computaci√≥n, cuando se habla de <strong>procesamiento de lenguaje natural (NLP "Natural Language Processing", por sus siglas en ingl√©s)</strong>, se hace referencia a la capacidad de una m√°quina para <strong>entender, interpretar, generar y responder</strong> a textos o conversaciones escritos u orales del mismo modo en que lo har√≠a una persona.</p>
<p>Cuando escribimos un texto como:</p>
<div class="highlight"><pre><span></span><code>&quot;El gato duerme en el sof√°&quot;
</code></pre></div>
<p>Este texto pasa por varios pasos para que la IA lo entienda:</p>
<h3 id="1-codificacion-numerica">1. Codificaci√≥n num√©rica</h3>
<p>Lo primero que debemos hacer es <strong>convertir las palabras en un formato que la computadora pueda entender</strong>, como por ejemplo una codificaci√≥n num√©rica. Una forma sencilla de hacerlo es utilizando c√≥digos como <strong>ASCII</strong>, donde a cada car√°cter se le asigna un n√∫mero espec√≠fico.</p>
<p align="center">
  <img src="../../images/codificacion.png" width="80%">
  <br>
  <em>Figura 5: Proceso de codificaci√≥n</em>
</p>

<h3 id="2-tokenizacion">2. Tokenizaci√≥n</h3>
<p>A nuestra red neuronal la <strong>entrenamos aliment√°ndola con una gran cantidad de datos ya codificados</strong>. A medida que procesa esta informaci√≥n, <strong>aprende a reconocer patrones repetitivos</strong> en el lenguaje. Estos patrones, conocidos como <strong>tokens</strong>, se van organizando y catalogando en una especie de vocabulario interno o lista, que luego usar√° para comprender y generar texto.</p>
<p align="center">
  <img src="../../images/patrones.png" width="80%">
  <br>
  <em>Figura 6: Red neuronal encontrando patrones</em>
</p>

<blockquote>
<p>üîç <strong>Aclaraci√≥n:</strong> un <em>token</em> no es necesariamente una palabra. Puede ser una palabra completa, una parte de una palabra, o incluso un conjunto de palabras o s√≠mbolos, dependiendo del modelo y del sistema de tokenizaci√≥n utilizado.</p>
</blockquote>
<h3 id="3-compresion">3. Compresi√≥n</h3>
<p>Una vez que nuestra red neuronal ha identificado los patrones en los datos, <strong>el siguiente paso es representar cada token mediante un n√∫mero o identificador √∫nico (ID)</strong>. Esta conversi√≥n permite <strong>simplificar el procesamiento y optimizar el almacenamiento</strong>, ya que trabajar con n√∫meros es mucho m√°s eficiente que hacerlo directamente con texto.</p>
<p>Por ejemplo:</p>
<table>
<thead>
<tr>
<th>Token</th>
<th>Token completo</th>
<th>ID</th>
</tr>
</thead>
<tbody>
<tr>
<td>hola</td>
<td>72 111 108 97</td>
<td>1923</td>
</tr>
<tr>
<td>hace</td>
<td>104 97 99 101</td>
<td>1234</td>
</tr>
<tr>
<td>fr√≠o</td>
<td>102 114 237 111</td>
<td>6543</td>
</tr>
</tbody>
</table>
<p>De esta forma, cuando la red procesa una frase como "hola hace fr√≠o", en realidad est√° trabajando internamente con una secuencia de n√∫meros: <code>1923, 1234, 6543</code>.</p>
<p align="center">
  <img src="../../images/id-token.png" width="80%">
  <br>
  <em>Figura 7: Representaci√≥n mediante ID</em>
</p>

<h3 id="4-marcadores-embedding">4. Marcadores / Embedding</h3>
<p>Es importante entender que <strong>el modelo no "sabe" lo que significa cada palabra</strong> como lo har√≠a una persona. No tiene una comprensi√≥n sem√°ntica real de los t√©rminos. Sin embargo, <strong>s√≠ puede identificar qu√© tokens est√°n relacionados entre s√≠</strong> gracias a c√≥mo se organizan en el espacio de embedding.</p>
<p>Por ejemplo, puede aprender que el token <code>"la"</code> aparece frecuentemente cerca de <code>"reina"</code>, y por eso los ubica <strong>cerca en el espacio vectorial</strong>. Esa proximidad representa una relaci√≥n estad√≠stica, no un entendimiento real del lenguaje.</p>
<p>Esta capacidad de asociar t√©rminos seg√∫n su contexto permite que el modelo genere texto coherente y mantenga el sentido general, aunque no comprenda el significado como lo har√≠amos nosotros.</p>
<p align="center">
  <img src="../../images/la_reina.png" width="80%">
  <br>
  <em>Figura 8: Ejemplo de relaci√≥n </em>
</p>

<p>De esta manera, puedo encontrar relaciones entre los token y agregar una marca en cada ocasi√≥n que tengan relaci√≥n entre ellos:</p>
<p align="center">
  <img src="../../images/relacion_token.png" width="60%">
  <br>
  <em>Figura 9: marcaci√≥n entre tokens </em>
</p>

<p>la idea es seguir sumando marcadores entre distintos token, de esta manera el modelo aunque no sabe exactamente qu√© significan, s√≠ saber como clasificarlos.</p>
<p align="center">
  <img src="../../images/relacion_token2.png" width="70%">
  <br>
  <em>Figura x: Relaci√≥n entre tokens </em>
</p>

<p>Esto se puede extender hasta <strong>cientos o miles de marcadores</strong> (dimensiones), formando lo que podr√≠amos imaginar como un tipo de <strong>ADN de la palabra</strong> (o del token, en general).</p>
<p>Cada token se representa mediante un <strong>vector num√©rico de gran dimensi√≥n</strong>, donde <strong>cada posici√≥n del vector captura una caracter√≠stica o relaci√≥n aprendida del contexto</strong>. As√≠, dos tokens con significados o usos similares tendr√°n vectores parecidos.</p>
<blockquote>
<p>üìå Nota: aunque en este caso hablamos de "token = palabra", un token tambi√©n puede ser parte de una palabra o incluso una secuencia de palabras, seg√∫n el modelo de tokenizaci√≥n utilizado.</p>
</blockquote>
<p align="center">
  <img src="../../images/relacion_token3.png" width="80%">
  <br>
  <em>Figura 10: Relaci√≥n entre tokens </em>
</p>

<p>Otra forma de representar estos vectores es imaginarlos en un <strong>espacio tridimensional</strong>. En la pr√°ctica, cada marcador corresponde a una dimensi√≥n, por lo que los vectores pueden tener <strong>cientos o miles de dimensiones</strong>. Sin embargo, al proyectarlos en 3D podemos visualizar mejor la <strong>relaci√≥n de cercan√≠a entre tokens</strong>.</p>
<p>Esta idea de "cercan√≠a sem√°ntica" nos permite ver, por ejemplo, que tokens como <code>"rey"</code> y <code>"reina"</code> est√°n m√°s cerca entre s√≠ que de <code>"auto"</code>.</p>
<blockquote>
<p>Esta representaci√≥n permite entender c√≥mo el modelo "comprende" las relaciones entre conceptos sin saber realmente qu√© significan.</p>
</blockquote>
<p>El siguiente ejemplo fue obtenido de <a href="https://projector.tensorflow.org/">esta herramienta de visualizaci√≥n de embeddings de TensorFlow</a>, donde pod√©s explorar estas relaciones en 2D o 3D de forma interactiva.</p>
<p align="center">
  <img src="../../images/embedding_argentina.png" width="100%">
  <br>
  <em>Figura 11: Representaci√≥n tridimensional embedding </em>
</p>

<p>En este caso, podemos ver que palabras cercanas a "argentina" son:</p>
<p align="center">
  <img src="../../images/puntos_arg.png" width="50%">
  <br>
  <em>Figura 12: Puntos m√°s cercanos a la palabra "argentina" </em>
</p>

<p>Esto permite realizar operaciones matem√°ticas entre vectores que representan palabras, tambi√©n conocidos como <em>embeddings</em>. Gracias a esta propiedad, se pueden hacer analog√≠as sem√°nticas, como por ejemplo:</p>
<ul>
<li><strong>"CORONA" + "HOMBRE" ‚âà "REY"</strong></li>
<li><strong>"REY" - "HOMBRE" + "MUJER" ‚âà "REINA"</strong></li>
</ul>
<p>Estos c√°lculos se basan en relaciones sem√°nticas aprendidas por el modelo a partir de grandes cantidades de texto. As√≠, los embeddings no solo capturan el significado de las palabras, sino tambi√©n sus relaciones contextuales.</p>
<h3 id="5-normalizacion-y-lematizacion">5. Normalizaci√≥n y Lematizaci√≥n</h3>
<p>Cuando trabajamos con texto, especialmente al darle instrucciones a una IA, hay muchos t√©rminos que no aportan valor significativo. Palabras como <strong>"la", "lo", "el", "y", "con"</strong>, o incluso los <strong>signos de puntuaci√≥n</strong>, suelen ser irrelevantes para el an√°lisis, ya que aparecen en casi todos los contextos y no cambian el significado central de una frase.</p>
<p>Para simplificar y optimizar el procesamiento, lo primero que hacemos es <strong>limpiar</strong> los textos antes de pasarlos al sistema. Veamos un ejemplo con la frase:</p>
<blockquote>
<p><strong>"El gato est√° durmiendo en el sof√°."</strong></p>
</blockquote>
<ol>
<li>
<p><strong>Frase original:</strong><br />
<code>"El gato est√° durmiendo en el sof√°."</code></p>
</li>
<li>
<p><strong>Eliminaci√≥n de palabras irrelevantes y signos de puntuaci√≥n:</strong><br />
<code>"gato est√° durmiendo sof√°"</code></p>
</li>
<li>
<p><strong>Lematizaci√≥n (reducci√≥n de las palabras a su forma base):</strong><br />
<code>"gato es dormir sof√°"</code></p>
</li>
<li>
<p><strong>Tokenizaci√≥n (divisi√≥n en unidades b√°sicas):</strong><br />
<code>"gato", "dormir", "sof√°"</code></p>
</li>
</ol>
<p>Esta frase queda <strong>comprimida</strong> y m√°s eficiente para el procesamiento por parte del modelo. En lugar de trabajar con muchas palabras y tokens innecesarios, esta versi√≥n reduce el texto a su <strong>m√≠nima expresi√≥n √∫til</strong>, acelerando la b√∫squeda y el an√°lisis dentro del modelo.</p>
<p>Modelos como ChatGPT <strong>preparan internamente nuestra entrada</strong> de esta forma para compararla con representaciones (embeddings) similares y generar una respuesta adecuada. Este proceso implica una <strong>p√©rdida de informaci√≥n</strong>, ya que comprimimos el mensaje original. Sin embargo, esto no es algo negativo: la IA no repite textualmente, sino que genera una <strong>respuesta original</strong> basada en la comprensi√≥n del contenido.</p>
<blockquote>
<p>Un buen ejemplo ser√≠a escuchar una clase y al d√≠a siguiente explicarle el concepto a alguien m√°s: no recordamos todas las palabras exactas, pero s√≠ entendemos y transmitimos la idea principal.</p>
</blockquote>
<h3 id="6-sampling">6. Sampling</h3>
<p>El <em>sampling</em> (muestreo) es una t√©cnica que permite al modelo generar respuestas de forma m√°s <strong>variada y creativa</strong>, introduciendo un <strong>componente aleatorio</strong> en el proceso de generaci√≥n de texto.</p>
<p>En lugar de elegir siempre la palabra m√°s probable seg√∫n el modelo, el sampling permite explorar otras opciones que, aunque sean ligeramente menos probables, siguen siendo <strong>coherentes</strong> dentro del contexto. Es como moverse ligeramente dentro del <strong>espacio de embeddings</strong>, eligiendo caminos diferentes que llevan a respuestas distintas pero igualmente v√°lidas.</p>
<blockquote>
<p>Esto evita que el modelo siempre d√© las mismas respuestas ante los mismos inputs y permite generar resultados m√°s <strong>naturales y diversos</strong>, sin perder el sentido general de la conversaci√≥n.</p>
</blockquote>
<h2 id="evolucion-de-las-redes-neuronales">Evoluci√≥n de las Redes Neuronales</h2>
<p>Las redes neuronales tradicionales tienen una gran limitaci√≥n: <strong>no tienen memoria</strong>. Es decir, procesan cada entrada (input) de manera aislada, sin recordar lo que pas√≥ antes. Pero, ¬øc√≥mo puede una red entender el contexto de una conversaci√≥n o el significado completo de una frase si solo ve una palabra a la vez?</p>
<h3 id="redes-neuronales-recurrentes-rnn">Redes Neuronales Recurrentes (RNN)</h3>
<p>Para resolver este problema apareci√≥ una arquitectura llamada <strong>Red Neuronal Recurrente</strong> (RNN), que introduce la idea de "memoria" al alimentar el resultado de un paso como entrada del siguiente.</p>
<ul>
<li>Procesan palabra por palabra en <strong>secuencia</strong>.</li>
<li>Cada palabra se analiza considerando lo anterior.</li>
<li>Problema: <strong>pierden contexto</strong> cuando el texto es muy largo.</li>
<li>No permiten <strong>paralelizar</strong>, por lo que el entrenamiento es m√°s lento.</li>
</ul>
<p align="center">
  <img src="../../images/rnn.png" width="20%">
  <br>
  <em>Figura 13: Ejemplo de RNN </em>
</p>

<h3 id="lstm-long-short-term-memory">LSTM (Long Short-Term Memory)</h3>
<p>Las <strong>LSTM</strong> son una mejora de las RNN tradicionales. Introducen una especie de "filtro de memoria" que decide qu√© informaci√≥n mantener y cu√°l olvidar.</p>
<ul>
<li>Manejan mejor el contexto de largo plazo.</li>
<li>Son m√°s precisas que las RNN simples.</li>
<li>Aun as√≠, siguen siendo <strong>secuenciales</strong> y <strong>no paralelizables</strong>, lo que limita su eficiencia.</li>
</ul>
<h2 id="el-gran-cambio-transformers">El Gran Cambio: Transformers</h2>
<p>El gran salto lleg√≥ en 2017 con el paper <a href="https://arxiv.org/abs/1706.03762"><strong>‚ÄúAttention is All You Need‚Äù</strong></a>, que introdujo el modelo <strong>Transformer</strong>. Esta arquitectura cambi√≥ por completo la forma en que las redes procesan secuencias.</p>
<h3 id="que-aportan-los-transformers">¬øQu√© aportan los Transformers?</h3>
<ul>
<li>Utilizan un mecanismo llamado <strong>Self-Attention</strong> para identificar la importancia de cada palabra en relaci√≥n con todas las dem√°s, <strong>en paralelo</strong>.</li>
<li>Permiten analizar <strong>todas las palabras al mismo tiempo</strong>, no de forma secuencial.</li>
<li>Esto facilita el entrenamiento y mejora la comprensi√≥n del <strong>contexto completo</strong>.</li>
<li>Son <strong>altamente paralelizables</strong>, lo que acelera enormemente el proceso.</li>
</ul>
<h3 id="ejemplo-con-self-attention">Ejemplo con Self-Attention</h3>
<p>En la frase:</p>
<blockquote>
<p>‚ÄúLa vida es bella, as√≠ que v√≠vela cada d√≠a.‚Äù</p>
</blockquote>
<p>El modelo analiza cada palabra en relaci√≥n con todas las dem√°s. Por ejemplo:</p>
<ul>
<li>"la" con respecto a "vida", "bella", "v√≠vela", etc.</li>
<li>"vida" con respecto a todas las dem√°s tambi√©n.</li>
</ul>
<p align="center">
  <img src="../../images/ejemplo_atencion.png" width="100%">
  <br>
  <em>Figura 14: Ejemplo de uso de transformers </em>
</p>

<p align="center">
  <img src="../../images/paralelizacion.png" width="100%">
  <br>
  <em>Figura 15: Ejemplo de paralelizaci√≥n </em>
</p>

<p>Esto permite entender cu√°les son las palabras m√°s importantes para construir el significado global de la frase, incluso si est√°n separadas por otras palabras.</p>
<p>Gracias a los Transformers, hoy podemos contar con modelos como GPT, BERT y otros LLM (Large Language Models) que comprenden mejor el lenguaje y generan respuestas m√°s coherentes, r√°pidas y √∫tiles.</p>
<hr />
<p>Bibliograf√≠a:</p>
<p>https://www.youtube.com/watch?v=FdZ8LKiJBhQ&amp;list=PL3ei_Xb7-ic5pkJDTplPxWvE8t13mm19W&amp;index=8
https://www.ntiva.com/blog/what-is-machine-learning
Aur√©lien G√©ron - Hands-On Machine Learning with Scikit-Learn, Keras and TensorFlow
http://personal.cimat.mx:8181/~mrivera/cursos/aprendizaje_profundo/RNN_LTSM/introduccion_rnn.html
https://visajourneypro.com/
https://projector.tensorflow.org/
https://arxiv.org/abs/1706.03762</p>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../../implementacion/02_implementacion-dorotea/" class="btn btn-neutral float-left" title="Implementaci√≥n en Dorotea"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../02_metod-invest/" class="btn btn-neutral float-right" title="Concepto general">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
      <p><img src="https://raw.githubusercontent.com/danunziata/pps-kevin_haponiuk-2025/main/docs/images/escudo_unrc.png" alt="Logo UNRC" style="height: 30px; vertical-align: middle; margin-right: 10px;">
<img src="https://raw.githubusercontent.com/danunziata/pps-kevin_haponiuk-2025/main/docs/images/escudo_ing.png" alt="Logo ING" style="height: 30px; vertical-align: middle; margin-right: 10px;">
&copy; 2025 <a href="https://www.ing.unrc.edu.ar/inicio.php" target="_blank" rel="noopener">Universidad Nacional de R√≠o Cuarto | Facultad de Ingenier√≠a</a>
</p>
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
        <span>
          <a href="https://github.com/danunziata/pps-kevin_haponiuk-2025" class="fa fa-github" style="color: #fcfcfc"> GitHub</a>
        </span>
    
    
      <span><a href="../../implementacion/02_implementacion-dorotea/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../02_metod-invest/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../..";</script>
    <script src="../../js/theme_extra.js"></script>
    <script src="../../js/theme.js"></script>
      <script src="../../js/custom.js"></script>
      <script src="../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
